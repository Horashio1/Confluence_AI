id,type,status,tiny_link,title,content,is_internal
538575113,page,current,/x/CQEaI,Big Data and Data Science,"Welcome to your new documentation space! Use it to organize any information that people need to find and reference easily , like your organisation's travel policies, design guidelines, or marketing assets. To start, you might want to: Customise this overview using the edit icon at the top right of this page. Create a new page by clicking the + in the space sidebar. Tip : Add the label featured to any pages you want to appear in the Featured pages list below. Search this space Featured Pages false BDDS false title page label = ""featured"" and type = ""page"" and space = ""BDDS"" featured Recently Updated page 5 true concise NEED INSPIRATION? Check out our guide on building better documentation to learn best practices for creating and organizing documents in Confluence.",False
539426913,page,current,/x/YQAnI,Setting up Virtual Network,"A virtual network is an IP address space where all the Big Data and Data Science infrastructure pieces will reside and be part of a single subnet. Follow the below steps to create a new Virtual Network as the second step of provisioning a new Big Data and Data Science environment. Login to the Azure Portal. Click ""All Services"" => ""Networking"" => ""Virtual networks"". Click the ""Add"" option. Provide the following information for the virtual network. A meaningful ""Name"". Provide an ""Address space"". Ensure the Address space provides enough subnetting options for your subnet requirements. Select ""Subscription"". Select the ""Resource Group"" (i.e. the resource group created under section ). Select the ""Location"" (i.e. same location as you created the resource group in). Provide a ""Subnet Name"" for the subnet where all HDInsight cluster resources will be assigned to. Provide the ""Subnet Address range"". Select ""Basic"" for ""DDoS protection"" (""Standard"" incurs additional charges). Enable ""Service endpoints"" and select ""Microsoft.Storage"" under ""Services"" (this allows Microsoft Storage to be accessed from this virtual network). Keep the ""Firewall"" disabled (we can configure firewall as required later-on). Click the ""Create"" button. It may take couple of minutes for the new virtual network to be created. Once the new virtual network is created you can locate it under ""Virtual networks"" sub-section of the portal.",False
539426939,page,current,/x/ewAnI,Adding MySQL support to Sqoop running in Hadoop Cluster,"Sqoop is the tool used to ingest data from relational databases to a Hadoop cluster. For additional details on Sqoop and it's integration into the PickMe Big Data and Data Science platform, refer section . Default installation of Sqoop in HDInsight Hadoop clusters does not come with MySQL support pre-provisioned. Thus, follow the below steps to add MySQL batch ingestion support to the Sqoop running in HDInsight Hadoop cluster. Download the ""mysql-connector-java.jar"" file from the following drive location ( Note: Please ensure to use this specific version of the connector only, as other / latest MySQL connectors may fail to work properly in HDInsight Hadoop clusters). https://drive.google.com/drive/folders/1p94WJyAy46Kvt2TuGGt1WV_XRTS42Avr?usp=sharing SCP the above .jar file to the Edge node of the cluster, and from the Edge node, SCP the .jar file to all Worker Nodes of the cluster ( To find the worker node hostnames to SSH, refer sub-section Ambari under the section Access, Maintenance and Monitoring of HDInsight Cluster Components ). SSH to a specific Worker Node and issue the ""find"" command shown below. Note the file path to the mysql-connector-java.jar under the ""sqoop"" directory. Navigate to the ""sqoop/lib"" directory and issue a long list command. Note the soft-linked directory where the mysql-connector-java.jar is residing. Navigate to this soft-linked directory (i.e. in above screenshot /usr/share/java). You will notice that, although the soft-link is created, the actual mysql-connector-java.jar is not deployed in this directory. Copy the SCPed mysql-connector-java.jar to this soft-linked directory. Repeat the above steps to copy the mysql-connector-java.jar to the soft-linked directory in ALL worker nodes of the HDInsight Hadoop cluster. Login to the Ambari of the HDInsight Hadoop cluster ( See Ambari sub-section under the Access, Maintenance and Monitoring of HDInsight Cluster Components on how to login to Ambari). In Ambari portal, Click on ""Sqoop"". Click on ""CONFIGS"". Click on ""Advanced sqoop-env"". To the jdbc.drivers textbox enter ""com.mysql.jdbc.Driver"". Click ""Save"" button. When you press the ""Save"" button, Ambari may pop-up a change tracking pop-up box where you are requested to describe the change you did. If this pop-up appears, enter a description such as ""Enabled MySQL support in Sqoop"" and proceed. If any confirmation dialog is shown, confirm and proceed. Restart Sqoop service from Ambari portal (refer sub-section of for further details on this component restarting).",False
539459585,page,current,/x/AYAnI,Architecture,This section includes the PickMe Big Data and Data Science stack architecture.,False
539459589,page,current,/x/BYAnI,Architecture V1.0,"Diagram below illustrates the V1.0 of the Big Data and Data Science architecture at PickMe. Limitations of architecture V1.0: Scaling issues due to non Big Data technologies usage (MSSQL, SSIS etc.). Tight coupling. E.g. VTiger DB to CRM Rating API POST Dispersed across multiple DCs / Clouds. Stability issues with the current IaaS / PaaS providers (i.e. Dialog and Iguazio). Due to above limitations, PickMe will be migrating to a more scalable / stable / mature / Big Data ready architecture described in-detail in the section .",False
539459617,page,current,/x/IYAnI,Architecture V2.0,Diagram below illustrates PickMe's Data Core Architecture V2.0 and how it integrates with other sub-systems at PickMe. Link to the drawing: https://docs.google.com/drawings/d/1xAJGr7WK0OCEfrdwXrFMfMjnXMCyKZCEOF99icwim30/edit?usp=sharing,False
539459677,page,current,/x/XYAnI,Adding an Edge Node to a HDInsight Cluster,"Edge node is the gateway node to a HDInsight cluster. It is automatically provisioned with all the HDInsight cluster client tools which is required to interact with the HDInsight cluster. This section only includes deploying an Edge node to a single HDInsight cluster. Using these same steps, an Edge node must be installed to all 4 HDInsight clusters (i.e. Hadoop, HBase, IQR and Spark). Visit the link: https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apps-use-edge-node#access-an-edge-node Click the ""Deploy to Azure"" button. In the deployment page, Select the ""Subscription"". Select the ""Resource Group"". Provide the ""Cluster Name"" of the cluster to which you need to install the edge node into (ensure that the cluster name is exactly the same that you provided during the cluster provisioning). Click on ""Edit template"" option. Click on ""applicationName"" variable. Provide a meaningful name for the edge node (ensure that the edge node name reflects that it is the edge node, and the cluster to which it is assigned to, for easy recognition). Scroll down and provide the VM size. VM size MUST BE the same size as cluster worker node size. Click the ""Save"" button. Check the ""I agree to the terms and conditions stated above"" checkbox. Click ""Purchase"" button. Edge node provisioning will take several minutes. Once successfully created the edge node will appear in the cluster overview page.",False
539459681,page,current,/x/YYAnI,Changing timezone of the HDInsight Cluster servers,"Follow the below steps to change the timezone of all cluster servers to Asia/Colombo timezone. SSH to the respective Edge node of the cluster (Refer sub-section under the section ). From the Edge node, again SSH into the respective cluster nodes (To find the cluster node hostnames to SSH, refer sub-section under the section ). To change the timezone of the respective server, run the following command in the terminal. Repeat these steps to change the timezone of all cluster servers including the Edge node.",False
539459687,page,current,/x/Z4AnI,Azkaban Installation on a HDInsight Cluster,"Azkaban is an open sourced workflow manager developed by LinkedIn and needs to be built from the source. The Git repository of Azkaban can be found in this link . Building Azkaban artifacts using the above repository is out-of-scope of this provisioning guide, and the latest Azkaban artifacts built using the instructions in above Git repository at the time of writing this provisioning document can be found in the following drive location. Location: https://drive.google.com/drive/folders/1qNZI5CUidPeKqUWfPbZa_3RJzKQz0Bxb?usp=sharing Follow the below steps to install Azkaban in Multi executor mode in a HDInsight cluster and for the required Azkaban artifacts indicated in these steps, refer the above drive location. Creating the required MySQL Schema Find your public IP address (you may simply type ""what is my ip"" in Google for this). Navigate to the Azkaban MySQL DB server overview page and do the following, Click on ""Connection security"". Add your public IP address as a new rule. Click ""Save"" button. Firewall rule may take several minutes to complete provisioning. Click on the MySQL server ""Overview"" page and note down the ""Server name"" and ""Server admin login name"". Open MySQL Workbench installed in your local machine and do the following, Click on ""+"" sign to create a new connection. Provide a meaningful name to the ""Connection Name"". Provide the previous step noted down ""Server name"" as the ""Hostname"" of the connection. Keep the port as ""3306"". Provide the previous step noted down ""Server admin login name"" as the ""Username"" of the connection. Click on ""Store in Vault"" button for ""Password"". Provide the MySQL admin login password and click ""OK"" button. Click on ""Test Connection"" button. If all steps up to now were successful, you should see the following connection successful message in the MySQL Workbench. Press ""OK"" button to close the pop-up. Click ""OK"" button in the ""Setup New Connection"" window to save the newly created connection in MySQL Workbench. Click on the newly created connection to connect to the Azkaban MySQL database from your local MySQL Workbench. Azkaban must be installed in thre e HDInsight clusters. Namely, Hadoop cluster, Spark cluster and HBase cluster (refer for additional details). For these three Azkaban installations, we need to create three MySQL databases as below (ensure to use meaningful names for the database names which represent the HDInsight cluster type for easier reference). Run the above create database queries in the MySQL Workbench query window one-by-one. Verify that each create database query executed successfully in the output window. Click the ""Refresh"" button in the schema navigator window of MySQL Workbench and the three new MySQL databases must appear in the navigator. Download the ""create-all-sql-x.xx.x.sql"" (where x.xx.x refers to the version) from the Azkaban artifact drive location mentioned at the beginning of this guide. Double click on a database name to make that database the active database in MySQL query editor. Click on ""SQL script open"" option in the MySQL Workbench. Locate the downloaded ""create-all-sql-x.xx.x.sql"" and ""Open"" this script file. Click the ""Execute"" option to execute the script on the selected MySQL database. Confirm that all steps in the script ran without any errors by reviewing the output console of the MySQL Workbench. Repeat running this ""create-all-sql-x.xx.x.sql"" script on all three MySQL databases. Uploading Azkaban Artifacts to the Cluster Servers Download the highlighted Azkaban artifacts from the drive location. These artifacts can be divided into two groups. Executor server provisioning artifacts, azkaban-exec-server-x.xx.x.tar.gz (x.xx.x reflects the version) az-hadoop-jobtype-plugin-x.xx.x.tar.gz (x.xx.x reflects the version) Web server provisioning artifacts, azkaban-web-server-x.xx.x.tar.gz (x.xx.x reflects the version) SCP all three tar artifacts to the Edge node of the respective cluster. From the Edge node, SCP the ""Executor server provisioning artifacts"" to all the Worker Nodes of the respective cluster and the ""Web server provisioning artifacts"" to one of the Head Nodes of the cluster. Provisioning the Azkaban Executor Server Follow the below steps for provisioning the Azkaban Executor Server. Repeat these steps in all Worker Nodes of the respective cluster. Navigate to the directory ""/usr/share/"". Create the directory ""azkaban"". Navigate inside the ""azkaban"" directory and copy the ""azkaban-exec-server-x.xx.x.tar.gz"" artifact inside this ""azkaban"" directory. Untar the ""azkaban-exec-server-x.xx.x.tar.gz"" artifact. Navigate inside the ""conf"" directory of the extracted artifact. You will see several property files inside this ""conf"" directory. Open the ""azkaban.properties"" file. In the ""azkaban.properties"" file change the following, Provide a meaningful name for the Azkaban instance. Provide a meaningful label for the Azkaban instance. Change the timezone to Asia/Colombo. Provide the absolute path to the ""conf/global.properties"" file (default conf file includes the relative path and it will not work as expected). Change the azkaban.webserver.url to the IP address of the Head Node where you will be deploying the Azkaban Web Server into. Provide the absolute path to the jobtype plugin directory (default conf file includes the relative path and it will not work as expected). In the ""azkaban.properties"" file change the following also, Set the mysql.host to the correct server hostname of the Azkaban MySQL DB provisioned under . Set the mysql.database to the correct MySQL database created for this cluster. Set the mysql.user to the MySQL DB admin username of the Azkaban MySQL DB server. Set the mysql.password to the MySQL DB admin password of the Azkaban MySQL DB server. Save and exit from the ""azkaban.properties"" file. Untar the ""az-hadoop-jobtype-plugin-x.xx.x.tar.gz"" in a temporary location (e.g. home directory of the SSH user), and locate the ""az-hadoop-jobtype-plugin-x.xx.x.jar"" file inside the extracted content. Navigate to ""/usr/share/azkaban/azkaban-exec-server-x.xx.x/plugins/jobtypes"" directory. Copy the ""az-hadoop-jobtype-plugin-x.xx.x.jar"" extracted into the temporary location, inside this ""/usr/share/azkaban/azkaban-exec-server-x.xx.x/plugins/jobtypes"" directory. Navigate inside the ""/usr/share/azkaban/azkaban-exec-server-x.xx.x/bin"" directory. Execute the ""start-exec.sh"" shell script as shown below. Open the Executor Server log file indicated below and verify that there are no error messages in Executor Server startup (you may ignore any warning messages). While still within the bin directory, execute the below curl command to activate and register the new Executor Server. You can issue the following ""SELECT"" query on the respective Azkaban MySQL database and verify that the new Executor Server is registered (i.e. a row for that particular executor should be visible in this table) and activated (active column should indicate 1). Provisioning the Azkaban Web Server Follow the below steps for provisioning the Azkaban Web Server. Only complete these steps in one Head Node of the respective cluster. Navigate to the directory ""/usr/share/"". Create the directory ""azkaban"". Navigate inside the ""azkaban"" directory and copy the "" azkaban-web-server-x.xx.x.tar.gz "" artifact inside this ""azkaban"" directory. Untar the "" azkaban-web-server-x.xx.x.tar.gz "" artifact. Navigate inside the ""conf"" directory of the extracted artifact. You will see several property files inside this ""conf"" directory. Open the ""azkaban.properties"" file. In the ""azkaban.properties"" file change the following, Provide a meaningful name for the Azkaban instance. Provide a meaningful label for the Azkaban instance. Change the web.resource.dir to the absolute path to the web directory (default conf file includes the relative path and it will not work as expected). Change the timezone to Asia/Colombo. Provide the absolute path to the ""conf/ azkaban-users.xml "" file (default conf file includes the relative path and it will not work as expected). Provide the absolute path to the ""conf/global.properties"" file (default conf file includes the relative path and it will not work as expected). In the ""azkaban.properties"" file change the following also, Set the mysql.host to the correct server hostname of the Azkaban MySQL DB provisioned under MySQL Server Provisioning for the usage of Azkaban Multi Executor Setup . Set the mysql.database to the correct MySQL database created for this cluster. Set the mysql.user to the MySQL DB admin username of the Azkaban MySQL DB server. Set the mysql.password to the MySQL DB admin password of the Azkaban MySQL DB server. Save and exit from the ""azkaban.properties"" file. Configure credentials for the users who need access to Azkaban Web UI in the ""azkaban-users.xml"" file (for more information refer this link ). Navigate inside the ""/usr/share/azkaban/azkaban-web-server-x.xx.x/bin"" directory. Execute the ""start-web.sh"" shell script as shown below. Open the Web Server log file indicated below and verify that there are no error messages in Web Server startup (you may ignore any warning messages).",False
539459691,page,current,/x/a4AnI,Mode BI connector installation on Interactive Query Cluster,"In order for the Mode BI to be able to connect to the Interactive Query Cluster Hive, follow the below steps. NOTE: Only a Mode BI ""Admin"" user can complete these steps. Login to the Mode BI portal and click on the account icon as indicated below. Click on ""Organization Settings"". Click on ""Connections"". Click on ""Connect a database"" button. From the dropdown select ""Hive"". Click on ""connect a new bridge"". Click ""Ubuntu"", as HDInsight cluster VMs are provisioned with Ubuntu OS. SSH into the HDInsight IQR cluster Edge node and copy-paste and execute below four commands one-by-one. Click ""Next >"" when all four commands are executed successfully in the IQR cluster Edge node. The new bridge connection will appear in the screen shown below within maximum couple of minutes. Click on the new bridge connector. In the Hive credentials page, Enter a ""Display name"". Enter the cluster Head Node hostname where Hive Server Interactive Process is running. HDInsight Hive port is 10001, thus specify the same as the ""Port"" value. Provide the database name followed by ;transportMode=http Select the Hive version. Select ""Limit access to this connection"" option (any required users can be added later on). Click ""Connect"" button. If all steps were successful, you will be navigated to the page where you are asked to add new users to access this connection. You may add any required additional users in Mode to be able to access this new connection. Or you can skip that step and complete the connection creation wizard (new users can be added later as well). IMPORTANT : If you need to access other Hive databases in the same IQR cluster, it is not required to install the bridge again. You can just select the existing bridge and proceed with the Hive credentials step mentioned above.",False
539492370,page,current,/x/EgAoI,Data Services API,This section describes the Service APIs provided by the Data backend.,False
539492384,page,current,/x/IAAoI,Food Recommendations API,"Revision History Date Version Change Request Date Description Author 2019-07-20 1.0.0 N/A Initial version Bathiya Priyadarshana 2019-07-22 1.0.1 N/A Added authentication and throttling related error codes to Client Errors sub-section. Bathiya Priyadarshana 2019-08-01 1.0.2 N/A Correlation Id was moved to a dedicated response header to make consistent with the request headers. Converted both Client and Server error response bodies to the same JSON schema for ease of integration for the caller. Added two new error codes to represent generic client error and generic server error respectively. Bathiya Priyadarshana 2019-10-01 1.0.3 N/A Updated the ""Request Schema => Query Parameters"" section to reflect that default limit has been updated to ""infinity"" from previous default limit of 5. Bathiya Priyadarshana 2020-05-19 1.0.4 N/A Changed the productIds CCP key name to productId Bathiya Priyadarshana 2020-05-21 1.0.5 N/A Changed the 'id' attribute name to 'restaurantId' as per the actual implementation in production. 'FlatItemsRec' new payload type was introduced to support recommendation types that output items. Bathiya Priyadarshana 2019-09-20 1.1.0 N/A Haversine restaurant filtering enabled prod release Bathiya Priyadarshana 2020-10-07 1.2.4 N/A Release including Newly Joined Restaurants, Popular Restaurants and FBT Items Recommendations Shehan Ishanka 2021-03-30 1.4.0 N/A Top Picks and Recommended for You recommendations Shehan Ishanka 2021-11-19 1.5.0 N/A SKU Id version of FBT Recommendations Sulhi Cader 2022-04-19 1.5.1 N/A Added displayImage param for Flat Restaurant Recommendations Payload Samitha Jayaweera 2022-06-08 1.5.2 N/A Added details for Must Try, Family Friendly & Trending Sliders Added position, description, listingImage params for Flat Restaurant Recommendations Payload Samitha Jayaweera 2022-07-07 1.5.3 N/A Added details for featured Sliders restaurant recommendation Chandrasegaran Shobanapriyan References Reference Version Date Remarks Introduction Food Recommendations API is one of the major APIs hosted in the PickMe data backend. This API will provide a one time integration architecture where new recommendation types can be made available with no / minimal changes to the consumer code. Currently the API is facilitated as a RESTful API. Request Schema Following table includes the request schema of the Food Recommendations REST API. URL http://<endpoint_addr>/iserv/v1/food/recommendations Description Service to request food recommendations based on channel identification and other context information. Request Method POST Request Body Format application/json Note 1: Request must specify “Content-Type” header with the value “application/json”. Note 2: Please see section Request Body for POST request body schema. Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters: Parameter Name State Type Values Description Remarks cid Mandatory String AndroidApp IosApp Channel identification. Note: Listed are the channel IDs which will be supported by Food Recommendation Service by default. New channel IDs can be added by submitting a request to the Big Data team. pgid Mandatory String Home PDP PMP Search SalesEvent ZeroSearch ProductNotFound ShoppingBag CartAdd Checkout OrderConfirmation Payment OrderReview Page identification. Note: Listed are the page IDs which will be supported by Food Recommendation Service by default. New page IDs can be added by submitting a request to the Big Data team. plids Mandatory String Horizontal1 Horizontal2 Horizontal3 Horizontal4 Horizontal5 Horizontal6 Horizontal7 Horizontal8 Horizontal9 Horizontal10 List of placement identifications in the given page. Each placement identification should be specified in the following format, <placement_id>|<limit> E.g. Horizontal1|10,Horizontal2|5,Horizontal3|7 Note 1: Listed are the placement IDs which will be supported by Food Recommendation Service by default. New placement IDs can be added by submitting a request to the Big Data team. Note 2: plids value must be URL encoded. I.e. for example, Horizontal1 %7C 10 %2C Horizontal2 %7C 5 %2C Horizontal3 %7C 7 Note 3: Limit component of each plid specifies the number of recommendations to be returned for that placement. If limit component is empty or not specified (e.g. Horizontal1| ,Horizontal2|3 OR Horizontal1|5, Horizontal2 ,Horizontal3|2) for any provided placement(s), a default limit of ""infinity"" will be applied for such placements. I.e. all available recommendations will be returned. At least 1 plid must be specified. Empty plid components (e.g. Horizontal1|10, |2 ,Horizontal5|5) will be ignored. Request Body: Request body of the Food Recommendation POST API will need to follow below JSON schema. { ""key1"": ""value1"", ""key2"": ""value2"", ""key3"": ""value3"" } E.g. { ""productId"": ""1000047,1000064"", ""customerId"": ""854124796325"", ""pickLat"": "" 6.909397"", ""pickLon"": "" 79.896342"", ""restaurantId"": ""541247"" } This JSON map is called the Channel Context Parameters (CCP) map. Values passed in this CCP map is used by Food Recommendation Service to provide a more enhanced recommendation response to the requesting channel. This is done by using these CCP parameters to: Run underlying recommendation algorithms (i.e. using CCPs as input parameters for the algorithms). To get information that is required for the personalization of recommendations. To get information that is required for the geo-localization of recommendations. To enable business to use some of these parameters to set up Business Rules that Food Recommendation Service should adhere to when providing recommendations (e.g. to boost certain products, to filter out certain products from recommendations etc) - Future Work . Table below illustrates the CCP parameters that Food Recommendation Service requires from channels on various contexts. Each parameter is also included with some example values and usage of those values in the previously mentioned use cases of Food Recommendation Service. Channels are encouraged to send these CCPs whenever available, to benefit from Big Data's constantly improving algorithms and additional capabilities without any overhead on channel developers having to modify any channel side code to enable such future use cases. CCP Name Description Example values Usage by Food Recommendation Service productId Comma separated list of one or more seed product Ids. E.g. (multi): ""1000047,1000064"" E.g. (single): ""1000047"" Used as input parameter for algorithms taking seed product Id(s) (View-View, Similar Products etc.). customerEmail Customer’s email address. ""abc@gmail.com"" Used as input parameter for customer recommendation algorithms and also for personalization of recommendations. customerMobile Customer's mobile number ""0777654123"" Used as input parameter for customer recommendation algorithms and also for personalization of recommendations. customerId Customer's PickMe Id. ""854124796325"" Used as input parameter for customer recommendation algorithms and also for personalization of recommendations. restaurantId Restaurant Id. ""541247"" Used as input parameter for restaurant specific recommendation algorithms (e.g. Top Trending in Restaurant, Best Sellers in Restaurant etc.). searchTerm Search term the customer entered. ""teriyaki chicken"" Used as input parameter to algorithms which support generating recommendations based on search terms (e.g. Contextual Search). pickLat Pickup latitude. "" 6.909397"" Used for the geo-localization of recommendations. pickLon Pickup longitude. "" 79.896342"" Used for the geo-localization of recommendations. dropLat Drop latitude. "" 6.914776"" Used for the geo-localization of recommendations. dropLon Drop longitude. "" 79.877658 "" Used for the geo-localization of recommendations. pickPlaceId Pickup place Id. ""8645"" Used for the geo-localization of recommendations. dropPlaceId Drop place Id. ""6521"" Used for the geo-localization of recommendations. cartAmt Current cart amount. ""510.45"" Current cart Rs. amount of the customer. Can be used for Cart (aka ShoppingBag) specific recommendations. *Note: New CCPs can be added based on the new use cases. Request Headers: Following table illustrates the Request Headers that must be included in the Food Recommendations POST API. Header Name State Example Value Description Content-Type Mandatory application/json Value MUST be application/json X-FDR-API-KEY Mandatory androidprod API key issued to the consumer by the Big Data team. Must be kept in consumer's config as value will be changed in a breach. X-FDR-API-SECRET Mandatory feaee65a-2f54-4b18-8a7e-ea409a2e0d72 API secret issued to the consumer by the Big Data team. Must be kept in consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 223e1203-0237-4770-89f6-31d65843f4c1 If provided, the given Correlation Id will be logged with all log messages logged by the Food Recommendation Service in executing this particular request. This will enable deeper issue analysis from source to food recommendation backend. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Food Recommendation Service will generate it's own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Schema HTTP 200(OK) response schema of Food Recommendations API will follow the base JSON response structure indicated below. { ""payload"": { ""cid"": ""(String)"", ""pgid"": ""(String)"", ""recommendations"": [ { ""plid"": ""(String)"", ""recMeta"": { ""type"": ""(String)"", ""templateId"": ""(String)"" }, ""recPayload"": {...} } ] } } payload object Contains the entire response payload. cid string Channel id which sent the request for this response. pgid string Page id in the channel which sent the request for this response. recommendations array This array of recommendations can contain one or more recommendations, one for each placement requested in the request for this response. plid string Placement id in the channel page which sent the request for this response. recMeta object Object containing metadata on how to interpret the recPayload. type string Type of recommendation included in the recPayload (See Recommendation Types sub-section below). templateId string Will be null for the time being. For future work. recPayload object Contains the actual experience payload and schema will vary based on the type of recommendation (See Recommendation Payload Types sub-section below.). Recommendation Types: Following table contains the current recommendation types supported by Food Recommendation Service. This table also indicates the relevant Recommendation Payloads sub-sections which provides the exact recPayload schema for each supported recommendation type. New recommendation types can be added in future based on the use cases. Recommendation Type (recMeta => type) Description Related recPayload sub-section FlatRestaurantsRec All recommendations that only consist of a single ordered Restaurant IDs list (e.g. Past Ordered Restaurants, Featured Restaurants, Favorite Restaurants, Top Trending Restaurants etc.). Flat Restaurant Recommendations Payload FlatItemsRec All recommendations that only consist of a single ordered Item IDs list (e.g. Frequently Bought Together Items, View View Items, Similar Items, Frequently Bought Together Skus etc.). Flat Item Recommendations Payload *Note: Channel integration developer should take required implementation steps to ignore any recommendation types that they do not want to process . Recommendation Payload Types: Flat Restaurant Recommendations Payload Please ensure that you have gone through Response Schema section for the base JSON response structure before reading this section. { ""displayText"": ""(String)"", ""displayImage"": ""(String)"", ""restaurants"": [ { ""rank"": (Integer), ""restaurantId"": (Integer) } ] } displayText string The display text that the channel needs to show as the caption of this recommendation to the customer. displayImage string The display Image URL that the channel needs to show as the image of this recommendation to the customer. position int The position of the channel description string The display text that the channel needs to show as the description of this recommendation to the customer. listingImage string The listing Image URL that the channel needs to show as the image of this recommendation to the customer. restaurants array List of recommended restaurants. rank integer Restaurant rank. This rank is in ascending order (i.e. lowest rank must be considered as highest priority restaurant). restaurantId integer Restaurant ID. Flat Item Recommendations Payload Please ensure that you have gone through Response Schema section for the base JSON response structure before reading this section. { ""displayText"": ""(String)"", "" items "": [ { ""rank"": (Integer), "" itemId "": (Integer) } ] } displayText string The display text that the channel needs to show as the caption of this recommendation to the customer. items array List of recommended items. rank integer Item rank. This rank is in ascending order (i.e. lowest rank must be considered as highest priority item). itemId integer Item ID. Response Headers: With all HTTP 200(OK) responses, Food Recommendations API will include the following response header(s). Header Name Description FDR-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. FDR-BUNDLE-PARAMS Provides all the information the channel needs for tracking purposes of food recommendations. This response header content will follow the JSON schema given below. { ""cid"": ""(String)"", ""pgid"": ""(String)"", ""executedInfoList"": [ { ""plid"": ""(String)"", ""placementRev"": ""(String)"", ""infoMeta"": { ""type"": ""(String)"" }, ""infoPayload"": { ""executedAlgorithmsInfo"": { ""bundleId"": ""(String)"", ""bundleRev"": ""(String)"", ""isMvt"": (Boolean), ""mvtId"": ""(String)"", ""mvtRev"": ""(String)"", ""optimizerRev"": ""(String)"", ""autoOptimized"": (Boolean), ""optimizedMetric"": ""(String)"" ""executedAlgorithmInfoList"": [ { ""algoId"": ""(String)"", ""ccpParametersUsed"": [ ""(String)"", ""(String)"" ], ""isPersonalized"": (Boolean) } ] }, ""executedRulesInfo"": { ""executedRuleInfoList"": [ { ""ruleId"": ""(String)"", ""ruleType"": ""(String)"" } ], ""backfillDisabledFromRule"": (Boolean) }, ""recGenerationCycleStatus"": { ""recGenerationCycleMask"": (Long) } } } ] } cid string Channel id which sent the request for this response. pgid string Page id in the channel which sent the request for this response. executedInfoList array This array of executed information can contain one or more executed information objects, one for each placement requested in the request for this response. plid string Placement id in the channel page which sent the request for this response. placementRev string Placement revision number which will be useful for tracking purposes and auto-optimizer. infoMeta object Object containing metadata on how to interpret the infoPayload. type string Type of executed information payload included in the infoPayload (See Executed Information Payload Types sub-section below). infoPayload object Contains the actual executed information payload and schema can vary based on the type of executed algorithm bundle / use case. executedAlgorithmsInfo object Object containing information about the executed algorithm(s) to generate the recommendation response. bundleId string Algorithm Bundle Id which generated recommendations for this response. bundleRev string Bundle revision number which will be useful for tracking purposes and auto-optimizer. isMvt boolean Indicates whether the response was generated from a configured multi-variate test. mvtId string If isMvt is True, this will include the multi-variate test Id, else null. mvtRev string If isMvt is True, this will include the multi-variate test revision, else null. optimizerRev string Optimizer revision number which will be useful for tracking purposes and auto-optimizer. autoOptimized boolean Indicates whether this bundle was selected via the optimizer or not. optimizedMetric string Indicates the metric used by the optimizer for taking the selection decision for selecting this bundle for this execution round (e.g. CTR - Click Through Rate, RPI - Revenue Per Impression etc.). executedAlgorithmInfoList array This array will contain one or more executed algorithms that were executed to generate the response, in the order of execution . algoId string Algorithm Id. ccpParametersUsed array CCP parameters used for executing the algorithm. isPersonalized boolean Indicates whether the algorithm was able to apply personalization or not. executedRulesInfo object Object containing information about the executed business rules on the final result generated from the algorithms before sending out the response. executedRuleInfoList array This array will contain one or more executed rules. Here the order does not matter as the rules are executed such that the final result is always consistent. ruleId string Rule Id. ruleType string Rule type (e.g., Boost, Bury, Only Recommend etc.). backfillDisabledFromRule boolean Indicates whether backfill was disabled from a Manual recommendation rule (in which case the recommendations will be those manually configured by the business user). recGenerationCycleStatus object Object containing any debug signals. recGenerationCycleMask long A long integer, when converted to binary, each bit will indicate a debug signal which can be used to easily interpret the actions taken to generate the response. Executed Information Payload Types: Following the same dynamic structure as the Food Recommendations REST API’s response payload, the FDR-BUNDLE-PARAMS response header format also follows a similar dynamic structure. That is, Based on the, ""infoMeta"": { ""type"": ""(String)"" } The JSON schema of the, ""infoPayload"": { … } Can change. Here the “type” represents the same recommendation types introduced in sub-section Recommendation Types . For all these recommendation types the same infoPayload JSON schema included above will be used unless stated otherwise in the following table. In case a different infoPayload JSON schema is used for any recommendation type, the Reference column in this table will point to the section in this document which illustrates that schema and how to interpret that format. Recommendation Type (infoMeta => type) Same header JSON schema? (Yes/No) Reference FlatRestaurantsRec Yes N/A FlatItemsRec Yes N/A Error Response Schema Error responses from Food Recommendations API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Food Recommendations API will include the following response header(s). Header Name Description FDR-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors: All client initiated error responses from Food Recommendations API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": ""(String)"", ""message"": ""(String)"" } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in consumer side. Following table illustrates the possible error codes and corresponding error messages. Error Code Error Message FDR-1000 Missing mandatory parameter cid. FDR-1001 Missing mandatory parameter pgid. FDR-1002 Missing mandatory parameter plids. FDR-1100 Malformed placement IDs. FDR-1200 Malformed CCP. FDR-1300 Missing authentication details. FDR-1301 Throttling limit reached. Try again in few seconds. FDR-4xx Client error. Please send an email to l2support@pickme.lk with the FDR-CORRELATION-ID response header value. Server Errors: All server error responses from Food Recommendations API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": ""(String)"", ""message"": ""(String)"" } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in consumer side. As per best practices in not exposing the exact error that occurred in the server side to the caller, the error code and message will always be the following. Error Code Error Message FDR-5xx Server error. Please send an email to l2support@pickme.lk with the FDR-CORRELATION-ID response header value. Server side administrators will be able to analyze the server logs and take corrective actions to mitigate the given 5xx error. Recommendation Details Recommendation Name Algorithm ID Algorithm type Placement Meta values Request Body Parameters Sample request Previously Ordered 1000 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal1 customerId (Mandatory) dropLat (Optional) dropLon (Optional) Note : If dropLat and dropLon included, haversine filtering would be applied. curl -d ""{\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip>/foodrec/iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal1' Popular Restaurants 1001 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal2 dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal2' Newly Joined Restaurants 1002 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal3 dropLat (Optional) dropLon (Optional) Note : If dropLat and dropLon included, haversine filtering would be applied. curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal3' Top Picks 1 1003 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal4 customerId (Mandatory) dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal4' Top Picks 2 1004 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal5 customerId (Mandatory) dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal5' Top Picks 3 1005 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal6 customerId (Mandatory) dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal6' Top Picks 4 1006 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal7 customerId (Mandatory) dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal7' Top Picks 5 1007 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal8 customerId (Mandatory) dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal8' Recommended for you 1008 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal9 customerId (Mandatory) dropLat (Optional) dropLon (Optional) Note : If dropLat and dropLon included, haversine filtering would be applied. curl -d ""{\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal9' Must Try 1009 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal10 customerId (Mandatory) dropLat (Optional) dropLon (Optional) Note : If dropLat and dropLon included, haversine filtering would be applied. curl -d ""{\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal10' Family Friendly 1010 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal11 dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal11' Trending 1011 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal12 dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal12' Featured 1012 FlatRestaurantsRec cid : AndroidApp or IosApp pgid : Home plid : Horizontal13 dropLat (Mandatory) dropLon (Mandatory) curl -d ""{\""dropLat\"":\""6.87425\"",\""dropLon\"":\""79.8886\"",\""customerId\"":\""729\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Home&plids=Horizontal13' Frequently Bought Together 2000 FlatItemsRec cid : AndroidApp or IosApp pgid : Checkout plid : Horizontal1 productId (Mandatory) curl -d ""{\""dropLat\"":\""6.869\"",\""dropLon\"":\""79.891\"",\""customerId\"":\""729\"",\""productId\"":\""172428,253482,253863\""}"" -H ""Content-Type: application/json"" -H ""X-FDR-API-KEY: <API-KEY>"" -H ""X-FDR-API-SECRET: <API-SECRET>"" -X POST 'http://<server-ip> /foodrec /iserv/v1/food/recommendations?cid=IosApp&pgid=Checkout&plids=Horizontal1' Frequently Bought Together ( SKU id version ) 2002 FlatItemsRec cid : AndroidApp or IosApp pgid : Checkout plid : Horizontal2 productId (Mandatory) ( SKU Ids ) curl --location --request POST ' http://<server-ip> /foodrec/iserv/v1/food/recommendations?cid=AndroidApp&pgid=Checkout&plids=Horizontal2 ' --header ""X-FDR-API-KEY: <API-KEY>"" -- header ""X-FDR-API-SECRET: <API-SECRET>"" --header 'Content-Type: application/json' --data-raw '{""productId"":""8700""}'",False
539492425,page,current,/x/SQAoI,Azure HDInsight Provisioning,This section includes the PickMe Big Data and Data Science stack provisioning instructions in Microsoft Azure HDInsight.,False
539492438,page,current,/x/VgAoI,Setting up Resource Group,"A resource group is a logical grouping of one or more Azure resources. You can decide the criteria for this logical grouping as per your infra requirements. For the purpose of Big Data and Data Science platform at PickMe, resource group has been selected to demarcate the environment (E.g. dev, staging, prod). Follow the below steps to create a new Resource Group as the first step of provisioning a new Big Data and Data Science environment. Login to the Azure Portal. Click ""All Services"" => ""General"" => ""Resource Groups"". Click the ""Add"" option. Under ""Basics"" tab. Select your ""Subscription"". Provide a meaningful name to the Resource group. Select the ""Region"". Click the button ""Next: Tags"". Under ""Tags"" tab. Select the ""Environment"" tag name. Select the respective environment type as the Tag value (e.g. UAT or PROD). Click ""Review + Create"" button. Review the information in the review page carefully. Click the ""Create"" button. It may take couple of seconds for your resource group to be created. Once the new resource group is created you can locate it under ""Resource groups"" sub-section of the portal.",False
539492444,page,current,/x/XAAoI,Setting up the Storage Accounts,"HDInsight clusters use Azure Blob Storage Accounts to store all persistent data (e.g. log files, job inputs / outputs etc.). For a production multi cluster setup, any HDInsight cluster requires two blob storage accounts. Default storage account Shared storage account Default storage account is used for the cluster related log files, job completion / failure log files etc. Shared storage account is the shared storage space between multiple HDInsight clusters, so that the output of jobs running in one cluster can be accessed from other HDInsight clusters for additional processing. For example, data ingested from the Hadoop cluster should be accessible from the Spark cluster for Data Science / Machine Learning use cases. Following steps illustrate how to setup a Storage Account in Azure. Please ensure to use these same steps to setup two Storage Accounts as mentioned above. Login to the Azure Portal. Click ""All Services"" => ""Storage"" => ""Storage accounts"". Click ""Add"" option. Under ""Basics"" provide the following, Select the ""Subscription"". Select the ""Resource Group"". Provide a meaningful name for the ""Storage account name"" (ensure the name can easily distinguish between default and shared storage accounts). Select the ""Location"". Select ""Standard"" as the ""Performance"" tier. Select ""StorageV2 (general purpose v2)"" as the ""Account kind"". Select ""Locally-redundant storage (LRS)"" as the ""Replication"" criteria (other replication types will incur additional costs). Select ""Hot"" access tier. Click ""Next : Advanced"" button. Under ""Advanced"" provide the following, Select ""Enabled"" for the ""Secure transfer required"" option. Select ""Selected network"" for the ""Allow access from"" option and select the correct Virtual Network and Subnet. Select ""Enabled"" for ""Blob soft delete"" option. Set the required retain days for the soft deleted blobs. Click ""Next : Tags"" button. Under ""Tags"" section, Select the ""Environment"" tag name. Select the appropriate tag value. Click ""Review + create"" button. Review the storage account information carefully. Click the ""Create"" button. It may take several minutes for the storage account to be created. After the creation is successful, you will be able to see the newly created storage account under the ""Storage accounts"" section of the portal.",False
539492457,page,current,/x/aQAoI,Setting up Spark Cluster,"Follow the below steps for deploying a HDInsight Spark Cluster. Login to the Azure Portal. Click ""All Services"" => ""Analytics"" => ""HDInsight clusters"". Click ""Add"" option. In HDInsight cluster provisioning wizard, select ""Custom"". DO NOT use ""Quick create"" option. Under ""Basics"" section provide the following, A meaningful ""Cluster name"". Select the ""Subscription"". Click on ""Cluster type"" and in the sub options page, Select ""Spark"" from the ""Cluster type"" dropdown. Select ""Spark 2.4.2"" from the ""Version"" dropdown. Enter a meaningful ""Cluster login username"" ( IMPORTANT : keep this admin login name documented somewhere). Enter a strong password for the ""Cluster login password"" ( IMPORTANT : keep this password documented somewhere ). Enter a meaningful ""Secure Shell (SSH) username"" ( IMPORTANT : keep this ssh login name documented somewhere ). Select the ""Resource group"". Select the ""Location"". Click ""Select"" in the ""Cluster type"" sub options page. Click ""Next"" in the ""Basics"" page. Under ""Security + networking"" section provide the following, Keep the ""Enterprise Security Package"" option ""Disabled"" (this incurs additional cost). Select the ""Virtual network"". Select the ""Subnet"". Keep the entire ""Identity"" section options as default. Click ""Next"" button. Under ""Storage"" section provide the following, Select ""Azure Storage"" as the ""Primary Storage type"". Keep the ""Selection method"" as ""My subscriptions"". Click on ""Select a Storage account"" option and from the sub options page, click on the default storage account that you created for the usage of the HDInsight clusters (refer Setting up the Storage Accounts ). Provide a meaningful name for the ""Container"". This container will be created inside the provided default storage account and be used by the Spark cluster for storing all cluster specific files. ( IMPORTANT : this name must be unique and ensure to select a name that would reflect the cluster type and environment which makes the maintenance easy). In the same ""Storage"" section scroll down and, Click on the ""Additional storage accounts"". In the sub options page, click on ""Add a storage key"". In the next sub options page, click on ""Select a Storage account"". From the final sub options page, click on the shared storage account that you created for the usage of the HDInsight clusters (refer Setting up the Storage Accounts ). Click ""Select"" buttons in all sub options pages in the reverse order. In the same ""Storage"" section scroll down and, Select the Hive Metastore DB that was created for the usage in clusters from the dropdown ""Select a SQL database for Hive"". Click on ""Authenticate SQL Database"" and in the sub options page, Provide the admin username configured for the Hive Metastore DB. Provide the password for the admin username. Click ""Select"" button of the sub options page. From ""Select a SQL database for Oozie"" dropdown, select the option ""I don't want to provide a database"" option (i.e. we are not going to use Oozie service in HDInsight clusters). Click ""Next"" button. In the ""Applications (optional)"" section, just click the ""Next"" button (we are not going to install any optional applications). In the ""Cluster size"" section, Provide the number of Worker nodes (number of worker nodes can also be increased / decreased after cluster creation). Select the ""Worker node size"" (this must be based on the expected cluster work load). Select the ""Head node size"" (it is best to go with the suggested VM size for the head nodes from the portal or a higher spec'd VM than the suggested). IMPORTANT : Please note the total number of cores that will be used for this cluster which is shown in this section as an information label. Each Azure account is assigned a set number of cores that ALL HDInsight clusters can utilize. When a cluster is created, the number of cores used for that cluster (i.e. based on the VM types selected) is deducted from the total allocated cores count to the Azure account. Ensure that you have enough total cores remaining that is required for the new cluster provisioning, as otherwise the cluster provisioning will fail. To add more cores to your account, you need to create a support ticket from within the Azure portal. Click ""Next"" button. In the ""Script actions"" section just click ""Next"" button. We are not going to perform any script actions during the cluster provisioning. In the ""Summary"" section, review all the cluster information carefully and click the ""Create"" button. Cluster creation can take anywhere from 20 mins to 1 Hr to complete.",False
539492461,page,current,/x/bQAoI,Setting up Interactive Query Cluster,"Follow the below steps for deploying a HDInsight IQR Cluster. Login to the Azure Portal. Click ""All Services"" => ""Analytics"" => ""HDInsight clusters"". Click ""Add"" option. In HDInsight cluster provisioning wizard, select ""Custom"". DO NOT use ""Quick create"" option. Under ""Basics"" section provide the following, A meaningful ""Cluster name"". Select the ""Subscription"". Click on ""Cluster type"" and in the sub options page, Select ""Interactive Query"" from the ""Cluster type"" dropdown. Select ""Interactive Query 3.1"" from the ""Version"" dropdown. Enter a meaningful ""Cluster login username"" ( IMPORTANT : keep this admin login name documented somewhere). Enter a strong password for the ""Cluster login password"" ( IMPORTANT : keep this password documented somewhere ). Enter a meaningful ""Secure Shell (SSH) username"" ( IMPORTANT : keep this ssh login name documented somewhere ). Select the ""Resource group"". Select the ""Location"". Click ""Select"" in the ""Cluster type"" sub options page. Click ""Next"" in the ""Basics"" page. Under ""Security + networking"" section provide the following, Keep the ""Enterprise Security Package"" option ""Disabled"" (this incurs additional cost). Select the ""Virtual network"". Select the ""Subnet"". Keep the entire ""Identity"" section options as default. Click ""Next"" button. Under ""Storage"" section provide the following, Select ""Azure Storage"" as the ""Primary Storage type"". Keep the ""Selection method"" as ""My subscriptions"". Click on ""Select a Storage account"" option and from the sub options page, click on the default storage account that you created for the usage of the HDInsight clusters (refer Setting up the Storage Accounts ). Provide a meaningful name for the ""Container"". This container will be created inside the provided default storage account and be used by the IQR cluster for storing all cluster specific files. ( IMPORTANT : this name must be unique and ensure to select a name that would reflect the cluster type and environment which makes the maintenance easy). In the same ""Storage"" section scroll down and, Click on the ""Additional storage accounts"". In the sub options page, click on ""Add a storage key"". In the next sub options page, click on ""Select a Storage account"". From the final sub options page, click on the shared storage account that you created for the usage of the HDInsight clusters (refer Setting up the Storage Accounts ). Click ""Select"" buttons in all sub options pages in the reverse order. In the same ""Storage"" section scroll down and, Select the Hive Metastore DB that was created for the usage in clusters from the dropdown ""Select a SQL database for Hive"". Click on ""Authenticate SQL Database"" and in the sub options page, Provide the admin username configured for the Hive Metastore DB. Provide the password for the admin username. Click ""Select"" button of the sub options page. From ""Select a SQL database for Oozie"" dropdown, select the option ""I don't want to provide a database"" option (i.e. we are not going to use Oozie service in HDInsight clusters). Click ""Next"" button. In the ""Applications (optional)"" section, just click the ""Next"" button (we are not going to install any optional applications). In the ""Cluster size"" section, Provide the number of Worker nodes (number of worker nodes can also be increased / decreased after cluster creation). Select the ""Worker node size"" (this must be based on the expected cluster work load). Select the ""Head node size"" (it is best to go with the suggested VM size for the head nodes from the portal or a higher spec'd VM than the suggested). Select the ""Zookeeper node sizes"" (it is best to go with the suggested VM size for the head nodes from the portal or a higher spec'd VM than the suggested). IMPORTANT : Please note the total number of cores that will be used for this cluster which is shown in this section as an information label. Each Azure account is assigned a set number of cores that ALL HDInsight clusters can utilize. When a cluster is created, the number of cores used for that cluster (i.e. based on the VM types selected) is deducted from the total allocated cores count to the Azure account. Ensure that you have enough total cores remaining that is required for the new cluster provisioning, as otherwise the cluster provisioning will fail. To add more cores to your account, you need to create a support ticket from within the Azure portal. Click ""Next"" button. In the ""Script actions"" section just click ""Next"" button. We are not going to perform any script actions during the cluster provisioning. In the ""Summary"" section, review all the cluster information carefully and click the ""Create"" button. Cluster creation can take anywhere from 20 mins to 1 Hr to complete.",False
539590721,page,current,/x/QYApI,Setting up Hive Metastore DB,"Apache Hive is the Data Warehouse / Data Lake platform available in Azure HDInsight suite and is one of the world's most widely used Data Warehousing technologies. While Hive gets provisioned when deploying an HDInsight cluster, it is important to keep the Hive metastore external to the cluster as a cluster disaster will not affect the Hive metastore to go down. Additionally, external metastore is the only mechanism where multiple HDInsight clusters can share the same Hive layer. For more details on the architecture refer PickMe Big Data and Data Science Architecture V2.0 . Follow the below steps to setup the Hive Metastore DB as the third step of provisioning a new Big Data and Data Science environment. Login to the Azure Portal. Click ""All Services"" => ""Databases"" => ""SQL databases"". Under ""Basic"" tab, provide the following, Select the ""Subscription"". Select the ""Resource Group"". Provide a meaningful ""Database name"". Under ""Server"", click the ""Create new"" option. In the ""New server"" window, provide the following details. A meaningful ""Server name"". A meaningful name for the ""Server admin login"" ( IMPORTANT : keep this admin login name documented somewhere). Provide a strong ""Password"" for the Server admin login ( IMPORTANT : keep this password documented somewhere). Enter the same password in ""Confirm password"" box. Select the ""Location"" (should be the same location as all other resources in the resource group). Check the ""Allow Azure services to access server"" option (This will be later disabled when configuring the specific firewall rules of the server). Click the ""OK"" button of the ""New server"" page to add that server to the ""Basic"" information section. Select ""No"" for ""Want to use SQL elastic pool?"" option. Configure the required DB server size under ""Compute + storage"" depending on the sizing requirements. Click on ""Next : Additional settings"". In ""Additional settings"" select the following, For ""Use existing data"" select ""None"" (this options is only required if you are restoring a backup). Keep the ""Database Collation"" as the default value that is generated from the portal. For ""Enable Advanced Data Security"" select ""Not Now"". Click ""Next : Tags"" button. Under ""Tags"" section, Select the ""Environment"" tag name. Select the appropriate tag value. Click ""Next : Review + create"" button. Review all the information and click ""Create"" button. It may take couple of minutes for the new Database to be created. Once the new Database is created you can locate it under ""SQL databases"" sub-section of the portal. Checking the connectivity to the created database Find the newly created ""SQL server"" under the resource group and click on it. In the ""SQL server"" details page, click on ""Firewalls and virtual networks"". Enter your current public IP address to the rules section and click ""Save"" button. Firewall rule update may take few minutes. After the firewall rule update is complete successfully, click on the ""SQL databases"" option of the SQL server, and click on the database name. Make a note of the ""Server name"". Open ""SQL Server Management Studio"" in your local machine and enter the following details, Enter the ""Server name"" (noted from the previous point). Enter the ""Login"" username (enter the admin username provided during the DB provisioning). Enter the ""Password"" (enter the admin password provided during the DB provisioning). Press the ""Connect"" button. If the connection was successful, you should be able to see the Hive metastore DB successfully in your SQL Server Management Studio as shown below. Setting up backups Follow the below steps to setup the backups of the Hive metastore DB. In the ""SQL server"" overview page, click on ""Manage Backups"" option. Select the ""SQL database"" by ticking the checkbox and click on ""Configure retention"". Provide the required ""Daily"", ""Weekly"", ""Monthly"" and ""Yearly"" backup retention periods and press ""Apply"". Configuring Hive metastore DB to be accessed only from within the cluster virtual network servers Navigate to the ""Virtual Network"" overview page of the virtual network created under section . Click on the ""Service endpoints"" and click ""Add"" option. Select ""Microsoft.Sql"" from the ""Service"" dropdown. Select the cluster virtual network subnet created for the deployment of clusters under section . Click ""Add"" button. Navigate back to the Hive metastore DB ""SQL server"" and click on ""Firewalls and virtual networks"". Click ""Add existing virtual network"" option. Create a VNet rule in the firewall by entering the following details. Meaningful ""Name"" for the VNet rule. Select the ""Subscription"". Select the ""Virtual network"". Select the ""Subnet name"". Click ""OK"" button. As the final step, click on ""OFF"" option under ""Allow access to Azure services"" and click on ""Save"". Firewall update may take few minutes to complete and once done the Hive metastore DB provisioning is complete.",False
539590763,page,current,/x/a4ApI,Installing Hue on a HDInsight Cluster,"Hue is the workbench for the developers who are interacting with the HDInsight cluster. Follow the below steps to deploy Hue on a HDInsight cluster. Use these same steps to deploy Hue in all four HDInsight clusters (i.e. Hadoop, Spark, HBase, IQR). Navigate to the overview page of the HDInsight cluster and click on ""Script actions"". Click on ""Submit new"" option. In the ""Submit script action"" sub-options page, Select the ""Install Hue"" option from the ""Script type"" dropdown. Give a meaningful name to track the script action later in script action execution history. Check ""Head"" checkbox in the ""Node type(s):"" section. Uncheck ""Persist this script ... "" option. Click ""Create"" button. Hue installation will take couple of minutes to complete. After the Hue installation, most of the cluster services needs to be restarted. For this, visit Ambari link of the cluster and restart all affected services one by one. See sub-section under the section for further details.",False
539623496,page,current,/x/SAAqI,MySQL Server Provisioning for the usage of Azkaban Multi Executor Setup,"Azkaban multi executor mode is the recommended production deployment setup for Azkaban (refer link ). Azkaban multi executor mode requires a MySQL database, and the following steps illustrate how to deploy a MySQL database for this purpose in Azure. Login to the Azure portal. Navigate to ""All services"" => ""Databases"" => ""Azure Database for MySQL servers"". Click on ""Add"" option. In the ""Create MySQL server"" page, Select the ""Subscription"". Select the ""Resource Group"". Provide a meaningful ""Server name"". Select ""None"" for the ""Data source"" (we are creating an empty database). Provide an ""Admin username"" ( IMPORTANT : keep this admin username documented somewhere). Provide a strong ""Password"" for the admin username ( IMPORTANT : keep this password documented somewhere). Confirm the same password in ""Confirm password"" textbox. Select the ""Location"". Select the MySQL version ""5.7"". Click on ""Configure server"" option to setup the sizing of the server. In the server sizing page, Select the server tier (in this case, we are provisioning a ""General Purpose"" server). Select the ""Compute Generation"" (in this case, we are selecting a ""Gen 5""). Select the number of ""vCore""s (in this case, we are selecting 4 vCores). Select the amount of ""Storage"" (in this case, we are allocating ~200 GB). In the server sizing page scroll down, Select ""Yes"" for ""Auto-growth"". Select the required ""Backup Retention Period"". Select ""Locally Redundant"" option for ""Backup Redundancy Options"" (Geo-Redundant option will incur additional costs). Click ""OK"" button in the server sizing page. Click on ""Next : Tags"" button. Under ""Tags"" section, Select the ""Environment"" tag name. Select the appropriate tag value. Click ""Review + create"" button. Review all the information and click ""Create"" button. Server creation will take several minutes. Configuring the MySQL database to be accessible for Azkaban In the newly provisioned MySQL server's overview page, click on ""Connection security"". In the ""Connection security"" page, Select ""ON"" for ""Allow access to Azure services"" option. Click on ""Adding existing virtual network"" option. In the ""virtual network rule"" pop-up page, Provide a meaningful ""Name"" to the rule. Select the ""Subscription"". Select the ""Virtual network"" created for the cluster servers. Select the ""Subnet name"" created for the cluster servers. Click ""OK"" button in the pop-up page. In the ""Connection security"" page, Ensure that the newly added virtual network rule is appearing. Select ""DISABLED"" for ""Enforce SSL connection"" option (Azkaban currently does not support SSL). Click ""Save"" button. Now the MySQL database is completely setup and ready to be configured for Azkaban Multi Executor mode.",False
539721785,page,current,/x/OYArI,Setting up Hadoop Cluster,"Follow the below steps for deploying a HDInsight Hadoop Cluster. Login to the Azure Portal. Click ""All Services"" => ""Analytics"" => ""HDInsight clusters"". Click ""Add"" option. In HDInsight cluster provisioning wizard, select ""Custom"". DO NOT use ""Quick create"" option. Under ""Basics"" section provide the following, A meaningful ""Cluster name"". Select the ""Subscription"". Click on ""Cluster type"" and in the sub options page, Select ""Hadoop"" from the ""Cluster type"" dropdown. Select ""Hadoop 3.1.0"" from the ""Version"" dropdown. Enter a meaningful ""Cluster login username"" ( IMPORTANT : keep this admin login name documented somewhere). Enter a strong password for the ""Cluster login password"" ( IMPORTANT : keep this password documented somewhere ). Enter a meaningful ""Secure Shell (SSH) username"" ( IMPORTANT : keep this ssh login name documented somewhere ). Select the ""Resource group"". Select the ""Location"". Click ""Select"" in the ""Cluster type"" sub options page. Click ""Next"" in the ""Basics"" page. Under ""Security + networking"" section provide the following, Keep the ""Enterprise Security Package"" option ""Disabled"" (this incurs additional cost). Select the ""Virtual network"". Select the ""Subnet"". Keep the entire ""Identity"" section options as default. Click ""Next"" button. Under ""Storage"" section provide the following, Select ""Azure Storage"" as the ""Primary Storage type"". Keep the ""Selection method"" as ""My subscriptions"". Click on ""Select a Storage account"" option and from the sub options page, click on the default storage account that you created for the usage of the HDInsight clusters (refer ). Provide a meaningful name for the ""Container"". This container will be created inside the provided default storage account and be used by the Hadoop cluster for storing all cluster specific files. ( IMPORTANT : this name must be unique and ensure to select a name that would reflect the cluster type and environment which makes the maintenance easy). In the same ""Storage"" section scroll down and, Click on the ""Additional storage accounts"". In the sub options page, click on ""Add a storage key"". In the next sub options page, click on ""Select a Storage account"". From the final sub options page, click on the shared storage account that you created for the usage of the HDInsight clusters (refer ). Click ""Select"" buttons in all sub options pages in the reverse order. In the same ""Storage"" section scroll down and, Select the Hive Metastore DB that was created for the usage in clusters from the dropdown ""Select a SQL database for Hive"". Click on ""Authenticate SQL Database"" and in the sub options page, Provide the admin username configured for the Hive Metastore DB. Provide the password for the admin username. Click ""Select"" button of the sub options page. From ""Select a SQL database for Oozie"" dropdown, select the option ""I don't want to provide a database"" option (i.e. we are not going to use Oozie service in HDInsight clusters). Click ""Next"" button. In the ""Applications (optional)"" section, just click the ""Next"" button (we are not going to install any optional applications). In the ""Cluster size"" section, Provide the number of Worker nodes (number of worker nodes can also be increased / decreased after cluster creation). Select the ""Worker node size"" (this must be based on the expected cluster work load). Select the ""Head node size"" (it is best to go with the suggested VM size for the head nodes from the portal or a higher spec'd VM than the suggested). IMPORTANT : Please note the total number of cores that will be used for this cluster which is shown in this section as an information label. Each Azure account is assigned a set number of cores that ALL HDInsight clusters can utilize. When a cluster is created, the number of cores used for that cluster (i.e. based on the VM types selected) is deducted from the total allocated cores count to the Azure account. Ensure that you have enough total cores remaining that is required for the new cluster provisioning, as otherwise the cluster provisioning will fail. To add more cores to your account, you need to create a support ticket from within the Azure portal. Click ""Next"" button. In the ""Script actions"" section just click ""Next"" button. We are not going to perform any script actions during the cluster provisioning. In the ""Summary"" section, review all the cluster information carefully and click the ""Create"" button. Cluster creation can take anywhere from 20 mins to 1 Hr to complete.",False
539721798,page,current,/x/RoArI,Setting up HBase Cluster,"Follow the below steps for deploying a HDInsight HBase Cluster. Login to the Azure Portal. Click ""All Services"" => ""Analytics"" => ""HDInsight clusters"". Click ""Add"" option. In HDInsight cluster provisioning wizard, select ""Custom"". DO NOT use ""Quick create"" option. Under ""Basics"" section provide the following, A meaningful ""Cluster name"". Select the ""Subscription"". Click on ""Cluster type"" and in the sub options page, Select ""HBase"" from the ""Cluster type"" dropdown. Select ""HBase 2.0.0"" from the ""Version"" dropdown. Enter a meaningful ""Cluster login username"" ( IMPORTANT : keep this admin login name documented somewhere). Enter a strong password for the ""Cluster login password"" ( IMPORTANT : keep this password documented somewhere ). Enter a meaningful ""Secure Shell (SSH) username"" ( IMPORTANT : keep this ssh login name documented somewhere ). Select the ""Resource group"". Select the ""Location"". Click ""Select"" in the ""Cluster type"" sub options page. Click ""Next"" in the ""Basics"" page. Under ""Security + networking"" section provide the following, Keep the ""Enterprise Security Package"" option ""Disabled"" (this incurs additional cost). Select the ""Virtual network"". Select the ""Subnet"". Keep the entire ""Identity"" section options as default. Click ""Next"" button. Under ""Storage"" section provide the following, Select ""Azure Storage"" as the ""Primary Storage type"". Keep the ""Selection method"" as ""My subscriptions"". Click on ""Select a Storage account"" option and from the sub options page, click on the default storage account that you created for the usage of the HDInsight clusters (refer Setting up the Storage Accounts ). Provide a meaningful name for the ""Container"". This container will be created inside the provided default storage account and be used by the HBase cluster for storing all cluster specific files. ( IMPORTANT : this name must be unique and ensure to select a name that would reflect the cluster type and environment which makes the maintenance easy). In the same ""Storage"" section scroll down and, Click on the ""Additional storage accounts"". In the sub options page, click on ""Add a storage key"". In the next sub options page, click on ""Select a Storage account"". From the final sub options page, click on the shared storage account that you created for the usage of the HDInsight clusters (refer Setting up the Storage Accounts ). Click ""Select"" buttons in all sub options pages in the reverse order. In the same ""Storage"" section scroll down and, Select the Hive Metastore DB that was created for the usage in clusters from the dropdown ""Select a SQL database for Hive"". Click on ""Authenticate SQL Database"" and in the sub options page, Provide the admin username configured for the Hive Metastore DB. Provide the password for the admin username. Click ""Select"" button of the sub options page. From ""Select a SQL database for Oozie"" dropdown, select the option ""I don't want to provide a database"" option (i.e. we are not going to use Oozie service in HDInsight clusters). Uncheck the ""Enable HBase Accelerated Writes"" option. Click ""Next"" button. In the ""Applications (optional)"" section, just click the ""Next"" button (we are not going to install any optional applications). In the ""Cluster size"" section, Provide the number of Worker nodes (number of worker nodes can also be increased / decreased after cluster creation). Select the ""Worker node size"" (this must be based on the expected cluster work load). Select the ""Head node size"" (it is best to go with the suggested VM size for the head nodes from the portal or a higher spec'd VM than the suggested). Select the ""Zookeeper node sizes"" (it is best to go with the suggested VM size for the head nodes from the portal or a higher spec'd VM than the suggested). IMPORTANT : Please note the total number of cores that will be used for this cluster which is shown in this section as an information label. Each Azure account is assigned a set number of cores that ALL HDInsight clusters can utilize. When a cluster is created, the number of cores used for that cluster (i.e. based on the VM types selected) is deducted from the total allocated cores count to the Azure account. Ensure that you have enough total cores remaining that is required for the new cluster provisioning, as otherwise the cluster provisioning will fail. To add more cores to your account, you need to create a support ticket from within the Azure portal. Click ""Next"" button. In the ""Script actions"" section just click ""Next"" button. We are not going to perform any script actions during the cluster provisioning. In the ""Summary"" section, review all the cluster information carefully and click the ""Create"" button. Cluster creation can take anywhere from 20 mins to 1 Hr to complete.",False
542572575,page,current,/x/HwBXI,Enterprise View of Customer (EVoC) API,"Revision History Date Version Change Request Date Description Author 2019-08-26 1.0.0 N/A Initial version Bathiya Priyadarshana References Reference Version Date Remarks Passenger Loyalty BRS N/A N/A Link: https://docs.google.com/presentation/d/12lrIiH_UOWWtMfngCjQd9VpBRuokhgKil5sKAbo7Jko/edit?usp=sharing Introduction Enterprise View of Customer (EVoC) API is one of the major APIs hosted in the PickMe data backend. This API will provide all information related to PickMe customers including but not limited to customer loyalty, customer demographics, customer propensities, customer segment information etc. Based on the use cases, the API is divided into two parts as Events API and RESTful API. EVoC Events API All events under EVoC Events API are published to one or more Kafka topics. EVoC Events API consist of the following sub APIs. EVoC RESTful API All information exposed under EVoC RESTful API are exposed by one or more RESTful APIs. EVoC RESTful API consist of the following sub APIs. TBD",False
542670866,page,current,/x/EoBYI,Enterprise View of Driver (EVoD) API,,False
543686676,page,current,/x/FABoI,Hue,"Hue is the workbench for the developers who are interacting with the HDInsight cluster. Although Hue has many capabilities to interact with cluster services, due to standardization among more capable / specific tools for some other tasks, Hue is only used as the Datawarehouse workbench at PickMe. Additional capabilities of Hue can be found in Hue's official site at link . This guide is only focused in documenting the following. Accessing Hue of a HDInsight cluster . Hue's Hive Query Editor . Hue's Hive Metastore Viewer .",False
543752222,page,current,/x/HgBpI,Edge Node,"Edge node is the entry point to a HDInsight cluster. It is a special node in the cluster with all the client tools related to cluster services (e.g. Hive, Spark, HBase) pre-installed. This allows you to issue commands using these client tools to perform various processing / operations on cluster services, such as performing HDFS commands, Hive commands, Spark commands etc. To SSH into the Edge node of a HDInsight cluster perform the following tasks. Navigate to the respective HDInsight cluster's overview page in Azure portal and click on ""SSH + Cluster login"" option. Click the expand option under Hostname dropdown. Select the Edge node's hostname from the dropdown. Click the indicated copy button. When you paste the copied content to a text editor, you will notice that it is the complete SSH command to SSH into the Edge node. Use any SSH client of your choice to SSH into the Edge node using the SSH command details mentioned above. How to use Edge node's client tools to perform various processing / operations on cluster services will be covered under different sub-sections mentioned below.",False
543752226,page,current,/x/IgBpI,Spark Shell,"HDInsight Spark clusters provide an easy to use REPL for quick Spark debugging and R&D purposes that developers can gain advantage of. This REPL comes in two flavors, namely, Scala REPL. Python REPL. In order to access Spark Shell of a HDInsight Spark cluster, you need to access the Edge node of the respective cluster. Use the steps mentioned in the Edge Node page to SSH into the Edge node of the Spark cluster. Once you have access to the Edge node, you either invoke the Spark Scala REPL or the Spark Python REPL as mentioned below. Spark Scala REPL Issue the command spark-shell in the Edge node CLI as below. In few seconds, you should be able to see the ""scala>"" prompt as below which means you are inside the Spark Scala REPL shell. You can issue / write any Spark Scala program inside the REPL and verify it's functionality. To gracefully exit the Spark Scala REPL shell, issue the command :quit as indicated below. Spark Python REPL Issue the command pyspark in the Edge node CLI as below. In few seconds, you should be able to see the "">>>"" prompt as below which means you are inside the Spark Python REPL shell. You can issue / write any Spark Python program inside the REPL and verify it's functionality. To gracefully exit the Spark Python REPL shell, issue the command quit() as indicated below.",False
543752307,page,current,/x/cwBpI,Azkaban Flow Status Notification Emails Setup,"Azkaban allows notification emails to be sent out when a configured flow successfully completed or failed. Follow the below steps to enable this email notification ability of Azkaban. SSH into the cluster's Edge node and from there, SSH into the Head Node in which Azkaban Web Server is installed. Navigate to the Azkaban Web Server ""conf"" directory. Open ""azkaban.properties"" file. Scroll down the ""azkaban.properties"" file and configure the following, Configure the ""mail.sender"". Configure the ""mail.host"". Configure the ""mail.user"". Configure the ""mail.password"". Configure ""mail.tls"" as true . Save and exit from the ""azkaban.properties"" file. Navigate to the Azkaban Web Server ""bin"" directory. Shutdown the Azkaban Web Server as below. Remove the old Azkaban log file. Start the Azkaban Web Server again as below.",False
543784970,page,current,/x/CoBpI,"Access, Maintenance and Monitoring of HDInsight Cluster Components","This section includes the PickMe Big Data and Data Science stack access, maintenance and monitoring related details.",False
543784976,page,current,/x/EIBpI,Beeline,"Beeline is the new official CLI client for interacting with Hive. Previous Hive CLI is deprecated and it is highly recommended to switch over to Beeline. In order to access Beeline CLI of a HDInsight cluster, you need to access the Edge node of the respective cluster. Use the steps mentioned in the Edge Node page to SSH into the Edge node. After you SSH into the Edge node, issue the command beeline as indicated below. If any username and/or passwords are prompted, simply press ""Enter"" key. If your connection to Hive was successful, then Beeline will show the ""Connected to:"" message as indicated in below screen. Once you have the successful connection, you can issue any valid Hive command in the Beeline shell. To gracefully exit the beeline shell, issue the command !quit in the beeline shell as indicated below and press ""Enter"" key. For the complete reference manual of Beeline and it's features, please refer Beeline's official documentation at this link .",False
543850526,page,current,/x/HoBqI,HBase Shell,"HBase shell is the de-facto CLI tool available in HDInsight HBase clusters which can used for both DDL and DML operations on HBase. In order to access HBase shell of a HDInsight HBase cluster, you need to access the Edge node of the respective cluster. Use the steps mentioned in the Edge Node page to SSH into the Edge node. After you SSH into the Edge node, issue the command hbase shell as indicated below. In few seconds, you should be able to see the ""hbase(main)>"" prompt as below which means you have successfully launched the HBase shell. You can issue the help command to view the list of available commands. In addition, this link provides a good overview of the most widely used HBase shell commands. To gracefully exit the HBase shell, issue the command exit as indicated below.",False
543916062,page,current,/x/HoBrI,Azkaban,"As mentioned in the Azkaban provisioning section , Azkaban is an open sourced workflow manager developed by LinkedIn. Official site of Azkaban can be found here . Azkaban is only accessible via a SOCKS5 tunnel to the cluster head node. Follow the same steps explained under Accessing Ambari of a HDInsight cluster guide's SOCKS5 tunnel based access to Ambari section to establish a SOCKS5 tunnel to the cluster and configure Firefox browser proxy settings to route traffic via this SOCKS5 tunnel. Then, as same as accessing Ambari via the SOCKS5 tunnel to the cluster using Firefox, use the headnode hostname and port 8081 to access Azkaban running in the respective HDInsight cluster. E.g. => http://<head_node_hostname>:8081 NOTE: If the first head node hostname gives a ""Connection Reset"" error page, then simply try by changing the hostname to the second head node's hostname and try the same 8081 port. If all steps up to now were performed successfully, you should see the following login page of Azkaban. Enter your Azkaban username and password and click the ""Login"" button. Under ""Projects"" tab you will see all the projects created by yourself, as well as other users and given you access permission. You can create a new project by clicking on ""Create Project"" button. In Azkaban, project is a placeholder where you can upload multiple flows. When you click on a particular project name from the previous page, you will be able to see all the flows uploaded to that project as shown below. You can download the entire project, or upload a new version using the ""Download"" and ""Upload"" buttons respectively. You can click on the name of a flow to view the DAG of that flow. From this screen you can also schedule the flow (i.e. to run at a given schedule) as well as manually run the flow. For this, click on the ""Schedule / Execute Flow"" button. Under ""Executions"" tab in the same above page, you can see the previous execution history of that particular flow. If you want to view the details of any particular execution, you can click on the ""Execution Id"" from the execution history table. When you click on a particular execution Id as mentioned above, you will see the DAG execution status of that execution run as shown below. Any failed jobs will be marked in ""Red"". You can click the ""Flow Log"" tab to view the overall execution log for that execution run. When you click on the ""Job List"" tab of the same above execution of the flow, you will be able to see the job level breakdown details of the particular execution run. To view the logs of a particular job for that execution run, click on the indicated job level ""Log"" option as below. When you click on the job level log option as mentioned above, you will be able to see that particular job's logs for that execution run. Any flows that you schedule to run by Azkaban will appear under the main ""Scheduling"" tab in Azkaban. Currently executing jobs will appear under the main ""Executing"" tab in Azkaban. History tab will list the entire execution history of all flows executed by this Azkaban server. Intention of this guide was to provide a simple introduction to Azkaban capabilities and basic navigation in the Azkaban UI. This guide is by no means a comprehensive guide to Azkaban usage. Azkaban UI is quite intuitive and the user can quickly learn the features and capabilities by navigating in the various UI sections. For comprehensive details about Azkaban, always refer the Azkaban site / documentation linked at the beginning of this guide.",False
543916109,page,current,/x/TYBrI,Configuring PySpark to run with Python3 in the Spark Cluster,"Given the version of the HDInsight Spark cluster you installed, PySpark maybe configured to run with Python 2.x. You can verify this by SSH into the Edge node of the Spark cluster and opening a PySpark shell. Follow the below steps to setup PySpark to run with Python3. Run the command ""which python3"" in the Edge node and note down the path to python3. Go to Ambari portal of the Spark cluster and click on ""Spark2 => CONFIGS => Advanced spark2-env"". Scroll down and find the ""content"" section. Scroll down this section till you find the export statement for ""PYSPARK_PYTHON"" variable. Modify that variable value from existing value to the python3 path identified above (you may keep the old statement commented as shown in below screenshot). Click on ""Save"" button. When you press the ""Save"" button, Ambari may pop-up a change tracking pop-up box where you are requested to describe the change you did. If this pop-up appears, enter a description such as ""Changed PySpark to run on Python3"" and proceed. If any confirmation dialog is shown, confirm and proceed. Restart Spark2 service from Ambari portal (refer sub-section of for further details on this component restarting).",False
543948813,page,current,/x/DQBsI,Ambari,"Apache Ambari is one of the de-facto web based tools which makes Hadoop management simpler. It provides an intuitive UI to provision, manage and monitor a Hadoop based distributed cluster. Additional details about Ambari can be found in the official Apache Ambari site . Microsoft Azure HDInsight clusters also use Ambari as it's management and monitoring interface, thus it is important to anyone using HDInsight clusters to learn how to use Ambari. The intention of this guide is not to provide an extensive user guide to Ambari which is already documented and can be found in this link . The intention of this guide is to give a basic overview of, Accessing Ambari of a HDInsight cluster . . . .",False
543948826,page,current,/x/GgBsI,HDFS,"HDFS is the distributed file system of a Hadoop environment. In HDInsight clusters, HDFS API is made available ""as-is"" and is backed by the Azure Blob storage as the default storage mechanism. While all HDFS commands in the HDFS API are supported in HDInsight clusters, the format of the file system paths must follow the Azure Blob storage naming conventions as mentioned below. In order to access HDFS API of a HDInsight cluster, you need to access the Edge node of the respective cluster. Use the steps mentioned in the page to SSH into the Edge node. After you SSH into the Edge node, you can issue any HDFS commands as shown below. As mentioned above, please note the file path naming convention used. I.e. wasbs://<container_name>@<FQDN_of_storage_account>/<any_sub_directory_path> E.g. wasbs://hive-db-raw@hdindevshrprimpkme.blob.core.windows.net/food_tags You can issue any valid HDFS command using the above approach (e.g. -mkdir, -rm etc.). For a complete list of HDFS API commands, please refer the official HDFS documentation at this link .",False
544047401,page,current,/x/KYFtI,Configuring Spark Cluster to connect to the Shared Hive Service,"HDInsight clusters are underneath Hortonworks Data Platform instances, and Hortonworks Data Platform provides a feature rich approach known as Hive Warehouse Connector to integrate Spark with Hive. A short article about the use-cases enabled by this Hive Warehouse Connector can be found in this link . Follow the below steps to setup the Hive Warehouse Connector in HDInsight clusters. Refer to get more information about how this Hive Warehouse Connector fits into the overall PickMe Big Data and Data Science architecture. SSH into the Edge node of the HDInsight IQR cluster and from there, SSH into the Head Node 0 of the IQR cluster. Open the /etc/hosts file in the IQR cluster Head Node 0. Copy the entire IP address to hostnames bindings list. Then SSH into the Edge node of the HDInsight Spark cluster and from there, SSH into the Head Node 0 of the Spark cluster. Open the /etc/hosts file in the Spark cluster Head Node 0. Paste the host entry list copied from the IQR Head Node 0 to the end of this Spark Head Node 0 hosts file as below. Save and exit from the hosts file of the Spark Head Node 0. Next login to the Ambari of Spark cluster and navigate to ""Spark2 => CONFIGS => Advanced spark2-defaults"". Scroll down the expanded config section till you locate the "" spark. sql. hive. metastore. jars"" config. Change the value of this spark. sql. hive. metastore. jars to /usr/hdp/current/hive_warehouse_connector/* as below. Next navigate to ""Spark2 => CONFIGS => Custom spark2-defaults"" in Ambari. Under this following six new configuration properties need to be added. To add any new property, scroll to the bottom of the property section and click on ""Add Property..."" option. Six new configuration properties to add, spark. datasource. hive. warehouse. load. staging. dir spark. datasource. hive. warehouse. metastoreUri spark. hadoop. hive. llap. daemon. service. hosts spark. hadoop. hive. zookeeper. quorum spark. security. credentials. hiveserver2. enabled spark. sql. hive. hiveserver2. jdbc. url Following is the method to derive the value for above six configuration properties. spark. datasource. hive. warehouse. load. staging. dir This must be a directory where both Spark cluster and IQR cluster has access to. Thus, this is simply a container location inside the shared storage account that was created and attached to all the HDInsight clusters when provisioning the clusters (refer cluster provisioning sections for additional information). spark. security. credentials. hiveserver2. enabled Simply set this property to the value false . spark. sql. hive. hiveserver2. jdbc. url The format of this connection string is, jdbc:hive2:// <IQR Cluster Name> .azurehdinsight.net:443/;user= <IQR Cluster Admin Username> ;password= <IQR Cluster Admin Password> ;ssl=true;transportMode=http;httpPath=/hive2 Other three remaining configuration property values must be extracted from the IQR cluster Ambari. Login to Ambari of IQR cluster. Navigate to ""Hive => CONFIGS => ADVANCED"". Under ""General"" section, Scroll down till you find the property named hive. metastore. uris (see below). This is the value you have to copy-paste into spark. datasource. hive. warehouse. metastoreUri property of the Spark cluster Ambari mentioned previously. In IQR cluster Ambari ""Advanced hive-site"" section, Scroll down till you find the property named hive. zookeeper. quorum (see below). This is the value you have to copy-paste into spark.hadoop.hive.zookeeper.quorum property of the Spark cluster Ambari mentioned previously. In IQR cluster Ambari ""Advanced hive-interactive-site"" section, Scroll down till you find the property named hive. llap. daemon. service. hosts (see below). This is the value you have to copy-paste into spark.hadoop.hive.llap.daemon.service.hosts property of the Spark cluster Ambari mentioned previously. With all above six configuration properties correctly set in Spark cluster Ambari, click the ""Save"" button to save and apply the configurations. When you press the ""Save"" button, Ambari may pop-up a change tracking pop-up box where you are requested to describe the change you did. If this pop-up appears, enter a description such as ""Integrated Spark to Shared Hive Service"" and proceed. If any confirmation dialog is shown, confirm and proceed. Restart Spark2 service from Ambari portal (refer Ambari sub-section of Access, Maintenance and Monitoring of HDInsight Cluster Components for further details on this component restarting).",False
544047988,page,current,/x/dINtI,Accessing Ambari of a HDInsight cluster,"There are two ways to access Ambari of a HDInsight cluster. Via public internet (some functionalities will not work in this access method). Via a SOCKS5 tunnel to the cluster head node (this will provide access to all the functionalities of Ambari). Public internet based access to Ambari Follow the below steps to access Ambari via public internet. Login to the Azure portal. Navigate to the respective HDInsight cluster's overview page. Click on the highlighted cluster URL. Browser will prompt for the credentials. Provide the HDInsight cluster admin username and password and press ""Sign in"" button. If the credentials were successful, you will be directed to the Ambari landing page shown below. SOCKS5 tunnel based access to Ambari Follow the below steps to establish a SOCKS5 tunnel to the respective HDInsight cluster's head node and then use a SOCKS5 proxy enabled browser to access the Ambari via this tunnel. This guide shows how to use Putty as the tunneling tool and Firefox as the SOCKS5 enabled browser. Login to the Azure portal. Navigate to the respective HDInsight cluster's overview page and click on ""SSH + Cluster login"" option. Select the cluster's general hostname from the ""Hostname"" dropdown. Copy the highlighted hostname portion of the SSH command. Open Putty and do the following, Paste the hostname portion copied from the previous step as the ""Host Name"" in Putty. Enter the ""Port"" as 22. Select the ""Connection type"" as ""SSH"". Next in Putty, click on the SSH + sign to expand it and from the sub-options click on ""Tunnels"". In the new Putty configuration page do the following, Provide a source port to be used to bind the tunnel to localhost (e.g. 9876). Click ""Add"" button to add that port to the ""Forwarded ports"" section as indicated below. For the ""Destination"", again provide the same cluster hostname copied previously. Select ""Dynamic"" option. Click ""Open"" button. When prompted, enter the respective HDInsight cluster's SSH username and the password. If credential were accurate, you will be SSH into the head node 0 of the cluster. Type the command ""hostname -f"" and copy the hostname of the head node 0. Next open the Firefox browser and go to ""Options"" and search for ""proxy"". As shown below, click on the ""Settings..."" button where ""proxy"" settings are located. In the appearing ""Connection Settings"" pop-up window do the following, Select ""Manual proxy configuration"". As ""SOCKS Host"" provide ""localhost"". Provide port for the ""SOCKS Host"" as ""9876"" (i.e. the same port number you bound the Putty tunnel to localhost). Check the checkbox ""Proxy DNS when using SOCKS v5. Click ""OK"" button. Next on the address bar of the browser, type the URL in the format, http://<Head Node 0 Hostname>:8080 In case you see a connection reset error page as below, it means Ambari is running on the second head node. In above case, simply change the head node hostname in the URL to the second head node (generally hn1), and refresh the page. Then you will shown the ""Sign in"" page of Ambari as below. Enter the cluster admin username and password and press the ""SIGN IN"" button. If credentials were successful, you will be directed to the Ambari landing page shown below.",False
544080574,page,current,/x/vgJuI,Basic navigation in the Ambari UI,"Immediate landing page that you see when you login to Ambari is the ""Dashboard"" page ""Metrics"" view. This view includes various widgets that shows cluster load and health related metrics. On the left hand navigation menu, you will be able to see all the services installed in this HDInsight cluster. These services will differ based on the type of HDInsight cluster you provisioned. You can click on any service to navigate to the page dedicated to that service. When you click on any service, you will see the Summary details page for that service. Also notice the Configs tab which you can use to change the config values particular to that service. In the right hand side Quick Links section, you will be able to find links to any other related tools deployed in the cluster particularly for various operations related to the selected service. When you select Configs tab of the given service, you will be able to see all the configuration parameters corresponding to that service categorized into sub-sections (e.g. Settings, Database, Advanced in the below screenshot). Any alerts related to a given service will appear as indicated in the below screenshot. You can click on the ""Bell icons"" shown to get additional details about the alert. The ""Bell icon"" at the top right hand corner will aggregate and show all alerts corresponding to all running services. To view the cluster Hosts, you can click on the ""Hosts"" option in the left hand navigation menu. To view the details corresponding to a particular host, you can click on that host. In the host details page corresponding to a particular host, you can see all the cluster components running in that particular host. Additionally, you can click on the Configs tab to view the service configurations of the services running in that particular host. Alerts tab includes the alerts corresponding to that particular host. Host metrics related widgets are also shown in this page. Scroll down this Host details section to view the physical details corresponding to this particular host.",False
544080578,page,current,/x/wgJuI,Basic usage for commonly used management functionalities,"You can see the maintenance actions you can perform on a particular service under the ""ACTIONS"" drop-down as shown below. Please note the ""Turn On Maintenance Mode"" option under the actions drop-down. Ensure to enable this maintenance mode prior to performing any maintenance actions (e.g. restart, start, stop etc.), to avoid unnecessary alerts from getting triggered when the maintenance actions are performed. The respective service's configurations are located under the ""CONFIGS"" tab of that service. As you see in the below screenshot, a service will consist of many configuration sub-sections. You can click on the sub-section name to expand the options under that sub-section. If you change a configuration under one or more sub-sections, you can either discard or save those changed configurations by clicking the respective ""DISCARD"" or ""SAVE"" button. Under the ""Hosts"" page, you can click the action button (3 DOTs) of a given component to view the actions you can perform on that component running in that given cluster server. Here also ensure to turn on the maintenance mode before performing any maintenance actions to avoid any unnecessary triggers from getting fired. To view the actions you can perform to a given Host at global level, you can click on the ""HOST ACTIONS"" drop-down in the Hosts page of a particular cluster server. Here also ensure to turn on the maintenance mode before performing any maintenance actions to avoid any unnecessary triggers from getting fired. Under the ""Service Auto Start"" page, you can select / deselect the services that you want Ambari to monitor and auto-restart in case of a failure. If you make a change to the settings in this page, ensure to click on the ""DISCARD"" or ""SAVE"" button to revert the changes or save the changes respectively. It is recommended to keep all the cluster services marked for auto-restart.",False
544113140,page,current,/x/9IFuI,Basic usage for commonly used monitoring functionalities,"All primary monitoring widgets of the respective cluster are located under Ambari Dashboard ""METRICS"" tab as shown in the below screenshot. Some services (e.g. HDFS, YARN etc.) have their own dedicated metrics pages. As shown in the below screenshot, these service level metrics pages are located in that particular service's page ""METRICS"" tab. Cluster servers have their own metrics section when you navigate to that specific cluster server's page under ""Hosts"". ""Alerts"" page lists all the alerts that are defined in Ambari for various services / components running in the cluster. You can click on the name of any alert definition to view the details page specific to that alert definition. When you navigate to the specific page of a single alert, you can click on the ""EDIT"" option as indicated below to change that particular alert's configurations. When you scroll down the same above page, you will be able to see the instances of that alert triggering. On the alert list page, you can easily enable / disable any alert as indicated in below screenshot. To view the alert related actions, you can click on the ""ACTIONS"" drop down in the alert list page. One of the important actions you can perform in this actions drop down is to ""Manage Notifications"". When you click on ""Manage Notifications"" from the previous screen, you will be shown the below options page. You can click on the ""+"" button to create a new alert notification. In the ""Create Alert Notification"" page shown below, you can configure the alert notification settings. There are few alerting methods, out of which ""EMAIL"" is the easiest and most common way of alert notifications. As shown below you can specify all email alert related parameters and click the ""SAVE"" button to enable that email alert notification.",False
553549853,page,current,/x/HYD_I,Hive Query Editor,"Hive Query Editor is one of the two primary query editors available in Hue (other being the Pig Query Editor which PickMe team will not be using). You can use the Hive Query Editor to issue any valid Hive query to the Hive running in the HDInsight cluster and also view the results of that query in a tabular view format. Below steps show how you can use the Hive Query Editor in Hue. After you successfully login to the Hue of a HDInsight cluster, you can click on the ""Query Editors"" menu as indicated below. From the drop down, select Hive. In the Hive query editor, You will be able to see the currently selected database. You can click on the database drop down icon to view all the databases available. When you select a database, in the tables section just below the database drop down, you will be able to see all the tables in that database. You can view the query input window as indicated by the arrow in the screenshot. All recently executed queries will be listed in the ""Recent queries"" tab. To execute a query, Select the correct database from the database drop down. Enter the query you want to execute into the query editor window. Click the ""Execute"" button. When the query execution is completed successfully, you will be able to view the results in the ""Results"" tab.",False
553549859,page,current,/x/I4D_I,Accessing Hue of a HDInsight cluster,"Hue is only accessible via a SOCKS5 tunnel to the cluster head node. Follow the same steps explained under guide's SOCKS5 tunnel based access to Ambari section to establish a SOCKS5 tunnel to the cluster and configure Firefox browser proxy settings to route traffic via this SOCKS5 tunnel. Then, as same as accessing Ambari via the SOCKS5 tunnel to the cluster using Firefox, use the headnode hostname and port 8888 to access Hue running in the respective HDInsight cluster. E.g. => http://<head_node_hostname>:8888 NOTE: If the first head node hostname gives a ""Connection Reset"" error page, then simply try by changing the hostname to the second head node's hostname and try the same 8888 port. If all steps up to now were performed successfully, you should see the following login page of Hue. Enter the ""Username"" and ""Password"" and click ""Sign in"" button to access Hue.",False
553549863,page,current,/x/J4D_I,Hive Metastore Viewer,"Hive metastore viewer is the graphical interface to the Metastore DB of Hive running in the HDInsight cluster. You can use this view of Hue to identify what databases are available in Hive, and all table schema related information for each database listed. Below steps show how you can use the Hive Metastore Viewer in Hue. After you successfully login to the Hue of a HDInsight cluster, you can click on the ""Metastore Manager"" menu option as indicated below. In the metastore manager page, you can see the currently selected database in the ""Database"" drop down. You can also click the drop down button and select any listed database. Once the database is selected, the Tables section indicated below will list all the tables residing in that database. You can click on a given table to get schema information about that table. When you click on a given hive table, the schema details page of that particular table will be shown as below. In the ""Columns"" tab, you will be able to see the column names, data type per column and comments (if any were provided during table creation). You can click on the ""Partition Columns"" tab to view any partition columns available in the table. Sample tab will always produce the following error which is a bug in the current version of Hue that is available for HDInsight clusters. Alternatively, you can use the Hive Query Editor mentioned under this section to execute any Hive query you need. Finally, under the properties tab, you will be able to see all additional Hive properties corresponding to that selected table.",False
559087796,page,current,/x/tABTIQ,Tier Renewal Event,"Revision History Date Version Change Request Date Description Author 2019-08-26 1.0.0 N/A Initial version Bathiya Priyadarshana References Reference Version Date Remarks Kafka Topic Details Topic Name passenger_loyalty Event Name tier_renewal Event Message Structure { ""id"": ""<UNIQUE_ID>"", // string - Unique UUID which is unique for each event ""type"": "" tier_renewal "", ""created_at"": <UNIX_TIMESTAMP>, // long - Event created Unix timestamp in milliseconds ""body"": { ""customer_id"": <CUSTOMER_ID> // long - Customer Id ""renewal_type"": ""<TYPE_OF_RENEWAL>"", // string - Can be either JUMPED / DROPPED / NOCHANGE ""old_tier"": ""<TIER>"", // string - Previous tier of the customer before the renewal event occurred. Can be either BLUE / BRONZE / SILVER / GOLD / PLATINUM. ""new_tier"": ""<TIER>"", // string - New tier of the customer after the renewal event occurred. Can be either BLUE / BRONZE / SILVER / GOLD / PLATINUM . ""point_balance"": <DOUBLE>, // double - Customer's remaining loyalty points balance after the renewal event ""point_deduction"": <DOUBLE>, // double - Number of points deducted as a result of renewal event ""next_renewal_date"": ""<yyyy-MM-dd>"" // string - Next tier renewal date in the format yyyy-MM-dd } }",False
559087800,page,current,/x/uABTIQ,Tier Secured Event,"Revision History Date Version Change Request Date Description Author 2019-08-26 1.0.0 N/A Initial version Bathiya Priyadarshana References Reference Version Date Remarks Kafka Topic Details Topic Name passenger_loyalty Event Name tier_secured Event Message Structure { ""id"": ""<UNIQUE_ID>"", // string - Unique UUID which is unique for each event ""type"": ""tier_secured"", ""created_at"": <UNIX_TIMESTAMP>, // long - Event created Unix timestamp in milliseconds ""body"": { ""customer_id"": <CUSTOMER_ID> // long - Customer Id ""current_tier"": ""<TIER>"", // string - Customer's current tier. Can be either BRONZE / SILVER / GOLD / PLATINUM. ""secured_tier"": ""<TIER>"" // string- Tier that the customer secured. Can be either BRONZE / SILVER / GOLD. } }",False
559087989,page,current,/x/dQFTIQ,Enterprise View of Merchant (EVoM) API,,False
559251518,page,current,/x/PoBVIQ,Customer Loyalty Events API,"Revision History Date Version Change Request Date Description Author 2019-08-26 1.0.0 N/A Initial version Bathiya Priyadarshana References Reference Version Date Remarks All events under Customer Loyalty Events API will follow the below base event structures for event key and event body. Event Key Structure { ""key"": <CUSTOMER_ID> // long - Customer Id } Event Body Structure { ""id"": ""<UNIQUE_ID>"", // string - Unique UUID which is unique for each event ""type"": ""<EVENT_TYPE>"", // string - daily_snapshot / tier_renewal / tier_secured / threshold_reached ""created_at"": <UNIX_TIMESTAMP>, // long - Unix timestamp in milliseconds ""body"": {...} // object - Content differs based on the EVENT_TYPE } Customer Loyalty Events API includes the following events. Please refer the individual events sub-sections for additional details regarding these events.",False
559251605,page,current,/x/lYBVIQ,Daily Snapshot Event,"Revision History Date Version Change Request Date Description Author 2019-08-26 1.0.0 N/A Initial version Bathiya Priyadarshana References Reference Version Date Remarks Kafka Topic Details Topic Name passenger_loyalty Event Name daily_snapshot Event Message Structure { ""id"": ""<UNIQUE_ID>"", // string - Unique UUID which is unique for each event ""type"": ""daily_snapshot"", ""created_at"": <UNIX_TIMESTAMP>, // long - Event created Unix timestamp in milliseconds ""body"": { ""customer_id"": <CUSTOMER_ID> // long - Customer Id ""current_tier"": ""<TIER>"", // string - Customer's current tier. Can be either BLUE / BRONZE / SILVER / GOLD / PLATINUM. ""current_tier_points"": <DOUBLE>, // double - Points required to retain current tier ""current_points"": <DOUBLE>, // double - Customer's current loyalty points ""target_tier"": ""<TIER>"", // string - Customer's target tier. Can be either BRONZE / SILVER / GOLD / PLATINUM. ""target_tier_points"": <DOUBLE>, // double - Points required to move to target tier ""highest_secured_tier"": ""<TIER>"", // string - Customer's currently secured highest tier. Can be either BLUE / BRONZE / SILVER / GOLD. ""next_renewal_date"": ""<yyyy-MM-dd>"" // string - Next tier renewal date in the format yyyy-MM-dd } }",False
559284482,page,current,/x/AgFWIQ,Threshold Reached Event,"Revision History Date Version Change Request Date Description Author 2019-08-26 1.0.0 N/A Initial version Bathiya Priyadarshana References Reference Version Date Remarks Kafka Topic Details Topic Name passenger_loyalty Event Name threshold_reached Event Message Structure { ""id"": ""<UNIQUE_ID>"", // string - Unique UUID which is unique for each event ""type"": "" threshold_reached "", ""created_at"": <UNIX_TIMESTAMP>, // long - Event created Unix timestamp in milliseconds ""body"": { ""customer_id"": <CUSTOMER_ID> // long - Customer Id ""threshold_type"": ""<THRESHOLD_CODE>"", // string - Type of threshold reached. Can be either ABOUT_TO_FALL / ABOUT_TO_JUMP. ""remaining_days"": <INT>, // int - Number of days remaining for threshold to expire ""percentage_away"": <DOUBLE>, // double - Percentage points away from reaching the set threshold ""points_away"": <DOUBLE> // double - Points away from reaching the set threshold } }",False
565936309,page,current,/x/tYC7IQ,Mappings,This section includes all the numeric mappings with their meanings that are used throughout the PickMe Data infrastructure.,False
565936313,page,current,/x/uYC7IQ,Booking Type,Defines how a certain booking is initiated. Booking Type ID Booking Types 0 Dispatcher 1 Device/App 2 Road Pick Up 3 Kiosk 4 Corporate Portal,False
566526091,page,current,/x/i4DEIQ,Ride Table Column Descriptions,Revenue - Commission from the trip to PickMe Trip Fare - Total amount paid by the customer to the driver - Amount after promotion Fare - Total fare of the trip - without the promotion applied Distance - Trip distance *distance on the Trip table is inaccurate,False
566689865,page,current,/x/SQDHIQ,Rejected by Driver,Driver rejection type mapping ID Description 0 Driver Time Out 1 Driver Rejected 2 System Rejection,False
566853709,page,current,/x/TYDJIQ,Branch,Branch ID to name mappings. ID Name 1 Colombo 2 Kandy 3 Negombo 4 Kalutara 5 Panadura 11 Gampaha 15 Galle 17 Matale 18 Kurunegala 19 Kegalle 20 Matara 21 Ja Ela 22 Male 23 Hulhule 24 Hulhumale,False
566853731,page,current,/x/Y4DJIQ,Food Order Status,Order status mappings. ID Description 1 queued 2 driver assigned 3 accepted 4 arrived at pickup point 5 order placed 6 order picked up 7 arrived at destination 8 order delivered 9 job complete 10 no driver available 11 cancelled 12 Merchant Accepted 13 Merchant Declined 14 Auto Canceled 15 Queue Accepted 16 Queue Declined 17 Prep time expired 18 Order ready,False
566886608,page,current,/x/0ADKIQ,Ride Hailing,This contains all mappings related to ride hailing.,False
566919208,page,current,/x/KIDKIQ,Cancel Reason,Trip cancel reason mappings. ID Description 1 Changed my mind 2 Driver refuse to come 3 Driver is late 4 I got a lift 5 Driver too far 6 Driver asked to cancel 7 Other 31 Changed my mind 33 I got a lift 36 Other,False
566984753,page,current,/x/MYDLIQ,Food Payment Methods,Payment module mapping. ID Description 1 Cash 2 Card 3 Points,False
567017620,page,current,/x/lADMIQ,Taxi Model,Identification of taxi models by service type.,False
567476252,page,current,/x/HADTIQ,Travel Status,The description of TravelStatusSK which is also the referred as TripStatus SK.,False
567574544,page,current,/x/EIDUIQ,Company ID,CompanyID used for testing purposes. These company ID's are used for testing purpose only and should be removed when mining for actual ride hailing data.,False
568819790,page,current,/x/ToDnIQ,Food,This contains mappings of all food related tables.,False
568983578,page,current,/x/GgDqIQ,Trip Table Column Description,The following columns in Trip table are inaccurate. Distance Waiting time Cost CompanySK CompanyID Join with Dim.Driver and on DriverSK to filter testing company 1 and 2. Following are accurate Pick up time - the time the passenger requested for a vehicle (in terms of pre-booking) Actual pick up time - the time the passenger was actually picked up Created date SK - time a trip was created ( the time when the passenger placed the request for a pre-booked trip) Dispatch time - time when the pre-booked trip was dispatched by the dispatcher,False
569114685,page,current,/x/PQDsIQ,Ride Payment Method,Show the passenger payment method descriptions for ride hailing IPAY - Payments made by LOLC wallet UPAY - Sanawa Development Bank Fuel Card - Payments made through dialog by dialog employees only ALIPAY - Payments through Alibaba GENIE - Payments made through dialog for general public,False
598573154,page,current,/x/YoCtIw,Meter Calculations,"meter_calculations description of features distance - actual distance traveled during the trip from start to end. - recorded in meters waiting - after the trip starts, the time the vehicle remains stopped till the end of the trip - recorded in seconds duration - the time of the trip from start to end - this includes the waiting time distance_fare - fare based on distance duration_fare - fare based on duration (small amount like 50 cents per min) additional_fare - surcharge plus other additional charges waiting_fare - for the time the vehicle was stopped during trip the fare (this is a significant amount like rs 4 per min) total_fare - amount actually paid by the customer (this may be the upfront fare shown, or else the meter calculation if the passenger got down ahead of where he actually put the drop location. Several parameters are considered to identify if the meter fare or the up front fare is used.) estimated_distance - the distance showed when booking the vehicle, based on the OSRM distance estimated_duration - the duration showed when booking the vehicle estimated_fare - based on estimated distance and duration (does not include a waiting fare) and any additional fares known at the beginning of the trip actual_distance_to_pickup - loss mileage waiting_till_pick_up - time from trip accepted to trip started.",False
609353827,page,current,/x/YwBSJ,Insights into HDInsight,"Date of talk: 2019/12/18 Place of talk: Microsoft Sri Lanka Event: Sri Lanka Data Community Meetup of December 2019 Content: What is Azure HDInsight How is Azure HDInsight built Anatomy of an HDInsight cluster How does the deployed HDInsight cluster look like What are Edge Node, Hue and Azkaban Using quick start templates Using HDInsight cluster script actions Deploying Azkaban on HDInsight clusters Connecting Spark clusters with Interactive Query cluster for getting maximum out of Spark and Hive in HDInsight Link to the presentation",False
609353836,page,current,/x/bABSJ,Big Data Sphere,Date of talk: 2019/11/06 Place of talk: PickMe Engineering Center Event: PickMe internal tech talk sessions Content: What is Big Data What is Hadoop Why Hadoop was invented What Hadoop brings to the table Hadoop Architecture HDFS YARN MapReduce framework Limitations of Hadoop What is Spark Why is Spark important Spark features Spark components Resilient Distributed Dataset (RDD) RDD operations Limitations of RDD DataFrame Spark stage Directed Acyclic Graph (DAG) Catalyst Optimizer Query execution phases Limitations of Spark Spark optimal coding practices Link to the presentation,False
609878018,page,current,/x/AgBaJ,Tech Talks,This section includes the tech talks our team has performed. Introduction to Machine Learning (By Bathiya) Insights into HDInsight (By Bathiya) Big Data Sphere (By Suchitha) How Data Science is Transforming Modern Day Decision Making (By Pulara),False
609878025,page,current,/x/CQBaJ,Introduction to Machine Learning,Date of talk: 2019/09/07 Place of talk: University of Colombo School of Computing (UCSC) Event: IntelliHack 2019 Content: What is machine learning Types of machine learning Data science skills Typical machine learning workflow Tools for machine learning An introduction to machine learning with Spark Linear regression with PySpark Link to the presentation,False
611057702,page,current,/x/JgBsJ,How Data Science is Transforming Modern Day Decision Making,Date of talk: 2019/05/30 Place of talk: University of Colombo School of Computing (UCSC) Event: PickMe external tech talk session Content: Explosion of data and reducing cost From data mining to knowledge discovery Evolving to Data Science How Data Science contributes to improve your PickMe experience - discussion of several use cases Link to the presentation,False
629047420,page,current,/x/fIB_JQ,Driver Incentives API,"Revision History Date Version Change Request Date Description Author 2020-01-02 1.0.0 N/A Initial version Bathiya Priyadarshana 2020-01-03 1.1.0 N/A Introduced new option 'e (expired)' to the status parameter in the request to distinguish incentives without an achievement. Completed & Expired incentives will be retained in the system for 7 days. Added currencyCode attribute to both elgInfo and achInfo objects in the txwkrcnt incentive type response schema. Added completion ratio target (compRatioTgt) to the elgInfo objects in the txwkrcnt incentive type response schema. Added next achievement calculation date time (nextAchDateTime) to the achInfo objects in the txwkrcnt incentive type response schema. Added corresponding HTTP codes to the error responses. Updated the final example to reflect newly added attributes to the response. Bathiya Priyadarshana 2020-05-26 1.2.0 N/A Introduced the incTitle attribute to the response. Updated relevant sections to introduce Food Weekly Job Count Based incentive. Bathiya Priyadarshana References Reference Version Date Remarks Driver Incentive Models by Driver Care N/A N/A Link Una Incentive Models N/A N/A Link Driver Incentive by Mithila N/A N/A Link Driver Incentive Screen Mocks in Driver App N/A N/A Link Introduction Driver Incentives API is one of the major APIs hosted in the PickMe data backend under Enterprise View of Driver (EVoD) API domain. This API will provide a one time integration architecture where new incentive types can be made available with no / minimal changes to the consumer code. Currently the API is facilitated as a RESTful API. Request Schema Following table includes the request schema of the Driver Incentives REST API. URL http://<endpoint_addr>/iserv/v1/evod/ {driverId} /incentives Description Service to request driver incentives based on driver id and other filtering parameters. Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters: Parameter Name State Type Values Description Remarks {driverId} Mandatory Path Any valid driver id Id of the driver whose incentives are being requested. E.g. http://<endpoint_addr>/iserv/v1/evod/ 821546 /incentives None type Optional String all (default) txwkrcnt fdwkrcnt Type of incentive(s) being requested. Multiple types can be specified as a comma separated list. E.g. type=txwkrcnt%2Ctxwecnt all - indicates to include all types ( default ) txwkrcnt - taxi model and weekday ride count based incentive fdwkrcnt - food weekly job count based incentive. In this type, week is considered as from Monday 00:00:00 to next Sunday 23:59:59 (i.e. entire 7 days). Note 1: Listed are only the current supported types, and new types will be added based on the new incentive types business comes up with. Note 2: Multiple types can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 3: Any unknown types will be silently ignored by the API (i.e. no error will be thrown). If all specified types are unknown, API will return an empty incentives array in the response payload. Note 4: If ‘all’ is specified among other types in the comma separated list of types, ‘all’ will be given priority and any other specific types will be ignored. I.e. response will be generated as though only ‘all’ or no type is specified in the request. txModel Optional Integer -99 (default) -1 Any valid taxi model Taxi model type(s) for which the incentive list must be returned for by the API. Multiple types can be specified as a comma separated list. E.g. txModel=1%2C2%2C4 -99 - any taxi model including non taxi model linked incentives ( default ) -1 - specified to include non taxi model linked incentives Any other valid taxi model(s) integer will apply filtering to return only the incentives applicable for that taxi model(s). Note 1: Multiple taxi model values can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 2: Any unknown taxi model values will be silently ignored by the API (i.e. no error will be thrown). If all specified taxi model values are unknown, API will return an empty incentives array in the response payload. Note 3: If ‘-99’ is specified among other taxi models in the comma separated list of taxi models, ‘-99’ will be given priority and any other specific taxi models will be ignored. I.e. response will be generated as though only ‘-99’ or no taxi model is specified in the request. Note 4: There may be certain incentive types that are not linked to a specific taxi model. These incentives will only be included into the response if the txModel parameter is not specified (or -99) or includes -1 as one of the provided taxi models. Please refer following examples, txModel= 1 %2C 2 %2C 4 (only return incentives linked to taxi models 1, 2 and 4) txModel= -1 %2C 1 %2C 2 %2C 4 (only return incentives linked to taxi models 1, 2 and 4, but also include all incentives that are not linked to a taxi model) status Optional String a (default) p f c e Status(es) of the incentives the API must return. Multiple statuses can be specified as a comma separated list. status=p%2Cf%2Cc a - any ( default ) p - progressing f - future c - completed (i.e. with an achievement) e - expired (i.e. without any achievement) Note 1: Multiple statuses can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 2: Any unknown statuses will be silently ignored by the API (i.e. no error will be thrown). If all specified statuses are unknown, API will return an empty incentives array in the response payload. Note 3: If ‘a’ is specified among other statuses in the comma separated list of statuses, ‘a’ will be given priority and any other specific statuses will be ignored. I.e. response will be generated as though only ‘a’ or no status is specified in the request. Note 4: Completed & Expired incentives will only be exposed by the API for 7 days from the actual completion calculation in the incentive backend. elgInfo Optional Boolean true (default) false Indicates whether the incentive objects contained in the incentives array in the response should include the elgInfo object or not. If set to false, then all the incentive objects in the incentives array in the response will contain elgInfo object’s value as null . E.g. elgInfo=false None. achInfo Optional Boolean true (default) false Indicates whether the incentive objects contained in the incentives array in the response should include the achInfo object or not. If set to false, then all the incentive objects in the incentives array in the response will contain achInfo object’s value as null . E.g. achInfo=false None. sort Optional String +startDateTime -startDateTime +endDateTime -endDateTime +achByDateTime -achByDateTime Indicates the sort order the incentives in the response must be sorted based on. Note 1: Please note that there is no default value for this parameter. I.e. if not specified, the returned incentives will be in no particular order. Note 2: + indicates ascending order and - indicates descending order. offset Optional Integer 0 (default) Any >0 integer value Used by the API to support pagination. Any <0 values will be considered as 0. limit Optional Integer not specified (default) Any >0 integer value Used by the API to support pagination. Note 1: If not specified (i.e. default), all incentives from the specified offset will be returned. Note 2: Any <0 values will be considered as the default (i.e. will be ignored and considered as limit is not specified). Note 3: If the specified limit is larger than the remaining incentives from the specified offset, then all incentives from the specified offset will be returned. Note 4: If the specified offset is the index of the last incentive object, then the specified limit will be ignored. Request Headers: Following table illustrates the Request Headers that must be included in the Driver Incentives API. Header Name State Example Value Description X-EVOD-INC-API-KEY Mandatory androidprod API key issued to the consumer by the Big Data team. Must be kept in consumer's config as value will be changed in a breach. X-EVOD-INC-API-SECRET Mandatory feaee65a-2f54-4b18-8a7e-ea409a2e0d72 API secret issued to the consumer by the Big Data team. Must be kept in consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 223e1203-0237-4770-89f6-31d65843f4c1 If provided, the given Correlation Id will be logged with all log messages logged by the Driver Incentives Service in executing this particular request. This will enable deeper issue analysis from source to driver incentives backend. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Driver Incentives Service will generate it's own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Schema HTTP 200(OK) response schema of Driver Incentive API will follow the base JSON response structure indicated below. { ""payload"": { ""driverId"": (Integer), ""incentives"": [ { ""rank"": (Integer), ""incId"": ""(String)"", ""incTitle"": ""(String)"", ""incMeta"": { ""type"": ""(String)"" }, ""incPayload"": {…} } ] } } payload object Contains the entire response payload. driverId integer Id of the driver to whom the incentives in this response is applicable. incentives array This array of incentives can contain zero or more incentives applicable for the given driver. rank integer Used to specify the sort order of the given incentive in ascending order. incId string Unique id assigned to the given incentive in the incentive backend. This id will be unique across all incentives globally. incTitle string Title string to display to the driver which will give a textual indication what this incentive is. (See Incentive Title sub-section below). incMeta object Object containing metadata on how to interpret the incPayload. type string Type of incentive included in the incPayload (See Incentive Types sub-section below). incPayload object Contains the actual incentive payload and schema will vary based on the type of incentive (See Incentive Payload Types sub-section below). Incentive Title: The following table contains the current incentive title structures for each incentive category. For the Weekday, Weekend incentives, The title would be a fixed one. And the Daily Incentives and Fixed Duration title would be a dynamic one which added through the driver portal. Incentives Category Title Structure Hailing Weekly Incentives “Taxi Weekly Ride Count” Hailing Weekend Incentives “Taxi Weekend Ride Count” Food Weekly Incentives ""Food Weekly Job Count - <District comes here>"" One Day Incentives “All Oneday Ride Count - <Oneday Incentives Title comes here> "" Fixed Duration Incentives ""Food Fixed Duration Job Count - <Fixed Duration Incentives Title comes here> - <District comes here>"" Incentive Types: The following table contains the current incentive types supported by Driver Incentives Service. This table also indicates the relevant Incentive Payloads sub-sections which provides the exact incPayload schema for each supported incentive type. New incentive types can be added in the future based on the use cases. Incentive Type (incMeta => type) Description Related incPayload sub-section txwkrcnt Taxi model and weekday ride count based incentive. Taxi Model and Weekday Ride Count Based Incentive Payload fdwkrcnt Food weekly job count based incentive. In this type, week is considered as from Monday 00:00:00 to next Sunday 23:59:59 (i.e. entire 7 days). Taxi Model and Weekday Ride Count Based Incentive Payload **Same incPayload structure of txwkrcnt incentive type is used to ease the integration. However, any irrelevant attributes will be always null or 0 (e.g. fraudTrips, itcTrips, hfcTrips, rppTrips). *Note: Integration developer should take required implementation steps to ignore any incentive types that they do not want to process or does not know how to process . This allows for a future-proof integration where the Driver Incentives Service may add new incentive types, but the integrated party will not break and will only start consuming such new types when the new code required for processing such new types are added to the integrated party binaries. Incentive Payload Types: Taxi Model and Weekday Ride Count Based Incentive Payload (incMeta => type => txwkrcnt OR incMeta => type => fdwkrcnt) Please ensure that you have gone through the Response Schema section for the base JSON response structure before reading this section. { ""taxiModel"": (Integer), ""elgInfo"": { ""strategy"": ""(String)"", ""startDateTime"": ""(yyyy-MM-dd HH:mm:ss)"", ""endDateTime"": ""(yyyy-MM-dd HH:mm:ss)"", ""currencyCode"": ""(String)"", ""compRatioTgt"": (Double), ""minTgt"": (Integer), ""minAmt"": (Double), ""maxTgt"": (Integer), ""maxAmt"": (Double), ""targets"": [ { ""rank"": (Integer), ""tgt"": (Integer), ""amt"": (Double) } ] }, ""achInfo"": { ""achByDateTime"": ""(yyyy-MM-dd HH:mm:ss)"", ""nextAchDateTime"": ""(yyyy-MM-dd HH:mm:ss)"", ""currencyCode"": ""(String)"", ""maxAchTgt"": (Integer), ""payAmt"": (Double), ""kpis"": { ""totalTrips"": (Integer), ""totalLegTrips"": (Integer), ""fraudTrips"": (Integer), ""itcTrips"": (Integer), ""hfcTrips"": (Integer), ""rppTrips"": (Integer), ""compRatio"": (Double) } } } taxiModel integer Taxi model to which this ride count based weekday incentive is valid for. elgInfo object Object containing information about the eligibility of this ride count based weekday incentive. strategy string Incentive strategy. startDateTime string in format yyyy-MM-dd HH:mm:ss Start date and time for the incentive. endDateTime string in format yyyy-MM-dd HH:mm:ss End date and time for the incentive. currencyCode string Currency code applicable to all corresponding monetary amounts in elgInfo object. compRatioTgt double Completion ratio target to maintain to achieve the incentive. minTgt integer Minimum ride count target to achieve the incentive. minAmt double Payment for achieving the minimum target. maxTgt integer Maximum ride count target that this incentive is qualified to achieve. maxAmt double Payment for achieving the maximum target. targets array Array containing all ride count targets this incentive is qualified to achieve. rank integer Sort order of the given target in ascending order. tgt integer Target ride count. amt double Payment for achieving the target. achInfo object Object containing information about the achievement of this ride count based weekday incentive. achByDateTime string in format yyyy-MM-dd HH:mm:ss Date and time till when the incentive achievement was calculated. nextAchDateTime string in format yyyy-MM-dd HH:mm:ss Date and time of the next achievement calculation. currencyCode string Currency code applicable to all corresponding monetary amounts in achInfo object. maxAchTgt integer Maximum achieved ride count target. 0 will indicate that at least the minimum target has not being achieved. payAmt double Payable amount for achieving the maximum achieved target. Will be 0 if maxAchTgt is 0. kpis object Objecting containing the driver performance KPIs used to calculate the achievement. totalTrips integer Total trips completed during the incentive period. totalLegTrips integer Total legitimate trips qualifying for the achievement of the target during the incentive period. fraudTrips integer Number of trips the fraud detection algorithm has marked as fraud trips during the incentive period. itcTrips integer Number of ITC trips during the incentive period. hfcTrips integer Number of HFC trips during the incentive period. rppTrips integer Number of Road Pickup trips during the incentive period. compRatio double Completion ratio during the incentive period. Response Headers: Header Name Description EVOD-INC-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. EVOD-INC-TOTAL-COUNT Indicates the total number of incentives available for the given driver id. Error Response Schema Error responses from Driver Incentives API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Driver Incentives API will include the following response header(s). Header Name Description EVOD-INC-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors: All client initiated error responses from Food Recommendations API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": ""(String)"", ""message"": ""(String)"" } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in consumer side. Following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code EVOD-INC-1300 Missing authentication details. 401 EVOD-INC-1301 Throttling limit reached. Try again in few seconds. 401 EVOD-INC-4xx Client error. Please send an email to l2support@pickme.lk with the EVOD-INC-CORRELATION-ID response header value. Any valid client error code as defined in link 10.4 Client Error section. Server Errors: All server error responses from Driver Incentives API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": ""(String)"", ""message"": ""(String)"" } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in consumer side. As per best practices in not exposing the exact error that occurred in the server side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code EVOD-INC-5xx Server error. Please send an email to l2support@pickme.lk with the EVOD-INC-CORRELATION-ID response header value. Any valid client error code as defined in link 10.5 Server Error section. An example request and response Following is an example request with all the optional parameters specified in the request. http://<endpoint_addr>/iserv/v1/evod/825147/incentives?type=txwkrcnt&txModel=-1%2C1%2C2&status=p%2Cf&elgInfo=true&achInfo=true&sort=+endDateTime&offset=0&limit=10 Please also note the following header values included into the above request. X-EVOD-INC-API-KEY: androidprod X-EVOD-INC-API-SECRET: feaee65a-2f54-4b18-8a7e-ea409a2e0d72 X-CORRELATION-ID: 223e1203-0237-4770-89f6-31d65843f4c1 In the above request the requester has requested, Incentives for the driver id 825147. The requested incentive type is “taxi model and weekday ride count based incentive” ( type=txwkrcnt ). The request has requested for taxi models 1 and 2, and has also requested to include any non taxi linked incentives ( txModel=-1%2C1%2C2 ). The request has requested for incentives that are currently in progress as well as occurring in future ( status=p%2Cf ). The request has requested to include the elgInfo objects in the response for incentives ( elgInfo=true ). The request has requested to include achInfo objects in the response for incentives ( achInfo=true ). The request has requested to sort the incentives based on the incentives end date time in ascending order ( sort=+endDateTime ). The request has requested to include the incentives from the first index ( offset=0 ). The request has requested to only return the first 10 incentives from the first index ( limit=10 ). And following is an example response for above request. { ""payload"": { ""driverId"": 825147, ""incentives"": [ { ""rank"": 0, ""incId"": ""998c149fd226f55861d852c95a4e7d672d964d9dd048d261b312d3c275e30647"", ""incTitle"": ""Taxi Weekly Ride Count - Colombo"", ""incMeta"": { ""type"": ""txwkrcnt"" }, ""incPayload"": { ""taxiModel"": 1, ""elgInfo"": { ""strategy"": ""Strategy1"", ""startDateTime"": ""2019-12-16 00:00:00"", ""endDateTime"": ""2019-12-20 23:59:59"", ""currencyCode"": ""LKR"", ""compRatioTgt"": 60.00, ""minTgt"": 20, ""minAmt"": 2500.00, ""maxTgt"": 120, ""maxAmt"": 15000.00, ""targets"": [ { ""rank"": 0, ""tgt"": 20, ""amt"": 2500.00 }, { ""rank"": 1, ""tgt"": 30, ""amt"": 4000.00 }, { ""rank"": 2, ""tgt"": 50, ""amt"": 7500.00 }, { ""rank"": 3, ""tgt"": 75, ""amt"": 10000.00 }, { ""rank"": 4, ""tgt"": 100, ""amt"": 12000.00 }, { ""rank"": 5, ""tgt"": 120, ""amt"": 15000.00 } ] }, ""achInfo"": { ""achByDateTime"": ""2019-12-19 00:00:00"", ""nextAchDateTime"": ""2019-12-20 00:00:00"", ""currencyCode"": ""LKR"", ""maxAchTgt"": 75, ""payAmt"": 10000.00, ""kpis"": { ""totalTrips"": 84, ""totalLegTrips"": 78, ""fraudTrips"": 0, ""itcTrips"": 0, ""hfcTrips"": 4, ""rppTrips"": 2, ""compRatio"": 61.25 } } } } ] } } Please also note the following header values included into the above response. EVOD-INC-CORRELATION-ID: 223e1203-0237-4770-89f6-31d65843f4c1 EVOD-INC-TOTAL-COUNT: 1",False
742031420,page,current,/x/PIA6L,Pre - booked trip fulfilment analysis,"Findings of the pre-booked trip fulfillment analysis. November, December of 2019 and January 2020 months were used for data exploration. All three months indicated the same pattern across all features considered for the analysis. Each month has around 30,000 pre-booked trip requests made. As per Figure 1, 88% of all pre-booking trips were fulfilled, about 10% (3,300) of trips were cancelled by driver or dispatcher and about 2.2% (660) trips were cancelled by the passenger after accepting. Close to 80% of all pre-booked trip requested are generated via the corporate portal. The distribution of trips: By the day of the week By taxi model By pick up and drop off locations Details on the attached report. Also, the quality of fulfillment was checked based on the delay to pick up from the expected pick up time. While there are delays as large as 23 hours from expected pick up time, which is unlikely, majority of the trips were completed with the following delays. Other details such as the notice time before the expected pick up hour was analysed for app and corporate portal separately. More details on this can be found in the document.",False
742031427,page,current,/x/Q4A6L,PickMe - Hailing,Identification of shareable rides Shuttle utilization simulation results for selected route https://pickme.atlassian.net/wiki/spaces/BDDS/pages/743112705/Customer+Value+Matrix Driver Fraud Detection Algorithm Trip fare prediction model Pre - booked trip fulfilment analysis Taxi Driver Pooling,False
742031434,page,current,/x/SoA6L,PickMe - Food,,False
742031441,page,current,/x/UYA6L,Food order pooling potential,"One months worth of food orders from 2019-11-16 to 2019-12-16 were used to identify food order pooling potential. The document outlines the data used and the steps carried out to identify the pooling potential. Only the results of the top 20 average selling clustered are documented here. It gives a break down of the top 20 pickup cluster and drop cluster pairs as well as the hour of order completion which show the highest average no of orders across the selected dates. Furthermore, you can find the graphs indicating the variation of the average no of orders based on the hour between the above top 20 pickup and drop clusters. The food pooling simulation results are as follows. The table shows the average of pooled orders as a fraction of the total orders for the tops selling 10 restaurants on based on weekday sales. The average pooling potential across restaurants vary between 15% - 44% . Simulation results based on 30 day historical data. (30 days prior 23rd Jan) Simulation results based on 90 day historical data. (90 days prior 23rd Jan)",False
742064273,page,current,/x/kQA7L,Frequently Bought Together,"Introduction The idea is to display other food items that are recommended for a customer when they are on the check-out page about to place an order. The recommendations will be based on the order history of the PickMe food user base. Ex: When a consumer has added a Chicken Biriyani from Big Bite Restaurant, it is required to check what are the other dishes that the consumers have ordered from same restaurant together with Chicken Biriyani . Based on this, in the check-out page it is required to show the recommendations with an option to add them to the cart from a single tap. This will help customers to find what they need improving their user experience as well as boost revenue by increasing average order value. Since PickMe food does not yet support placing orders from multiple stores at once we will only consider the association of food items within the restaurant. One of the suggested results. Initial analysis based of Apriori. Two approaches were considered for the recommendation algorithm: Apriori FP Growth Parameter selection and performance of the two algorithms were analysed based on the number of rules generated, coverage of the menu and accuracy of predictions. The detailed report summarizing the outcomes can be found here. There is a major limitation for the results due to inconsistency in item id. Based on this Frequent Pattern Growth model was selected for FBT prediction. Initial code deployed has a static min_support defined. However, due to the skewed distribution involving the transaction count with the introduction of marketplace this needed to be a dynamic support based on each merchants performance. Therefore, the second iteration of the modeling includes a dynamic support calculation which will take into account the number of transactions for a given merchant within the data collection period (in this case a 30 day period) . Dynamic Support Calculations will be based on the median frequency in the item set. Pre filters: Number of transactions should be above 30. Maximum basket size would be less than 50. Update on - July 2nd 2020 It can be noticed that dynamic support as Q1 provides the best coverage. Hence the dynamic support was computed using the Q1/number of transaction.",False
742162538,page,current,/x/aoA8L,Popular Restaurant - phase 1,"The sample data set used for analysis consist of the all completed orders in September,2019. There were total of 405 distinct restaurants in selected data set. Criteria for popularity – (phase 1) Restaurant has completed above average number of orders on 20 days out of 30-day rolling window. The cut off value for popular restaurants will be the average sales across all restaurants that falls under above list. Any restaurant that have completed orders above this average will be labeled as popular. The orders of all restaurants will be filtered for outliers and be replaced with the upper fence value in order to remove highly skewness of the restaurant sales distribution, before computing the average sales. Since phase 1 only includes volume of sales, following phased can include aspects such as diversity and rate to improve the model for popularity. As an immediate next step would be to consider a Bayesian approach for sales volume can be used to compute a weighted average for sales.",False
742228058,page,current,/x/WoA9L,Recommendations,"In e-commerce and other digital domains, companies frequently want to offer personalised product recommendations to users. Automatically recommending a product to a customer has become a fundamental part of many successful digital companies. They collect data on their customers’ preferences and use various machine learning approaches to match up customers with the product that they are most likely to enjoy. “Wow,” you think. “They’ve done a pretty good job!” In a situation where companies have more data – however limited it may be – on their customers, products and customer interaction, they can consider using some of the classic recommender systems: Collaborative filtering works by recommending things that similar customers liked. However, it suffers from the cold-start problem, which is when there are new products that no one has used or liked. Content based filtering matches up content or products based on their attributes to a customer’s preferences. This is hard when you don’t yet know a lot about the customer, or you don’t understand what features of a product are pertinent. To avoid this we have discussed a hybrid approach of getting the best of the two worlds. This does not address all the edge cases where you can get rid of all cold start issues, however we have been able to curb this situation to a certain extent. Following are the documentation completed for each time of recommendations. Enjoy the read! Popular Restaurant - phase 1 https://pickme.atlassian.net/wiki/spaces/BDDS/pages/742064273/Ferequently+Bought+Together ""Recommended for you"" personalized recommendations LightFM vs ALS testing",False
742293592,page,current,/x/WIA_L,Discovery Catalog,"The discovery catalog is being divided to two segments as PickMe - Haling and PickMe - Food PickMe - Hailing Identification of shareable rides Shuttle utilization simulation results for selected route https://pickme.atlassian.net/wiki/spaces/BDDS/pages/743112705/Customer+Value+Matrix Driver Fraud Detection Algorithm Trip fare prediction model Pre - booked trip fulfilment analysis Pre-booking risk predictor model building and summary Travel Time Prediction Model Model retraining V2.0.0 Model retraining V2.1.0 Driver churn prediction driver profiling - initial analysis CRM data analysis and feature building Churn prediction - (model building) Taxi Driver Pooling PickMe - Food Recommendations Popular Restaurant - phase 1 https://pickme.atlassian.net/wiki/spaces/BDDS/pages/742064273/Ferequently+Bought+Together ""Recommended for you"" personalized recommendations LightFM vs ALS testing Food order pooling potential Dynamic Delivery Radius Discovery Vendor density analysis Promotion framework related work Miscellaneous Miscellaneous Subscription price modeling v2 Subscription pricing model v1",False
742293632,page,current,/x/gIA_L,Shuttle utilization simulation results for selected route,"Summary The simulation looked at historical data from March 2019 to identify the feasibility of operating a shuttle along Galle Road between Moratuwa and WTC. The route was selected based on the previous analysis done to identify the feasibility of ride sharing. The journey from Moratuwa to WTC is 16.5km and the return journey is 20.5km in travel distance. Based on an average distance of 285m, shuttle stop locations were identified for both journeys. Stops which were at least 250m apart were used to establish pick-up/drop-off points in order to increase the dataset for the simulation, since the current pick-up and drop-off locations are unique based on the passenger's location and preference. The passengers who had pick-up/drop-off locations within the Pick-Up and Drop off Radii 1 were all considered as potential rides for a shuttle. Prior to this analysis, the data was cleaned to exclude certain existing rides which did not meet a specific criterion, these criteria are further explained under Data Preprocessing. In this simulation 56 shuttles were sent at 15-minute intervals from both Moratuwa and WTC from 6am to 8pm. Google API was used to compute the time taken to get from one shuttle stop to the next. This provided the ability to include the estimated time to travel between stops factoring the traffic condition depending on the hour and day of the week. The detailed summary can be found under subtitle Shuttle Schedule. The following table summarizes the study where the maximum and average utilization hours are shown for both journeys. The maximum value indicates average maximum across the weekdays. The average number of rides for Wednesday is penalized slightly due to a Poya day falling on 20 th or March. There are a few caveats that we need to consider when reading the above summary. For example, since the pick-up and drop-off locations are spread within a range from 174m to 628m Further details under subtitle Pick-Up and Drop-Off Radii in this simulation the assumption is that the passengers will need to arrive at the pick-up location which is an inconvenience. On a given weekday the average number of passengers travelling in each of the shuttles is shown in the figures below. For clear view refer the document attached. Key Takeaways Moratuwa to WTC Indicates a maximum of 10 and an average of 3-5 riders using the shuttle between 7-9 am. Out of the total 165.5 passenger kilometers (passengers 10 x 16.5 km) the usage ranges between 80 to 100 passenger kilometers on average for the shuttles which travel from 7 – 9 am. The usage is then close to 50% for the shuttle being used at peak hours. The general tendency indicates that the shuttle reaches close to full capacity around Mount Lavinia – Kollupitiya stretch. WTC to Moratuwa Indicates a maximum of 10 and an average of 4-6 riders using the shuttle from 12 noon till 7pm. Indicates a maximum of 10 and an average of 5 riders using the shuttle between 7-9 am. Out of the total 205.4 passenger kilometers (passengers 10 * 20.5 km) the usage ranges between 100 to 130 passenger kilometers on average for shuttles after 12 noon. This is slightly above 55% of the shuttle being used at the peak hours. Unlike in Moratuwa to WTC stretch where the shuttle reaches full capacity during mid-way of the journey (Mount Lavinia - Kollupitiya), in this direction the shuttle capacity reaches a peak at the start of the journey _GoBack around Hyde Pack/ Slave Island and continues till mid-way towards Bambalapitiya/Wellawatta area. The utilization gradually reduces towards Moratuwa. The utilisation reduces significantly if we consider level 10 H3 cells or 500m apart stops. With the preprocessing step close to 30% of the rides were removed from the original historical dataset to incorporate a level interpretability. The analysis was done only based on the rides taken on PickMe platform. There will be a tendency for new users to use PickMe shuttles in the event they are introduced given they provide a better level of rider experience. Link to the detailed report.",False
743112705,page,current,/x/AQBLL,Customer Value Metrics,"Introduction The purpose of this report is to convey the findings of the RFM (Recency, Frequency, Monetary) analysis completed for the month of July 2019. Understanding customer segments based on parameters such as, current RFM value of customer, demographics, purchasing patterns and future customer value will provide the ability for the business to initiate more targeted campaigns which will aim at simulating each customer segment with the right kind of stimulus. The initial analysis as per the operation teams request will be to identify the customer segments based on their Recency, Frequency and Monetary (RFM) value. Recency: How recently did the customer purchase? i.e. Duration since last purchase (this could be interpreted as the last visit day or the last login time to the PickMe app, but for the purpose of this analysis recency was considered to be the last day the passenger hailed and completed a ride) Frequency: Number of completed rides within July 2019. Monetary: The total fare of all the trips completed within July 2019. Several approaches can be used to segment the data. Segmentation based on business input. Segmentation based on ML clustering, using K-means algorithm Segmentation using the RFM_score as a basis. Out of the proposed methods, I have empirically proven instead of the 27 segments used in approach one, we are able to use a much succinct segmentation of 4 clusters based on approach 2 and 3. This is further described in the Results section of this document. The main advantage of this process is to be able to adopt different marketing strategies for different customer segments. Moreover, clustering customers into different groups improves the quality of the recommendation and helps decision makers identify market segments more clearly and therefore develop more effective strategies. The document contains: Data gathering and pre-preprocessing Approach 1 – Segmentation based on business input Approach 2 – Segmentation based on K-means algorithm Approach 3 – Segmentation using the RFM_score as a basis Detailed document with results is here. Key- Takeaways Approach 2 and 3 provides a more granular segmentation of four, compared to the method given by the business that results in 27 segments. These segments are more directly actionable compared to the 27 segments. However, the combination of these segmentation can be used hand in hand to understand passenger behaviors in detail. The distribution of passengers in approach 1 results in an uneven distribution of passengers across the 27 segments with some having less than 50 passengers. Therefore, it will become difficult to use these segments to create value for passengers in an effective manner. Approach 2 and 3 both provide four segments and the categorization of customers is loosely similar. The RFM score-based approach provides a lower number of customers in the best segments (gold & silver) where monetary and frequency values are high compared to the K-means method where the passenger count is almost double. K-means segmentation provides a more even distribution of passengers across the segments compared to the RFM_score-based approach. K-means clustering method’s lower segment has approximately 100,000 passengers with an average monetary value of approximately Rs. 400 compared to the 120,000 passengers with an average monetary value of approximately Rs. 600 in the RFM_score-based method. Sample results: It can be noticed that customers with RFM_score nine are distinctly on a higher level in monetary value than the rest. Next planned: Stage 2: Using RFM values to perform a churn analysis/customer lifetime value (CLV) and profit potential. This could be modeled via a regression or boosting/ensemble model. The analysis can include features listed below other than the RFM values. Total time spend on the product (AON) Distance of average trip Number of times a trip is made on a given day Number of app opens Number of promotions set, contact points Click stream tracking – if available Prior promotion responses Customer loyalty points Demographics Stage 3: Build on the above model with further feature engineering. We will be able to use Collaborative filtering and hidden neural network layers as our new features. Build a user/item interaction matrix for prediction. Here the item, could be a promotion. Building an activity map based on the POI's. A probabilistic model of MCM (Markov Chain Model) can be used to in that they explicitly account for the uncertainty of the customers future behavior. In the sense, this can address customer retention and customer migration. Retention is we assume that once a customer is lost, PickMe relationship with the customer terminated. Whereas migration refers to non-response not necessarily means permanent loss.",False
764706849,page,current,/x/IYCULQ,Identification of shareable rides,"This was the initial analysis carried out to identify how many shareable rides were taking place on the PickMe platform. The report includes shareable ride counts and locations based on March 2019 completed rides. Limitations Summary Shareable ride component Ride breakdown for each day of the week by the date Shareable ride distribution by the number of shares Hourly break down of shared rides for each day of the week Hourly spread of shared rides as a percentage of the rides for the day Based on this analysis we were able to determine that one of the most dense regions for shared rides on PickMe is the Moratuwa to WTC stretch. Following this a simulation was done to understand the utilization of a shuttle which runs on this particular route. Detailed report for the shareable ride analysis. Sample results as follows: Hourly breakdown of shared rides for each day of the week Monday - Over four Mondays in March there is close to 1,900 rides which indicated that they can be shared. There is a peak in shared ride count in the morning hours from 8am – 9am and in the evening around 5pm. The number of shared rides during peak hours vary roughly between 300 – 700 rides. Pick up heat is concentrated around Fort and Mount-Lavinia areas.",False
764707094,page,current,/x/FoGULQ,Taxi Driver Pooling,"The driver pooling model proposed by Vindula required weight assignment for each of its parameters. The ILP solver provides the optimal driver assignment to each trip by its formulation of the optimization function. The scaling of decision variables are required as we need to provide a weight to incorporate business requirements. We propose below weights which can be used by the core team to multiply each decision variable. Considering business requirement: Directional hire will be assigned a higher priority. This will be given twice the priority over the ETA. Rating will be assigned a smaller weight. This will give more prominence to the ETA as opposed to a marginal difference in Rating. Opportunity cost of a trip will be assigned the same weight as the ETA, since the OC is decided dependent on the ETA and is fixed across all trips. Report submitted by Vindula The scaling and suggested weights can be found in:",False
764772411,page,current,/x/O4CVLQ,Driver Fraud Detection Algorithm,"Drivers resort to various hacks to misuse these incentive schemes. For instance, the higher the number of rides, the more money a driver makes. Therefore, we tried to flag out such drivers who indicate unusual behavior with selected passenger accounts compared to rest of the driver population. The focus of this analysis will be to understand the association between a driver passenger pair by analyzing the number of total completed trips between them over three months. We will hence forth call a completed ride between a driver and a passenger a transaction. The initial analysis and report considering various outlier detection models can be found here. Summary of the algorithm: The effectiveness of the finalized algorithm is documented here using the incentive run from 14th October to 18th October. Reconciliation between the manual driver fraud tagging vs the algorithm can be found here, with a step by step explanation of how a driver-passenger pair is tagged as fraudulent.",False
764969074,page,current,/x/coCYLQ,Trip fare prediction model,"Multiple Linear Regression Applicability of multiple linear regression was considered to predict the trip fare of a completed trip on PickMe platform. Data exploration and results based on multiple linear regression can be found here. Multiple regression models were concidered: Segment 1 - For all taxi models Fare ~ distance + Taxi Model + duration + day of week + pick hour + pick hex Fare ~ distance + Taxi Model + waiting cost + day of week + pick hour + pick hex Fare ~ distance + Taxi Model + duration + day of week + pick hour Fare ~ distance + Taxi Model + waiting cost + day of week + pick hour Log (Fare) ~ distance + Taxi Model + duration + day of week + pick hour + pick hex Log (Fare) ~ distance + Taxi Model + waiting cost + day of week + pick hour + pick hex Log (Fare) ~ distance + Taxi Model + duration + day of week + pick hour Log (Fare) ~ distance + Taxi Model + waiting cost + day of week + pick hour Segment 2 - For Tuk, on Fridays with 5pm pickup hour (models were tested for a restricted data set) Fare ~ distance + duration + day of week + waiting cost Fare ~ distance + waiting cost + speed Fare ~ distance + waiting cost + I (distance *2) Log (Fare) ~ distance + waiting cost Log (Fare) ~ distance + waiting cost + I (distance *2) np.log(fare) ~ distance + waiting cost + speed Moving to Random Forest model These models that were used to predict the taxi fare based on one month historical data did not adhere to the assumptions of linear regression and produced poor results. The next document will discuss linear regression models used for fare prediction and their shortcomings and outline a new approach for prediction based on Random Forest. The regression model before will be used as a base model.",False
805797968,page,current,/x/UIAHM,Runbooks,,False
805797975,page,current,/x/V4AHM,Spark Cluster Batch Workflows,Flow Name Runbook Link previuously_ordered_restaurants https://docs.google.com/document/d/1IHZSpJb6LOTuegUZqLZtMocYpGWgOrz43GmN7vb_mcE/edit?usp=sharing spark_h3 https://docs.google.com/document/d/1jki4rFoU1YgLg1N5pq1zBSlyOeoLBtoV3jZ7v4FJAeU/edit?usp=sharing spark_json https://docs.google.com/document/d/1BGDpRPphRleoHzPWfpuqxfPAASzXt9UOxf-8lNUh_4k/edit?usp=sharing cust_loyalty_points_calculator https://docs.google.com/document/d/1WQ6fwyjoR_ev3ai7H4a50hVuFLJgWvDu26GFz_Tqmvo/edit?usp=sharing cust_loyalty_state_processor https://docs.google.com/document/d/1W7XQuzlLm6IYInYVkmT5--EeHT4JG9RfE-liqSWbFPQ/edit?usp=sharing loyalty_eligibility https://docs.google.com/document/d/1BnFZLIGC4KUCqWF0WFG3n1SSmQJkQBu4pIor86MSYsw/edit?usp=sharing cust_loyalty_event_publisher https://docs.google.com/document/d/1l1M5KRU8Bh7xVLf-fc9qYElwOGs_4YjNbRHrtKvzmEY/edit?usp=sharing trip_flow_restaurants https://docs.google.com/document/d/1-uSQzpffq7TJywtO0tTIFmbYr1qKgrgbwhuXTqPSwd0/edit?usp=sharing SupplyDemand https://docs.google.com/document/d/1M4gKrRngaGWaBkjbgfnvGitCblOG0OeKqgcY58cWUVo/edit?usp=sharing spark_nrt_driver_utilization https://docs.google.com/document/d/1HUeM8b7TfFttPJKwfBh2LCG2qyZ8MJC0Z_1DMhu-QzQ/edit?usp=sharing DailyNoDriverFound https://docs.google.com/document/d/1pQWn1TpHEWuim7rEUG7KIW9xMAZlf_NyX1ScX1Q_KdE/edit?usp=sharing DailyRegionBasedDemand https://docs.google.com/document/d/1iLYczYEYSBOazkdsQVXUDuiR5wKzLkEIVw-Xbj-dei4/edit?usp=sharing DailyRegionBasedExcessSupply https://docs.google.com/document/d/1GUD5Q663tDwIJNeqIxwDPxrTsA5PTtCgNu74cHP38lc/edit?usp=sharing DailyRegionBasedSupply https://docs.google.com/document/d/1C-neMLpSiy2RtBfOEWwtJ-V09_-AGvdkKvsuepA4r58/edit?usp=sharing DailyRegionBasedTripComplete https://docs.google.com/document/d/18N_3II234Sm933J7JEpLfG4xna2oui-5OwW1lbFHDC0/edit?usp=sharing spark_driver_utilization_daily_country https://docs.google.com/document/d/1Ql8JFrZKOgOEOqQWGuiKMg-tC8HHpBGBR4wwZXmOvq4/edit?usp=sharing spark_driver_utilization_daily_hourly https://docs.google.com/document/d/1XXDo3t9rWN2yqu1hqhLU16ZQ7p-JR5nxmAilkXVo2AM/edit?usp=sharing spark_driver_utilization_daily_dsd https://docs.google.com/document/d/131Q4rKHRVvx-gRU8XcAT1QB2sUeTFAJt9eCbtWX2Pm4/edit?usp=sharing fact_hex https://docs.google.com/document/d/1d4jnBgfENjNoJ_jw4qJ1N_UDiZiMSttbfzrQl-9bYAw/edit?usp=sharing driver_incentives_fraud_tagger https://docs.google.com/document/d/1WMRdH9s-B0ehGXEI-L3nBbUhR7uSl9BPHa2ITe_WBWk/edit?usp=sharing driver_incentives_qualifier https://docs.google.com/document/d/1vIl0pjrqCnZB39TdlvBl-Q1yDZx2V984OK7eNEJpyOs/edit?usp=sharing driver_incentives_daily_calculator https://docs.google.com/document/d/1Rg-DqqnhFHXZX7967ZWNQG_zAxpwX9SzC7iQtf-EVQg/edit?usp=sharing service_level_parameter https://docs.google.com/document/d/1a7IKd5SlvTo4Ay1kSgH0HO2t6ko3BCZRqV1MINJL2Hc/edit?usp=sharing Driver-ranking https://docs.google.com/document/d/10TRKklVUrhco5BfBB_8fnqCjU_BXAkLVEHfsFmRSceE/edit?usp=sharing Driver-rating https://docs.google.com/document/d/1sgmA2fZLsky3VYUqUzJfW3sjd0vBTsfYVSzlcks01-8/edit?usp=sharing spark_dsd_visibility https://docs.google.com/document/d/1Fnu1j88YuillD4PaNrbqDloiHYRqq7pUOP8XxYFuMS4/edit?usp=sharing,False
805797992,page,current,/x/aIAHM,HBase Cluster Batch Workflows,Flow Name Runbook Link workflow_foodCustomerRecsRestuarants https://docs.google.com/document/d/1ypB9mLYJSebJtbGJo6zB3uFhNxLFCa3xcRLM5edwZV8/edit?usp=sharing workflow_restaurantMDM https://docs.google.com/document/d/1LxgvoCXESamwMXY04oGXc7R_Oxu2mmrQB3brxCdoaPI/edit?usp=sharing driver_utilization_last_seven_days https://docs.google.com/document/d/1c_0VbZzvnY51KZpVSvZ9oDZoyTre04xQmE39D5nCti0/edit?usp=sharing nrt_driver_utilization https://docs.google.com/document/d/1aAfYq0Tq0Wt2bf5kynPgfMhdGbYS12R7uXkQsvROz_4/edit?usp=sharing,False
806256716,page,current,/x/TIAOM,Hadoop Cluster Batch Workflows,Flow Name Runbook Link food_job https://docs.google.com/document/d/10fuiISHiH1IeEPlUH--JKA6WAy_qrxUHw44lFceEBo0/edit?usp=sharing food_menu https://docs.google.com/document/d/1CxvQByFl-HyHG0CT535HZJUqUdegudMytCVKOyq4J94/edit?usp=sharing food_restaurant https://docs.google.com/document/d/1tPfD0M90Qip8BQvhTYmoGtRHwUvAWj1uBG6qS4SW4Uo/edit?usp=sharing food_tag https://docs.google.com/document/d/1fPyDyl-0LpjnqjUT_pQIhmkyuqc_rYdq8yn-q1dkDx4/edit?usp=sharing hailing_activities https://docs.google.com/document/d/1eeZycTP3Po9bEvwRCYb_EBdcLXaGM3WlXKcUJUqqWIM/edit?usp=sharing hailing_bank_account https://docs.google.com/document/d/1oDBDEV2fRMNBP-npJQUzJqa0GCrI3za8lMuBbZ1F1-8/edit?usp=sharing hailing_cancels https://docs.google.com/document/d/1sdIZMGAl5MSUjXhO7S12-hNb97CdendPlCNsOF2kUKc/edit?usp=sharing hailing_coporates https://docs.google.com/document/d/1RsHdRuGsloZ86y8IHNpJM5fTWX-0WmKNutHeEeds9O4/edit?usp=sharing hailing_driver_details_stage_1 https://docs.google.com/document/d/1ZOuzHDiuSWF3R1Wt_zyH7W2iA043TN-YI4Sj5OCOR6Y/edit?usp=sharing hailing_driver_details_stage_2 https://docs.google.com/document/d/1k0GVT4aq-Q0Owagne-5s73zryN5c55NpiN1gZ6LfFsU/edit?usp=sharing hailing_driver_details_stage_3 https://docs.google.com/document/d/1--RRUKjYzHlV2dQVQtVVbyCBVgLyh47L1pzayRDlPjE/edit?usp=sharing hailing_driver_details_stage_4 https://docs.google.com/document/d/1pMrXS9ekIQGNMmDhpvKtMmC0p35R-0UePtCot3ooRsQ/edit?usp=sharing hailing_driver_details_stage_5 https://docs.google.com/document/d/1-SquhnpKTJwjaEN8LGBER-AS6-7cKrrC6n-FXZGuizU/edit?usp=sharing hailing_favourite https://docs.google.com/document/d/1V910mRDuST721ps1tkfevHwDXRyBIoAHg832OD7tFk8/edit?usp=sharing hailing_packages https://docs.google.com/document/d/1JLv3bhiLy8YTRAeCKw2Iuryot6kmaOuh-6XEn4t4-bw/edit?usp=sharing hailing_payment_stage_1 https://docs.google.com/document/d/1xVMHEkXfHNeCbnAOg188pdenp7P-3Of3xfbvc-Gjbo4/edit?usp=sharing hailing_payment_stage_2 https://docs.google.com/document/d/1j3IsWnIHyVuuOrLYvpV9kcyUcnTAVN5rv9yNMpKf9Yg/edit?usp=sharing hailing_payment_stage_3 https://docs.google.com/document/d/1BFKxjHXOPAYTdJi_xfsqGBIbQM1jWWLa_k_AUxhxRaw/edit?usp=sharing hailing_pickme https://docs.google.com/document/d/1kAPGXlUUn5OL_pRD_ONN4rY0EGKepGw3dF0yeTJMG68/edit?usp=sharing hailing_promo https://docs.google.com/document/d/1vStSqGJzBIrWEjm9RaeryRCoGEIkzWIPOvqMEQrFg2Q/edit?usp=sharing hailing_stored_values https://docs.google.com/document/d/1Qhvlzu-sstNez16SNz5WVl1MGkIk2HyC2aZTstHjwNM/edit?usp=sharing hailing_taxi https://docs.google.com/document/d/1RY3qmndCrris-sPhBLJOahOSaq9QZspoxwn0ReNTeb0/edit?usp=sharing hailing_transaction https://docs.google.com/document/d/1xa_Quu_Odm7FV0qLOSmVvGNuZSLoG7Vt0ZPt3otXbLg/edit?usp=sharing hailing_trips https://docs.google.com/document/d/1IMAF-cM-OGPDpRcwvokIRT7qrjMIglzoMh2bQJ1SKvQ/edit?usp=sharing hailing_vehicle https://docs.google.com/document/d/11bBmHumqR3zHKACp2IiJVc-yYlV6sR47mIJaT08Uhls/edit?usp=sharing dim_level_0_charges https://docs.google.com/document/d/1_uctfPvbhY_ihM854I0yUAqn4QPl5My-5wiwtRh_LFM/edit?usp=sharing dim_level_0_passenger https://docs.google.com/document/d/1rRJdxCZAUzms7L6cFHFxid4ivWM1OGUcZz4p4e4u_sM/edit?usp=sharing dim_level_0_payment https://docs.google.com/document/d/1cIjE2P-SUbw587u68DRoE78BwHddnQARSYp_EfDd7aY/edit?usp=sharing dim_level_0_pickme https://docs.google.com/document/d/1E79i-KxmGnvL3HYvNd3r3Hp5O0Zk5kr0OT3daKCtTkA/edit?usp=sharing dim_level_0_trip https://docs.google.com/document/d/1GZ-BmSaB-auFgEb9qfEspsLbAQpZkETqL9AgLYed9Eg/edit?usp=sharing fact_level_0_company https://docs.google.com/document/d/1lyNcQMYhtXha0mxSdh_h2WGBK4z7KgxrGc5mBCoU5i8/edit?usp=sharing fact_level_0_driver https://docs.google.com/document/d/13RKE1x9_ybsGjFLKIl2NOQP67VJFhBWMQEu0CwIpToE/edit?usp=sharing fact_level_0_payment https://docs.google.com/document/d/1CeBH0K90GX48keSKtbSf57A68Fw5iPtlM0QhXlgcYto/edit?usp=sharing fact_level_0_store https://docs.google.com/document/d/1TSagXFD0e9XsrloX2WheKWxrNBJfjg2gEODppDZrphQ/edit?usp=sharing fact_level_0_taxi https://docs.google.com/document/d/1cbaI-F8ZkkTj5hKByyjUzhZd2IoPIdklKq52dy4mmME/edit?usp=sharing fact_level_0_driver_device https://docs.google.com/document/d/13zaUwCr3wfset98S7KtFFqyvQzOvMTscngENZ7NMc7M/edit?usp=sharing fact_level_0_driver_payment https://docs.google.com/document/d/1lK_OCngEF1bZlqRLMzTW_WC-a0K_P4oco_VjZS9cEjs/edit?usp=sharing fact_level_0_store_account https://docs.google.com/document/d/1sbkiZLgc9h_IDzfvpYouNgUH2scQ6_T_LW20xOzEG1o/edit?usp=sharing fact_level_0_trip https://docs.google.com/document/d/1PRW1Dy2gWMb6UhRVwsXebEe-AHm06XB1rnaRkxgrP-U/edit?usp=sharing dim_menu https://docs.google.com/document/d/171k-MS14CXPchb36UXJ4VpjTlj0AH_yolhnSMU_VzE0/edit?usp=sharing dim_payment https://docs.google.com/document/d/1w_SllGzQK8bk8Bv7XHY2WcULuX7bCwwSHNJI-Uxq7lg/edit?usp=sharing fact_unique_level_1_device https://docs.google.com/document/d/1qfhRbQg4yrJtO19mXiWwsVI-5kCAtykVeR9qCGdKxe4/edit?usp=sharing fact_unique_level_1_driver https://docs.google.com/document/d/1EDeqtOniNmPygx3ku5D5DUwcXhqXk8wIPcdYvDyqQ98/edit?usp=sharing fact_unique_level_1_trip https://docs.google.com/document/d/1mDe7P-VCmjv2a1-Viw0eTOC7nAiJrcZWY-N-bDV3nLE/edit?usp=sharing fact_unique_level_1_payment https://docs.google.com/document/d/1kQyYvgePc-oBPHXEhLYONp3B05OtN51KSxfjE3jef-o/edit?usp=sharing fact_orders https://docs.google.com/document/d/1LVDSND1aIVq0v6VpUhUNz8tYJCFe1wMh6cztZcKalCQ/edit?usp=sharing fact_unique_orders https://docs.google.com/document/d/1QmL4q3EuhzIEn1RImpuvHl6H6SEFm5kPaZYOJZqKkuU/edit?usp=sharing fact_level_1_driver https://docs.google.com/document/d/1xu7R2jjrP6BELuC3r92p8GwA7tK1CCVhwcuQs8ZTLF4/edit?usp=sharing dim_level_2_driver https://docs.google.com/document/d/1ofXk-ly_wqstnPzq5j_7Xz7eshqdPjimFkwXMiXepss/edit?usp=sharing fact_unique_level_3_trip https://docs.google.com/document/d/1Odw2jQlgZmWka5Me4T6Eurm8Ui4xvNO3jINPtFm5bWE/edit?usp=sharing fact_level_4_promo https://docs.google.com/document/d/1rehZnaKoIxL_Pff6JWwPcKH2U6HR1fbiZmwPfJImvfA/edit?usp=sharing fact_unique_level_4_trip https://docs.google.com/document/d/1SdieatUW_uvcGmajlFCFnxKlv8xHFnyFdWJYcFM10Hk/edit?usp=sharing fact_level_4_trip https://docs.google.com/document/d/1vW9KEZGEdUfivJWPWc26KEKnN-4iK8g_P6ZaEns-Nao/edit?usp=sharing hailing_meter https://docs.google.com/document/d/1RIHL5ofm9OwuSjYqRPa67RiCWCUFTOjUNIHIusJ0jmk/edit?usp=sharing hailing_uuid https://docs.google.com/document/d/1JIz_dIDr-h3duNDz_F4Q_PBMrFu1btes2pWMpgASZBU/edit?usp=sharing fact_level_5_rides https://docs.google.com/document/d/12qz-YL2QbpG5sTSo2HEws3VKQEcyFLZRg3fnoRan1Vo/edit?usp=sharing fact_unique_level_5_trip https://docs.google.com/document/d/1xFwkMmPNglZifed_33xGewC8XmnUqC2cObUwr4sx6-k/edit?usp=sharing fact_level_6_driver https://docs.google.com/document/d/1X36cYrH9XhxE_-1Rjd32OmvzXE4iRdgnXOHvT-P0zPk/edit?usp=sharing fact_level_7_driver https://docs.google.com/document/d/17z8HcC8d0blmDGASSL1CEnFmSkqa2JnX7Emu6vLWeug/edit?usp=sharing fact_level_7_trip https://docs.google.com/document/d/1eNvDAtbvOtKZ8zS02kf11yltu9qRvJPOL-YYw2106Z0/edit?usp=sharing fact_unique_level_8_trip https://docs.google.com/document/d/1ZJDoxcxxDyDw7wFOxEZbcolDql8JWpajVPmAbwU9CbI/edit?usp=sharing fact_level_9_best_day https://docs.google.com/document/d/1R5CCfK8rA8AiZBN4DWip1a-84oFawlwTZAO2V7jW1Ek/edit?usp=sharing fact_level_8_driver https://docs.google.com/document/d/1NFubHhObLnGQCoQqdpOVTr-L4MUnx9oL1pkubbXjR6Q/edit?usp=sharing fact_level_0_uuid https://docs.google.com/document/d/1EKQ4DuZYvMfWKsZXwu1gxu3eAS-54EFXOpMYbOVM1CI/edit?usp=sharing crm_ticket https://docs.google.com/document/d/1nRxZHg0nSeab32cj6pko72ZOw1BKSD31c8tJr-1xKdo/edit?usp=sharing crm_tokens https://docs.google.com/document/d/1c2v7zF0pgLUc4JqCnc--SzJn2lb4ACJ_4NFsOQc72bY/edit?usp=sharing crm_people https://docs.google.com/document/d/1rfU9SNo61JEwehLO2Mc8TRWLa3Y2vUZwrVucUEc2lUw/edit?usp=sharing dim_level_0_crmdims https://docs.google.com/document/d/1kzmf5JbsqvRH7cMtrZZy9c2f6lbRw_Ed_HgmsoM2qY8/edit?usp=sharing fact_level_0_crmfactstoken https://docs.google.com/document/d/1uCftIOoYxSX2Pd7tvcGtuBHuh20HZKc0ltc7Bi0my9s/edit?usp=sharing fact_level_1_crmfactsticket https://docs.google.com/document/d/1BnGTHgvV2_a4xikYqpmwvWzm7xkjZYk1Lig1MDESaNU/edit?usp=sharing fact_level_2_crmfactsticketupdates https://docs.google.com/document/d/1QBkcOpRqHvIK6wFiVHGblGgiW41rY7LLVAMVXydH-9c/edit?usp=sharing fact_unique_level_0_crmfactstoken https://docs.google.com/document/d/1oarduQies7zWSEb_8gSgQh2ol7unkRHxWeVS7q2ZaQg/edit?usp=sharing fact_unique_level_1_crmfactsticket https://docs.google.com/document/d/12wdhSxNmq1Wn8JgFRQN9W0CLarKDHEQVkZdPzDHF1A4/edit?usp=sharing fact_unique_level_2_crmfactsticketupdates https://docs.google.com/document/d/1_4EC1a4Qi0EMDKiizpUr8yz9yC8J04mpUdbtaB723Qc/edit?usp=sharing fgl_fgl https://docs.google.com/document/d/1e_09ACEMW9kUM2hhZHnxdFPL0P6iyf3zy0VWEsdXWSU/edit?usp=sharing hailing_passengers https://docs.google.com/document/d/1SwnZxm97eC79_KOcTkhKo6YfB5vwAckBvs2TRVuUFNI/edit?usp=sharing food https://docs.google.com/document/d/1x9yIZNItAa68Akt1g7WaoKztmsZ8pNOmu3ns55fAc70/edit?usp=sharing crm_near_realtime_ticket https://docs.google.com/document/d/1N4WbS8_RNujfPtTG18LZv5Cu8Mxg5fUkrWOJ0Ax-rts/edit?usp=sharing hailing_selfreg_driver_information https://docs.google.com/document/d/1xDElPrtRG54QCdPHHZ6aGfUt0ZBiZCRJOAxjjdMnIoA/edit?usp=sharing dim_level_0_driver https://docs.google.com/document/d/1nF0q9mgry_UnR9wd-CiRGKbeU0RgsDxFlMzrFCtQjx4/edit?usp=sharing fact_level_2_trip https://docs.google.com/document/d/1S2SiIK3NehGj486zX0cdWBkS1N-iGCAsOEQS9PxIPqE/edit?usp=sharing workflow_DriverBasicDetails https://docs.google.com/document/d/1RPOK1ovOfbvngmPEgqHFgZhNHZOErs5MPVcx8OYtSJ4/edit?usp=sharing workflow_DriverBlock https://docs.google.com/document/d/1EIWTxI5l1o2bdIITf9023kd7RA7F1UE29p4ptusYIdo/edit?usp=sharing workflow_DriverCancelledReasonPercentage https://docs.google.com/document/d/1aCF2B_edEt2I7hNV-oy6pvK3aD19dHFc5e5dZLPtheM/edit?usp=sharing workflow_DriverPerformance https://docs.google.com/document/d/1U4cD9Ioar2R47ZP0anvhq6DRC_THtB2K8EikJQ283Gk/edit?usp=sharing workflow_PassengerDetail https://docs.google.com/document/d/1ykBnz9ctoMtTfNezcH3RbK03eAsFX-znuxG-eP4_S8I/edit?usp=sharing,False
806420525,page,current,/x/LQARM,Spark Cluster Realtime Workflows,Job Name Runbook Link trip_driver_arrived_raw https://docs.google.com/document/d/1I9-oWpTHBmUaTB4tFJrtToLjnlEctFmvkWp4mU00IMg/edit?usp=sharing trip_driver_assigned_raw https://docs.google.com/document/d/1nMvJ2cTGpc3nKRhBNRqao-53CxJ1_QDc1nLms7okgzI/edit?usp=sharing trip_driver_pool_no_driver_found_raw https://docs.google.com/document/d/1lj3hdY26wfPoyS7FHt7o4rZ1ssiRRssnFkqHIzrDppg/edit?usp=sharing trip_driver_selected_raw https://docs.google.com/document/d/1CNucsw17oeSgtlv-vv0Ued0Y5gRgAeDb2yX4LYybWJg/edit?usp=sharing trip_no_driver_found_raw https://docs.google.com/document/d/1VIdOVhR-grsFmdvRncr-wuBHWeI2zmmfRM0IIHQih84/edit?usp=sharing trip_trip_accepted_raw https://docs.google.com/document/d/16dxqbUYwZBJghjTEd7wOuaJTxFHZFw4L5sLz3JUUSg0/edit?usp=sharing trip_trip_cancelled_raw https://docs.google.com/document/d/1pLGTFY_D-_YftQhyjknIwZXXnQ69_XDiv0G64FpWtxo/edit?usp=sharing trip_trip_completed_raw https://docs.google.com/document/d/1o1aRmFz1u2qfOiN86L4vf9yY9YOEZ-GbCNog6wof-ZQ/edit?usp=sharing trip_trip_created_raw https://docs.google.com/document/d/122lVjR5QrxaKUckLKKbqZjXfiKG7DVrRvyOHW5HTUT4/edit?usp=sharing trip_trip_ended_raw https://docs.google.com/document/d/1ZhrQPWGnhwX88WYRTev-X6H1YKri50N-VVG4Ok-zhG8/edit?usp=sharing trip_trip_fare_updated_raw https://docs.google.com/document/d/1zciu8Y9dUsR-kvJDmrMrs5yODS-Rdhbo_xCQLk7Gr1k/edit?usp=sharing trip_trip_notification_timed_out_raw https://docs.google.com/document/d/1kEyJgqkWbSUvCMCeTa9xJ2vMLQ3xFEDB-Io3IQGrxMk/edit?usp=sharing trip_trip_received_by_driver_raw https://docs.google.com/document/d/1fjh3sQ1oeWa_mk74mk9VQDjYmUnXxmk3GdN8XoB9BYI/edit?usp=sharing trip_trip_rejected_by_driver_raw https://docs.google.com/document/d/1kQco0oWmKr86yZ6vvoxdqPQExjVX1QJ-TrceTTSk-UE/edit?usp=sharing trip_trip_started_raw https://docs.google.com/document/d/1-sJ01BjBuoeFM1HQueRh8mRmtADpk55zj-ClcZfGUWY/edit?usp=sharing trip_trip_updated_raw https://docs.google.com/document/d/1AWwlPDxHDaQHs8_6XnyCzrXGEY0CDZJWOih6kS3EP3Y/edit?usp=sharing passenger_driver_requested_raw https://docs.google.com/document/d/1QLsxwU_gfU1dSN_nPn1AleYUFRV6xlJNO8XXNUnnDLA/edit?usp=sharing driver_status_changed_raw https://docs.google.com/document/d/1QXwAYYnWbNV4JBYB1iebAIxLkSa2S_1YcjJe_s9vZwI/edit?usp=sharing surge_job https://docs.google.com/document/d/1gAZN-JF9Q-tD09k1g3oJQl06_0SHfNlNYOdRsBGHFII/edit?usp=sharing surge_raw_ingest https://docs.google.com/document/d/1ftczBMU3OQhhLnKBekpj6peXYVEqgL6OFZHrPu9DqdY/edit?usp=sharing driver_locations_driver_location_changed_raw https://docs.google.com/document/d/1UjnvCRHUGrDYpX9K-HfkntYsGDyDSufbMUCalBes0dM/edit?usp=sharing driver_shift_status_driver_shift_status_changed_raw https://docs.google.com/document/d/1wDwdW8Zhf2LWsDgYbQOxUdautkhCKi-jPjs8on10Yak/edit?usp=sharing,False
833192161,page,current,/x/4YCpMQ,Pre-booking risk predictor model building and summary,"Key points note The analysis is based on three months historical data and patterns, considering November, December 2019 and January 2020. An impressive 88% of all pre-booked trips are successful. Only 10% of pre-booked trips are canceled by dispatcher/driver. Close to 80% of pre booked trip requests are made via corporate and most of these happen between 8-10 pm. Out of the 10% cancellations, about 8% is accounted for by corporate customers. As a next step, we should identify the specific corporates that have higher cancellations and their pre-booking patterns to perhaps provide a customized solution (e.g. a pre-allocation of taxis for a reduced fee). 44% of corporate trips come to our dispatcher team with a maximum lead time of 30 mins. With under 30 min lead time close to 8,800 trips (per month) on average were fulfilled which is a significant achievement. However, there are also close 1,000 trip requests which are not fulfilled, and this is quite a large number which probably could be improved by increasing the lead time. We could improve this by looking at the specific corporate customers based on the data and speaking with them to improve on the lead time to provide a better service. Most pre-book trip requests are made on weekdays while the most requested taxi model is Mini (about 14,000 requests per month) which accounts for twice of Flex or Tuk requests which comes to about 7000 requests per month. Again there is a higher number of cancellation happening on Mini requests compared with the other taxi models. Most app based pre-booking requests are generated during early morning starting from 4.30 am and running till about 9 am and the unfulfillment rate is high despite the low request volume when compared with the evening. The detailed report can be found here. Model Building Note: there are certain limitations discussed in the document which led to a lack in certain data points that would have been useful to optimize the model further and get more insight in terms of understanding the level of difficulty in securing a driver for a pre-booked trip.",False
901283989,page,current,/x/lYC4NQ,Driver Aggregated KPI API,"Revision History Date Version Change Request Date Description Author 2020-05-06 1.0.0 N/A Initial version Dinesh Sandaruwan 2020-07-03 1.1.0 N/A Added the KPI catergory type ‘tripCount' instead of 'jobCount’. Added ‘timedOutRejCount' and ‘afterAcceptedRejCount’ under the KPI category type ‘rejection’ Modification in Response Schema. “serviceType“: (integer) is changed to “serviceType“: [(integer)] Ashkar Yoosuf 2021-03-25 1.2.0 2021-03-24 Received trip count : Both ride and job received count Completed trip count : Both ride and job completed count Dinesh Sandaruwan 2021-10-06 1.3.0 N/A Added KPI Category type ‘computed’ computed category includes ‘driverCompletionRatio’ Aathif Sanaya References Reference Version Date Remarks SRS - DATA - Agent Portal (Delivery) N/A N/A Link SRS - Fleet Management Portal- Phase 01 N/A N/A Link Introduction Driver Aggregated KPI API is one of the major APIs hosted in the PickMe data backend under Enterprise View of Driver (EVoD) API domain. This API will provide a one time integration architecture where new KPI category types can be made available with no / minimal changes to the consumer code. Currently the API is facilitated as a RESTful API. Request Schema Following table includes the request schema of the Driver Aggregated KPI REST API. URL http://<endpoint_addr>/iserv/v1/evod/ {driverIds} /kpis/agg Description Service to request aggregated driver KPIs based on driver id and other filtering parameters. Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters: Parameter Name State Type Values Description Remarks {driverIds} Mandatory Path One or more valid driver ids Id(s) of the driver(s) whose KPIs are being requested. E.g. http://<endpoint_addr>/iserv/v1/evod/ 821546%3B821547 /kpis/agg At least one driver Id must be provided. In case multiple driver ids are provided they must be separated with semicolon characters as shown below. String of format driverId1;driverId2 Note - Please ensure to URL encode all semicolon(;) characters in the URL (i.e. %3B). aggRange Optional String dailyinc (default) Range of days for aggregation. dailyinc - from 00:00:00 upto now for today Note - For the time being the API only supports daily incremental kpi aggregated values. In future additional aggregation ranges such as last day, last seven days etc., will be provided. type Optional String all (default) onlineTime distance rejection tripCount computed Type of kpi(s) being requested. Multiple kpi category types can be specified as a comma separated list. E.g. type=distance%2ConlineTime all - indicates to include all types ( default ) onlineTime - No of seconds the driver has spent on free, busy and active trip statuses. distance - No of kilo meters the driver has travelled on free, busy and active trip statuses. rejection - System rejection count and Manual rejection count tripCount - Received trip count and completed trip count computed - KPIs which were calculated using existing KPI’s. (Eg: Completion Ratio) Note 1: Listed are only the current supported kpi category types, and new types will be added based on the new kpi business comes up with in future. Note 2: Multiple types can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 3: Any unknown types will be silently ignored by the API (i.e. no error will be thrown). If all specified types are unknown, API will return an empty response payload. Note 4: If ‘all’ is specified among other types in the comma separated list of types, ‘all’ will be given priority and any other specific types will be ignored. I.e. response will be generated as though only ‘all’ or no type is specified in the request. serviceType Optional Integer -99 (default) Service types for which KPIs are being requested for. -99 - Any service type ( default ) Note 1: For the time being only total aggregated KPIs at drivers primary vehicle model is available via this API. However in future this API will support filtering KPIs at specific service types such as food delivery, parcel delivery etc. Request Headers: Following table illustrates the Request Headers that must be included in the Driver Aggregated KPI API. Header Name State Example Value Description X-EVOD-AGG-KPIS-API-KEY Mandatory androidprod API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-EVOD-AGG-KPIS-API-SECRET Mandatory feaee65a-2f54-4b18-8a7e-ea409a2e0d72 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 223e1203-0237-4770-89f6-31d65843f4c1 If provided, the given Correlation Id will be logged with all log messages logged by the Driver Aggregated KPI Service in executing this particular request. This will enable deeper issue analysis from source to driver kpi service backend. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Driver Aggregated KPI Service will generate its own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Schema HTTP 200(OK) response schema of Driver Aggregated KPI API will follow the base JSON response structure indicated below. { ""payload"":[ { ""driverId"": (integer), ""serviceType"": [ (integer) ], ""driverKpis"":[ { ""kpiMeta"":{ ""type"": ""(string)"" }, ""kpiPayload"":{ } } ] } ] } payload array Contains the entire response payload.This array can contain the kpi of given all drivers. driverId integer Id of the driver to whom the kpis in this response is related. serviceType integer array Service types for which KPIs are being requested for. driverKpi array This array of kpis can contain zero or more kpis related for the given driver. kpiMeta object Object containing metadata on how to interpret the kpiPayload. type string Type of kpi category included in the kpiPayload (See KPI Category Types subsection below). kpiPayload object Contains the actual kpi payload and schema will vary based on the type of kpi category (See KPI Payload Types subsection below). KPI Category Types: Following table contains the current KPI category types defined by the pickme data team. This table also indicates the relevant KPI Payloads sub-sections which provide the exact kpiPayload schema for each supported KPI category type. New KPI category types can be added in future based on the use cases. Kpi category Type (kpiMeta => type) Description Related kpiPayload subsection onlineTime No of seconds which have been worked by a particular driver in different trip statuses. onlineTime based KPI Payload distance No of kilo meters which has been traveled by a particular driver in different trip statuses. distance based KPI payload rejection System rejection count and Manual rejection count rejection based KPI payload tripCount Received trip count and completed trip count tripCount based KPI payload computed KPIs which were calculated from existing KPI’s computed KPI payload Note: Integration developers should take required implementation steps to ignore any kpi category types that they do not want to process or do not know how to process . This allows for a future proof integration where the Driver Aggregated KPI Service may add new kpi category types, but integrated parties will not break and will only start consuming such new types when the new code required for processing such new types are added to the integrated party binaries. KPI Payload Types Please ensure that you have gone through the Response Schema section for the base JSON response structure before reading this section. onlineTime based kpi Payload (kpiMeta => type => onlineTime) { ""freeOnlineTime"": (integer), ""busyOnlineTime"": (integer), ""activeOnlineTime"": (integer) } Object containing KPI details of onlineTime category type . All values are in seconds (s). freeOnlineTime integer Online time with Free status busyOnlineTime integer Online time with Busy status activeOnlineTime integer Online time with Active status distance based kpi Payload (kpiMeta => type => distance) { ""freeDistance"": (double), ""busyDistance"": (double), ""activeDistance"": (double) } Object containing KPI details of distance category type . All values are in meters (m). freeDistance double Distance with Free status busyDistance double Distance with Busy status activeDistance double Distance with Active status rejection based kpi Payload (kpiMeta => type => rejection) { ""systemRejCount"": (integer), ""manualRejCount"": (integer), ""timedOutRejCount"": (integer), ""afterAcceptedRejCount"": (integer) } Object containing KPI details of rejection category type . systemRejCount integer System rejection count manualRejCount integer Manual rejection count timedOutRejCount integer Timed out rejection count afterAcceptedRejCount integer After accepted rejection count tripCount based kpi Payload (kpiMeta => type => tripCount) { ""receivedTripCount"": (integer), ""completedTripCount"": (integer) } Object containing KPI details of tripCount category type . receivedTripCount integer Both ride and job received count completedTripCount integer Both ride and job completed count computed kpi Payload (kpiMeta => type => computed) { ""completionRatio"": (double) } Object containing KPI details of computed category type . completionRatio double Driver completion ratio. Response Headers: Header Name Description EVOD-AGG-KPIS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Error Response Schema Error responses from Driver Aggregated KPI API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Driver Aggregated KPI API will include the following response header(s). Header Name Description EVOD-AGG-KPIS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors: All client initiated error responses from Driver Aggregated KPI API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"":[ { ""code"": “(string)”, ""message"": “(string)” } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. Following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code EVOD-AGG-KPIS-1300 Missing authentication details. 401 EVOD-AGG-KPIS-1301 Throttling limit reached. Try again in a few seconds. 401 EVOD-AGG-KPIS-4xx Client error. Please send an email to l2support@pickme.lk with the EVOD-AGG-KPIS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.4 Client Error section. Server Errors: All server error responses from Driver Aggregated KPI API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"":[ { ""code"": “(string)”, ""message"": “(string)” } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. As per best practices in not exposing the exact error that occurred in the server side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code EVOD-AGG-KPIS-5xx Server error. Please send an email to l2support@pickme.lk with the EVOD-AGG-KPIS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.5 Server Error section. An Example Request and Response Following is an example request with 'type' optional parameter specified in the request: http://<endpoint_addr>/iserv/v1/evod/22%3B108%3B136/kpis/agg?type=distance%2ConlineTime%2CtripCount Please note that the following headers were included with the request: X-EVOD-INC-API-KEY: androidprod X-EVOD-INC-API-SECRET: feaee65a-2f54-4b18-8a7e-ea409a2e0d72 The above request has requested for: Driver aggregated KPI details for the driver ids 22, 108 and 136 ( 22%3B108%3B136 ). The requested service type is -99 which is the default value. The requested KPI category types are ‘distance', ‘onlineTime’ and ‘tripCount’ ( type=distance%2ConlineTime%2CtripCount ). Following is an example response for the above request: c { ""payloads"": [ { ""driverId"": 108, ""serviceType"": [ -99 ], ""driverKpis"": [ { ""kpiMeta"": { ""type"": ""distance"" }, ""kpiPayload"": { ""freeDistance"": 86.98814986772648, ""busyDistance"": 0.18620418416919984, ""activeDistance"": 0.0 } }, { ""kpiMeta"": { ""type"": ""tripCount"" }, ""kpiPayload"": { ""receivedTripCount"": 0.0, ""completedTripCount"": 2.0 } }, { ""kpiMeta"": { ""type"": ""onlineTime"" }, ""kpiPayload"": { ""freeOnlineTime"": 25616.0, ""busyOnlineTime"": 145.0, ""activeOnlineTime"": 0.0 } } ] }, { ""driverId"": 22, ""serviceType"": [ -99 ], ""driverKpis"": [ { ""kpiMeta"": { ""type"": ""tripCount"" }, ""kpiPayload"": { ""receivedTripCount"": 0.0, ""completedTripCount"": 0.0 } }, { ""kpiMeta"": { ""type"": ""distance"" }, ""kpiPayload"": { ""freeDistance"": 1.1717487535544002, ""busyDistance"": 0.0, ""activeDistance"": 0.0 } }, { ""kpiMeta"": { ""type"": ""onlineTime"" }, ""kpiPayload"": { ""freeOnlineTime"": 1708.0, ""busyOnlineTime"": 0.0, ""activeOnlineTime"": 0.0 } } ] }, { ""driverId"": 136, ""serviceType"": [ -99 ], ""driverKpis"": [ { ""kpiMeta"": { ""type"": ""onlineTime"" }, ""kpiPayload"": { ""freeOnlineTime"": 39152.0, ""busyOnlineTime"": 2510.0, ""activeOnlineTime"": 600.0 } }, { ""kpiMeta"": { ""type"": ""distance"" }, ""kpiPayload"": { ""freeDistance"": 134.05945709626516, ""busyDistance"": 5.604753548309232, ""activeDistance"": 2.4292267024275933 } }, { ""kpiMeta"": { ""type"": ""tripCount"" }, ""kpiPayload"": { ""receivedTripCount"": 0.0, ""completedTripCount"": 6.0 } } ] } ] } Note the following header value was included into the above response: EVOD-AGG-KPIS-CORRELATION-ID: 05d0edec-254d-40df-93d5-5a6aef71558e",False
985759841,page,current,/x/YYDBOg,Data Kafka Topic Related Information,"Generic When you login to the Kafka cluster Edge Node, use the following command to navigate to the Kafka binaries location in the server. All subsequent commands are provided as issued from this location. cd /usr/hdp/current/kafka-broker List all the topics bin/kafka-topics.sh --list --zookeeper zk0-kfk-pr:2181 Describe a topic bin/kafka-topics.sh --zookeeper zk0-kfk-pr:2181 --describe --topic surge_stage Delete a topic bin/kafka-topics.sh --zookeeper zk0-kfk-pr:2181 --delete --topic excess_demand Subscribe to a topic bin/kafka-console-consumer.sh --bootstrap-server wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 --topic heat_changed Create a topic bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic driver_arrived Trip Life-cycle Related Topics bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic driver_arrived bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic driver_assigned bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic driver_pool_no_driver_found bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic driver_selected bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic no_driver_found bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_accepted bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_cancelled bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_completed bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_created bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_ended bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_fare_updated bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_notification_timed_out bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_received_by_driver bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_rejected_by_driver bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_started bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_updated Passenger Related Topics bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 3 --topic passenger_driver_requested Driver Heartbeat Related Topics (Decommissioned) bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 3 --topic driver_status_changed Driver Related Topics bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 3 --topic driver_locations bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 3 --topic driver_shift_status Heatmap Related Topics bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 6 --topic heat_changed bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 6 --topic surge_ingest bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 6 --topic surge_stage bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 6 --topic surge_count_stage Job Related Topics bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_created bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic queue_accepted bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic queue_declined bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_confirmed bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_declined bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_timed_out bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_preptime_expired bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_order_ready bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_accepted bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic arrived_at_pickup bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic order_picked_up bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_end bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_cancelled bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic job_completed Trip ETA bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic trip_eta Continuous Driver Alert bin/kafka-topics.sh --create --zookeeper zk0-kfk-pr:2181,zk2-kfk-pr:2181,zk4-kfk-pr:2181 --replication-factor 1 --partitions 1 --topic continuous_driving_alert",False
986087551,page,current,/x/f4DGOg,Spark Realtime Jobs Submit Commands,"Trip Raw Ingestion Jobs spark-submit --class runner.tripDriverArrivedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_driver_arrived gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_driver_arrived spark-submit --class runner.tripDriverAssignedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_driver_assigned gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_driver_assigned spark-submit --class runner.tripDriverPoolNoDriverFoundRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_driver_pool_no_driver_found gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_driver_pool_no_driver_found spark-submit --class runner.tripDriverSelectedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_driver_selected gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_driver_selected spark-submit --class runner.tripNoDriverFoundRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_no_driver_found gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_no_driver_found spark-submit --class runner.tripTripAcceptedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_accepted hive_db_raw_rt/trip_trip_accepted spark-submit --class runner.tripTripCancelledRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_cancelled gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_trip_cancelled spark-submit --class runner.tripTripCompletedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_completed hive_db_raw_rt/trip_trip_completed spark-submit --class runner.tripTripCreatedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_created hive_db_raw_rt/trip_trip_created spark-submit --class runner.tripTripEndedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_ended hive_db_raw_rt/trip_trip_ended spark-submit --class runner.tripTripFareUpdatedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_fare_updated_new hive_db_raw_rt/trip_trip_fare_updated_new spark-submit --class runner.tripTripNotificationTimedOutRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_notification_timed_out gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_trip_notification_timed_out spark-submit --class runner.tripTripReceivedByDriverRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_received_by_driver gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_trip_received_by_driver spark-submit --class runner.tripTripRejectedByDriverRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_rejected_by_driver hive_db_raw_rt/trip_trip_rejected_by_driver spark-submit --class runner.tripTripStartedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_started hive_db_raw_rt/trip_trip_started spark-submit --class runner.tripTripUpdatedRaw --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --queue rt /pickme/artifacts/spark/realtime_trip_raw/dp-rt-trip-raw-assembly-1.1.12.jar Kafka-prd-prim-pkme-w-0:9092,kafka-prd-prim-pkme-w-1:9092,kafka-prd-prim-pkme-w-2:9092 true hive_db_raw_rt.trip_trip_updated gs://pickme-dataprod-cluster-config-gcs/hive-warehouse/hive_db_raw_rt.db/checkpoint/trip_trip_updated Driver Heartbeat Raw Ingestion Jobs (Decommissioned) spark-submit --class runner.driverDriverStatusChangedRaw --master yarn --deploy-mode cluster --executor-memory 2G --total-executor-cores 2 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_driver_raw/dp-rt-driver-raw-assembly-1.0.15.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true Passenger Raw Ingestion Jobs check point dir -> hive_db_raw_rt/passenger_driver_requestedSurgeStageJob spark-submit --class runner.passengerDriverRequestedRaw --master yarn --deploy-mode cluster --executor-memory 2G --total-executor-cores 2 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_passenger_raw/dp-rt-passenger-raw-assembly-1.0.12.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true Driver Raw Ingestion Jobs check pointd dir-> hive_db_raw_rt/driver_locations_driver_location_changed HADOOP_USER_NAME=hive spark-submit --class runner.driverLocationsDriverLocationChangedRaw --master yarn --deploy-mode cluster --executor-memory 2G --total-executor-cores 2 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_driver_raw/dp-rt-driver-locations-raw-assembly-1.1.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true driver_locations_driver_location_changed hive_db_raw_rt/driver_locations_driver_location_changed HADOOP_USER_NAME=hive spark-submit --class runner.driverShiftStatusDriverShiftStatusChangedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_driver_raw/dp-rt-driver-shift-status-raw-assembly-1.1.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true Heatmap Jobs (Decommissioned) spark-submit --class runner.SurgeJob --master yarn --deploy-mode cluster --executor-memory 12G --executor-cores 2 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-1.0.10.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 hdfs://mycluster/checkpoints/surgejobv3 1 spark-submit --class runner.CoreSurgeJob --master yarn --deploy-mode cluster --executor-memory 12G --executor-cores 4 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-2.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wasbs://configs@hdinprdshrprimpkme.blob.core.windows.net/spark/realtime_heatmap/surge_downgrade_agg_config.json 1 spark-submit --class runner.DataPlatformSurgeJob --master yarn --deploy-mode cluster --executor-memory 12G --executor-cores 4 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-2.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wasbs://configs@hdinprdshrprimpkme.blob.core.windows.net/spark/realtime_heatmap/surge_downgrade_agg_config.json 1 Heatmap Jobs Heatmap Stage Job check point dir -> hdfs://mycluster/checkpoints/SurgeStageJob spark-submit --class runner.SurgeStageJob --master yarn --deploy-mode cluster --executor-memory 12G --executor-cores 4 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-3.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wasbs://configs@hdinprdshrprimpkme.blob.core.windows.net/spark/realtime_heatmap/surge_downgrade_agg_config.json Heatmap Surge Count Stage Job check point dir -> hdfs://mycluster/checkpoints/SurgeCeilingJob spark-submit --class runner.SurgeCeilingJob --master yarn --deploy-mode cluster --executor-memory 4G --executor-cores 2 --num-executors 3 --conf spark.yarn.maxAppAttempts=1 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-4.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 Heatmap Core Side Job for Downgraded Types check ponit dir -> hdfs://mycluster/checkpoints/CoreSurgeDowngradedJob spark-submit --class runner.CoreSurgeDowngradedJob --master yarn --deploy-mode cluster --executor-memory 4G --executor-cores 1 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-3.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 1 Heatmap Core Side Job for Non-Downgraded Types check point dir -> hdfs://mycluster/checkpoints/CoreSurgeNonDowngradedJob spark-submit --class runner.CoreSurgeNonDowngradedJob --master yarn --deploy-mode cluster --executor-memory 4G --executor-cores 1 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-4.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 1 Heatmap Data Side Job for Downgraded Types check point dir -> hdfs://mycluster/checkpoints/DataPlatformSurgeDowngradedJob spark-submit --class runner.DataPlatformSurgeDowngradedJob --master yarn --deploy-mode cluster --executor-memory 4G --executor-cores 1 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-4.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 1 Heatmap Data Side Job for Non-Downgraded Types check point dir -> hdfs://mycluster/checkpoints/DataPlatformSurgeNonDowngradedJob spark-submit --class runner.DataPlatformSurgeNonDowngradedJob --master yarn --deploy-mode cluster --executor-memory 4G --executor-cores 1 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.scheduler.mode=FAIR --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt /pickme/artifacts/spark/realtime_heatmap/dp-rt-surge-job-assembly-3.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 1 Heatmap Data Side Raw Ingestion Job check point dir -> hive_db_raw_rt/heat_changed spark-submit --class runner.SurgeRawIngest --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_heatmap_raw/dp-rt-surge-raw-job-assembly-1.2.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 hive_db_raw_rt/heat_changed Job Raw Ingestion Jobs check point dir -> hive_db_raw_rt/job_arrived_at_pickup spark-submit --class runner.JobArrivedAtPickupRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobJobAcceptedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobJobCancelledRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true check point dir -> hive_db_raw_rt/job_job_completed spark-submit --class runner.JobJobCompletedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true check point dir -> hive_db_raw_rt/job_job_confirmed spark-submit --class runner.JobJobConfirmedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true check point dir -> hive_db_raw_rt/job_job_created spark-submit --class runner.JobJobCreatedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobJobDeclinedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobJobEndRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobJobOrderReadyRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobJobPreptimeExpiredRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobJobTimedOutRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobOrderPickedUpRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobQueueAcceptedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true spark-submit --class runner.JobQueueDeclinedRaw --master yarn --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 --num-executors 1 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_job_raw/dp-rt-job-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 true Trip ETA HADOOP_USER_NAME=hive spark-submit --class runner.tripETARaw --master yarn --deploy-mode cluster --executor-memory 2G --total-executor-cores 2 --num-executors 3 --conf spark.yarn.maxAppAttempts=2 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 --queue rt --jars /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.2.2-1.jar /pickme/artifacts/spark/realtime_trip_eta_raw/dp-rt-trip-eta-raw-assembly-1.0.0.jar wn0-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn1-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092,wn2-kfk-pr.n5nj0oolsseunhwimki0nkx0md.ix.internal.cloudapp.net:9092 hive_db_raw_rt/trip_eta_trip_eta Total Running Jobs => 40 + 2 Thrift Servers",False
1090748618,page,current,/x/yoADQQ,"""Recommended for you"" personalized recommendations","This document covers details of building a personalized recommendation for delivery (food and marketplace) customers based on the customer propensities. Derivation of customer propensities have been modeled based on historical customer purchases on the platform based on their 90 day past behavior. However, this is not the ideal scenario and we can improve the customer propensities further by monitoring customer behavior using click stream and search related data which is not in our data lake as of 28 th July 2020. Top twenty most popular tags were used to determine customers preference in terms of food categories and marketplace categories. Further customer purchasing power was determined based on order value and mealtime preference was also taken into consideration based on order placement hour for food-delivery vertical. A probability score was then assigned to each customer based on their past orders. Example for a customer A. This shows the propensities of a given customer towards the top 20 selling categories in the food - delivery space. As you can see this customer seems to order relatively more in desserts compared with other food categories. Here you can notice that this person in general does not purchase food items for dinner, most purchases are made during lunch hours and afternoon snacks. In tems of purchasing power, the customer seems to spend mostly less than 1000 and rest in the range of 1500-3000. Similarly features for restaurants were mined by looking at the presence of a tag. Of cause this is not the ideal way to go about the characterization, but due to the lack in data we will currently give equal weights to all tags that is present in a restaurant. Limitations: Currently there is no proper tagging taxonomy carried out at the item level, therefore categorization of restaurants based on menu item or merchandise is not possible. If they were available, we could have categorized the restaurants based on their sales volumes. Further information with regard to the methodology used in deriving recommendations can be found in the attached document. Sample recommendation for Meliza Diaz: Further improvements There are two main angles from which the model can be improved Use better data Item level tagging User behavior tracking using click stream User intent mining through search query Use a different algorithm for model recommendations Currently I have used the WARP loss matrix in the LightFM library, however there are other loss matrix, as well as KNN approach, Nural Network embedding methods and SVD that can be used for deriving user recommendations. These approached can be explored once we set up a mechanism to validate the usage of the slider.",False
1154547921,page,current,/x/0QDRR,Driver Fatigue Check API,"Date Version Change Request Date Description Contributors 2020-08-24 1.0.0 N/A Initial version Dinesh Sandaruwan Shehan Ishanka The event under Driver Fatigue Check API will follow the below event structures for event key and event body. Topic Name continuous_driving_alert Event Key Structure { ""key"": <DRIVER_ID> // long - Driver Id } Event Body Structure { ""id"": ""<UNIQUE_ID>"", // string - Unique UUID which is unique for each event ""type"": ""<EVENT_TYPE>"", // string - continuous_driving_alert ""body"": { ""driver_id"": <DRIVER_ID>, // long - Driver Id ""alert_time"": <UNIX_TIMESTAMP>, // long - Unix timestamp in milliseconds ""alert_level"": <LEVEL>, // int - Higher value will indicate a higher severity ""activeness"": <ACTIVENESS_INDEX> // double - activeness index value. E.g. 0.75 (75% active) } ""created_at"": <UNIX_TIMESTAMP>, // long - Unix timestamp in milliseconds ""expiry"": <UNIX TIMESTAMP> ,// long (optional) - unix timestamp in seconds ""version"": <EVENT VERSION> ,//int (optional) ""trace_info"":{ ""trace_id"":{ ""high"":0, ""low"":0 }, ""span_id"":0, ""parent_id"":0, ""sampled"":false } }",False
1256390677,page,current,/x/FQDjSg,Recommended Locations API,"Revision History Date Version Change Request Date Description Author 2020-10-05 1.0.0 N/A Initial version Dinesh Sandaruwan 2024-03-30 1.2.0 N/A Designated Pickup Amantha Sandupa References Reference Version Date Remarks N/A N/A N/A N/A Introduction Recommended Location API is one of the major APIs hosted in the PickMe data backend under Enterprise View of Customer (EVoC) API domain. Mainly, two types of recommendations are provided by this API. Recommended Pick-up Recommended Drop-off Public Pickups Designated Pickups Recommended Pick-up Under the type of recommended pickup, It serves two kinds of pickup locations. They are private and public pickup points. Recommended Drop-off Under the type of recommended drop off, it serves two kinds of drop locations. They are associated drop locations and generic drop locations. Request Schema Following table includes the request schema of the Recommended Locations REST API. URL http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/ {passengerId} /lat/ {lat}/ lon/ {lon} Description Service to request recommended locations based on passenger id and other filtering parameters. Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters: Parameter Name State Type Values Description Remarks {passengerId} Mandatory Path (Integer) Any valid passenger id Id of the passenger whose recommended locations are being requested. E.g. http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/ 931222 /lat/6.87/lon/79.68 Note 1: Any unsupported values will result in an HTTP 4xx error. {lat} Mandatory Path (Double) Any valid latitude Latitude of the passenger current location E.g. http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/931222/lat/ 6.87 /lon/79.68 Note 1: Any unsupported values will result in an HTTP 4xx error. {lon} Mandatory Path (Double) Any valid longitude Longitude of the passenger current location E.g. http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/931222/lat/6.87/lon/ 79.68 Note 1: Any unsupported values will result in an HTTP 4xx error. recLocType Optional String all (default) pickup dropoff Type of recommended locations being requested. all - Indicates to include all types ( default ) pickup - This is to request private and public pickup recommendations. dropoff - This is to request associated and generic dropoff recommendations. Note 1: Any unknown types will result in an HTTP 4xx error. Request Headers: Following table illustrates the Request Headers that must be included in the Recommended Locations API. Header Name State Example Value Description X-EVOC-REC-LOCS-API-KEY Mandatory androidprod API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-EVOC-REC-LOCS-API-SECRET Mandatory feaee65a-2f54-4b18-8a7e-ea409a2e0d72 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 223e1203-0237-4770-89f6-31d65843f4c1 If provided, the given Correlation Id will be logged with all log messages logged by the Recommended Locations API Service in executing this particular request. This will enable deeper issue analysis from source to recommended locations service backend. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Recommended Locations API Service will generate its own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Schema HTTP 200(OK) response schema of Recommended Locations API will follow the base JSON response structure indicated below. { ""payload"":{ ""passengerId"":""(integer)"", ""lat"":""(double)"", ""lon"":""(double)"", ""recommendations"":[ { ""recMeta"":{ ""type"":""(string)"" }, ""recPayload"":[ { } ] } ] } } payload object Contains the entire response payload. passengerId integer Id of the passenger to whom the recommendations in this response is related. lat double Latitude of the location which recommendations are being requested for. lon double Longitude of the location which recommendations are being requested for. recommendations array This array of recommendations can contain one or two types of recommendations related for the given passenger and location. recMeta object Object containing metadata on how to interpret the recPayload. type string Type of recommendations included in the recPayload (See Recommendation Location Types subsection below). recPayload array This array of recPayload can contain one or two subtypes of recommendations payload and schema will vary based on the type of Recommendation Location Type (See Recommendation Location Types subsection below). Recommendation Location Types: Following table contains the current Recommendation Location Types supported by the Recommended Locations service. This table also indicates the relevant Recommendation Payloads sub-sections which provide the exact recPayload schema for each supported Recommendation Location Type. New recommendation types can be added in future based on the use cases. Recommendation Location Type (recMeta => type) Description Related recPayload subsection pickup Under the type of recommended pickup, It serves two kinds of pickup locations. They are private and public pickup points. Pickup Recommendations Payload dropoff Under the type of recommended drop off, it serves two kinds of drop locations. They are associated drop locations and generic drop locations. Dropoff Recommendations payload *Note: Integration developers should take required implementation steps to ignore any Recommendation Location Types that they do not want to process or do not know how to process . This allows for a future proof integration where the Recommended Locations API Service may add new Recommendation Location Types, but integrated parties will not break and will only start consuming such new types when the new code required for processing such new types are added to the integrated party binaries. Recommendation Payload Types: Please ensure that you have gone through the Response Schema section for the base JSON response structure before reading this section. Pickup Recommendations Payload (recMeta => type => pickup) { ""subType"":""(String)"", ""locations"":[ { ""latitude"":""(Double)"", ""longitude"":""(Double)"", ""address"":""(String)"" } ] } Object containing recommended pickup locations. subType string Type of the recommended pickup location. (public, private) locations array This array of locations can contain zero or more recommended pickup locations applicable for the given passenger. latitude double Latitude of the recommended drop location longitude double Longitude of the recommended drop location address string Address of the recommended drop location Dropoff Recommendations Payload (recMeta => type => dropoff) { ""subType"":""(String)"", ""locations"":[ { ""latitude"":""(Double)"", ""longitude"":""(Double)"", ""address"":""(String)"" } ] } Object containing recommended dropoff locations. subType string Type of the recommended dropoff location.(generic, associated) If there are no associated drop locations, api will return generic drop locations and if there are associated drop locations api will only return associated drop locations. locations array This array of locations can contain zero or more recommended drop locations applicable for the given passenger. latitude double Latitude of the recommended drop location longitude double Longitude of the recommended drop location address string Address of the recommended drop location Response Headers: Header Name Description EVOC-REC-LOCS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Error Response Schema Error responses from Recommended Locations API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Recommended Locations API will include the following response header(s). Header Name Description EVOC-RECS-LOCS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors: All client initiated error responses from Recommended Locations API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"":[ { ""code"": “(string)”, ""message"": “(string)” } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. Following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code EVOC-RECS-LOCS-1200 Malformed recLocType. 400 EVOC-RECS-LOCS-1300 Missing authentication details. 401 EVOC-RECS-LOCS-1301 Throttling limit reached. Try again in a few seconds. 401 EVOC-RECS-LOCS-4xx Client error. Please send an email to l2support@pickme.lk with the EVOC-RECS-LOCS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.4 Client Error section. Server Errors: All server error responses from Recommended Locations API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"":[ { ""code"": “(string)”, ""message"": “(string)” } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. As per best practices in not exposing the exact error that occurred on the server side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code EVOC-RECS-LOCS-5xx Server error. Please send an email to l2support@pickme.lk with the EVOC-RECS-LOCS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.5 Server Error section. Example Requests and Responses Following is an example request with all the optional parameters specified in the request. http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/931222/lat/6.87/lon/79.68?recLocType=pickup Please also note the following header values included into the above request. X-EVOC-REC-LOCS-API-KEY: androidprod X-EVOC-REC-LOCS-API-SECRET: feaee65a-2f54-4b18-8a7e-ea409a2e0d72 X-CORRELATION-ID: 223e1203-0237-4770-89f6-31d65843f4c1 In the above request the requester has requested, Recommendation for the passengerId 931222. The requested recommendation type is “recommendation pickup” ( recLocType=pickup) And following is an example response for the above request. { ""payload"": { ""passengerId"": 931222, ""lat"": 6.78, ""lon"": 79.67, ""recommendations"": [ { ""recMeta"": { ""type"": ""pickup"" }, ""recPayload"": [ { ""subType"": ""private"", ""locations"": [ { ""latitude"": 6.86637047358571, ""longitude"": 79.8732092721, ""address"": """" }, { ""latitude"": 6.9117233753, ""longitude"": 79.85121154785, ""address"": """" } ] }, { ""subType"": ""public"", ""locations"": [ { ""latitude"": 6.86637047358571, ""longitude"": 79.8732092721, ""address"": """" }, { ""latitude"": 6.9117233753, ""longitude"": 79.85121154785, ""address"": """" } ] } ] } ] } } Following is an example request with all the optional parameters specified in the request. http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/931222/lat/6.87/lon/79.68?recLocType=dropoff Please also note the following header values included into the above request. X-EVOC-REC-LOCS-API-KEY: androidprod X-EVOC-REC-LOCS-API-SECRET: feaee65a-2f54-4b18-8a7e-ea409a2e0d72 X-CORRELATION-ID: 223e1203-0237-4770-89f6-31d65843f4c1 In the above request the requester has requested, Recommendation for the passengerId 931222. The requested recommendation type is “recommendation drop off” ( recLocType=dropoff) And following is an example response for the above request. { ""payload"":{ ""passengerId"":931222, ""lat"":6.78, ""lon"":79.67, ""recommendations"":[ { ""recMeta"":{ ""type"":""dropoff"" }, ""recPayload"":[ { ""subType"":""associated"", ""locations"":[ { ""latitude"":6.86637047358571, ""longitude"":79.8732092721, ""address"":"""" }, { ""latitude"":6.9117233753, ""longitude"":79.85121154785, ""address"":"""" } ] } ] } ] } } Following is an example request with all the optional parameters specified in the request. http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/931222/lat/6.87/lon/79.68?recLocType=all or http://<endpoint_addr>/iserv/v1/evoc/location/recommended/passenger/931222/lat/6.87/lon/79.68 Please also note the following header values included into the above request. X-EVOC-REC-LOCS-API-KEY: androidprod X-EVOC-REC-LOCS-API-SECRET: feaee65a-2f54-4b18-8a7e-ea409a2e0d72 X-CORRELATION-ID: 223e1203-0237-4770-89f6-31d65843f4c1 In the above request the requester has requested, Recommendation for the passengerId 931222. The requested recommendation type is “recommendation drop off and pickup” ( recLocType=all) And following is an example response for the above request. { ""payload"": { ""passengerId"": 931222, ""lat"": 6.78, ""lon"": 79.67, ""recommendations"": [ { ""recMeta"": { ""type"": ""pickup"" }, ""recPayload"": [ { ""subType"": ""private"", ""locations"": [ { ""latitude"": 6.86637047358571, ""longitude"": 79.8732092721, ""address"": """" }, { ""latitude"": 6.9117233753, ""longitude"": 79.85121154785, ""address"": """" } ] }, { ""subType"": ""public"", ""locations"": [ { ""latitude"": 6.86637047358571, ""longitude"": 79.8732092721, ""address"": """" }, { ""latitude"": 6.9117233753, ""longitude"": 79.85121154785, ""address"": """" } ] } ] }, { ""recMeta"": { ""type"": ""dropoff"" }, ""recPayload"": [ { ""subType"": ""generic"", ""locations"": [ { ""latitude"": 6.86637047358571, ""longitude"": 79.8732092721, ""address"": """" }, { ""latitude"": 6.9117233753, ""longitude"": 79.85121154785, ""address"": """" } ] } ] } ] } } Designated Pickup Following endpoints are extensions of the existing Recommended Location API developed by the Data Team.Through these endpoints, the operations teams should be able to UPLOAD, DELETE, and VIEW designated pickups . Designated Pickup Update Endpoint This endpoint allows users to upload files related to designated pickup locations. The following table includes the request schema for the designated pickup update endpoint of the Recommended Location API. URL http://<endpoint_addr>/iserv/v1/evoc/location/designated/upLocation/:fileName Description Endpoint to update designated pickups Request Method POST Response Schema (Success) application/json HTTP 200 (OK) response code. Response Schema (Error) Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Success Response { ""status"": ""success"", ""message"": ""File {fileName.json} added"" } Error Response { ""errors"":[ { ""code"":""(string)"", ""message"":""(string)"" } ] } Designated Pickup View All Files Endpoint URL http://<endpoint_addr>/iserv/v1/evoc/location/designated/viewAll Description Endpoint to view designated pickup location files Request Method GET Response Schema (Success) application/json HTTP 200 (OK) response code. Response Schema (Error) Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Success Response { ""designated-pickups"":{ ""files"":[ ""fileNameOne.json"", ""fileNameTwo.json"" ] } } Error Response { ""errors"":[ { ""code"":""(string)"", ""message"":""(string)"" } ] } Designated Pickup Delete Endpoint This endpoint allows users to delete files from the bucket. The following table includes the request schema for the designated pickup delete endpoint of the Recommended Location API. URL http://<endpoint_addr>/iserv/v1/evoc/location/designated/deleteLocation/:fileName Description Endpoint to update designated pickups Request Method DELETE Response Schema (Success) application/json HTTP 200 (OK) response code. Response Schema (Error) Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Success Response { ""status"": ""success"", ""message"": ""File {fileName.json} removed"" } Error Response { ""errors"":[ { ""code"":""(string)"", ""message"":""(string)"" } ] }",False
1363902503,page,current,/x/J4BLUQ,Travel Time Prediction Model,"Problem Introduction In the existing scenario, we have used both osrm api and google api. Estimation of travel time from driver location to passenger location is done by using osrm api while estimation of travel time from pick up location to drop location is done by google api. However, accuracy of osrm is being low and the cost of google api is being high was the problem of current implementation. Travel time prediction based on XGBoost Data Raw columns: TripId DriverId VehicleModel StartLatitude StartLongitude EndLatitude EndLongitude StartTime EndTime ActualDistance OSRMDistance WaitingTime OSRMTravelTime PredictedTravelTime Extracted features: ActualTravelTime (EndTime - StartTime) DayOfWeek (The day of the week with Monday=0, Sunday=6) HourBin AverageSpeed (ActualDistance / (ActualTravelTime - WaitingTime) ) DistanceRatio (OSRMDistance / ActualDistance) TimeRatio (WaitingTime / ActualTravelTime) StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude Our road trips can be divided into two phases. Accept to arrive Start to end So here we decided to provide two models for these two phases to predict travel time since it increases the prediction accuracy.",False
1395326984,page,current,/x/CAArUw,Dynamic Delivery Radius Discovery,"This analysis was conducted in order to find an optimum delivery radius for each restaurant in different time intervals. The sample dataset used for the analysis consist of all the completed food orders from September, 2020 to November, 2020. The restaurants which completed more than 100 orders in a particular time interval was considered for the analysis. Criteria: The second derivative of the graph between the cumulative revenue and the displacement from the restaurant was considered to decide the optimum delivery radius in a particular time interval. The displacement point where the minimum second derivative of the graph was considered as the optimum delivery radius. Measuring the performance: Optimization of the delivery radius might result in increase of profitability and decrease in delivery time of a food order. Thus, the delivery time can be used as an indicator of the performance of the new delivery radius schedule. This delivery time is the time between the order pickup time (status 6) and order delivered time (status 8). The average delivery time before and after applying the the optimum delivery schedule can be compared to measure the performance.",False
1742766647,page,current,/x/N4LgZw,Azkaban User Permissions Setup,"Azkaban has the flexibility to create users with different roles. These roles mainly differ from the permissions (access levels) they’ve been given. Once Azkaban web server is successfully installed, a separate configuration known as azkaban-users.xml will be created for user management. This file can be under the conf directory in the web server. Change directory to the web server - cd /usr/share/azkaban/azkaban-web-server-x.xx.x From the web server view the azkaban-user.xml file - cat ./conf/azkaban-users.xml If needed azkaban-user.xml file can be edited to create new users or user groups with different access levels. Azkaban permission can be given to a single user or a group of users. Following are the various permissions given by azkaban. Permission Description ADMIN Grants all access to everything in Azkaban. READ Gives users read only access to every project and their logs WRITE Allows users to upload files, change job properties or remove any project EXECUTE Allows users to trigger the execution of any flow SCHEDULE Users can add or remove schedules for any flows CREATEPROJECTS Allows users to create new projects if project creation is locked down When a project is created, the creator is automatically given an ADMIN status on the project. This allows the creator to view, upload, change jobs, run flows, delete and add user permissions to the project. An admin can remove other admins, but cannot remove themselves. By default any new user can create their own project and becoming it’s ADMIN, regardless of their permissions given in the azkaban-user.xml fille. Although that user cannot access projects which are created by others beyond the given permission level. In order to avoid unauthorized users from creating their own projects, set lockdown.create.projects to true in the azkaban.properties file. azkaban.properties file is located in conf directory in the web server. From the web server view the azkaban.properties file - cat ./conf/azkaban.properties Once a configuration file is edited. The web server needs to be restarted. This can be done by executing the ./shutdown-web.sh and ./start-web.sh bash scripts which are located in the bin directory in the web server. Here is a sample configuration xml <azkaban-users> <user groups=""azkaban"" password=""azkaban"" roles=""admin"" username=""azkaban""/> <user password=""metrics"" roles=""metrics"" username=""metrics""/> <user groups=""data_shuju"" password=""123456"" roles=""data_shuju"" username=""data_shuju""/> <role name=""data_shuju"" permissions=""READ,EXECUTE""/> <user username=""wangye"" password=""123456"" groups=""data_shuju""/> <user username=""changshichao"" password=""123456"" groups=""group_read_execute""/> <group name=""group_user"" roles=""user""/> <group name=""group_read_execute"" roles=""ree""/> <role name=""ree"" permissions=""READ,EXECUTE""/> <role name=""wangye"" permissions=""READ,EXECUTE""/> <role name=""admin"" permissions=""ADMIN""/> <role name=""metrics"" permissions=""METRICS""/> </azkaban-users>",False
1928233065,page,current,/x/aYDucg,driver profiling - initial analysis,"This was the initial exploration carried out to identify the variables related to the driver churn prediction. This document includes the features which can affect the driver churning. The driver profiling in the hailing platform can be done based on various characteristics related to the drivers and the rides. Following are the initial variables which were proposed in order to identify the driver behavior. Driver joined date Driver started date Passenger complaints Excess supply (no hires) Driver dissatisfaction Inquiries related to blocked/settlements Inquiries related to information Inquiries related to driver behavior Driver outstanding balance Driver blocking frequency Number of call center calls Walk-in frequency Driver incentives eligibility issues** Number of driver working days/ rejection driver ratings driver loyalty Driver details (age, gender) The data related to above variables have to be obtained from different tables and the information related to that are as follows. CRM tickets There are two types of tickets under CRM tables which are raised by drivers and passengers. Each ticket has been categorized under different categories and titles. There can be multiple entities under one ticket ID which explains the status of the ticket (whether its open, closed or in progress, etc.) These tickets can be raised by calling, e-mails, streaming and walk-ins for drivers. Passengers have usually raised tickets only by calling or streaming. Ticket categories - 'Bike', 'Complaints', 'Corporate', 'Dispatcher', 'Driver App', 'Driver Device', 'Driver Profile', 'Food', 'General', 'Inquiries', 'Marketing', 'Other', 'Passenger Profile', 'Payments', 'Registration', 'Request', 'Service', 'Stream', 'Truck' CRM Tokens The CRM tokens account for all the complaints, inquiries or requests raised by drivers by walk-in to the pickme. These tokens fall under two categories as driver and registration. All of these tokens are categorized into different tags which explains the nature of the complaint. Variables which can be extracted from CRM tickets/tokens Passenger complaints Excess supply (no hires) Driver dissatisfaction Inquiries related to blocked/settlements Inquiries related to information Inquiries related to driver behavior Number of call center calls Walk-in frequency FGL reports The rules in the FGL reports can be used to calculate the driver outstanding balance at the end of a particular day. Driver block reason Driver blocked reason can be obtained by fact driver blocked reason and also the blocking frequency can be calculated. Driver utilization The driver utilization can be obtained by driver country utilization table. This table defines the driver utilization under 3 statuses as active, free and busy. The driver is active or free means the driver is online at that time. The busy status defines the time a driver takes to reach a passenger and known as loss mileage. Dim drivers This table can be used to obtain driver demographic information. Fact rides This table can be use to obtain the active drivers within a considered period of time those who have worked on the hailing platform. daily driver ranking This table can be use to gather information on completed trips, rejected trip count, latest driver rating, driver loyalty label etc. Fact daily driver KPI This table can be used to acquire information on the true date where the driver started working. Also, this table provides information earnings and revenue.",False
1986887681,page,current,/x/AYBtdg,Projects,,False
1986887783,page,current,/x/Z4Btdg,2021-04-09 Data Science Initiatives,Date Participants List meeting participants using their @mention names: Goals We try to achieve following objectives Build clarity around data science initiatives Discuss bottlenecks/risks Collectively agree on the deliverable Discussion topics Time Item Presenter Notes 15 min Discuss the focus areas for Data Science Sanjaya Continuous improvements on the existing models Hypothesis testing Experimentation 10 min Data Science Roadmap Sanjaya Data Science & Engineering Roadmap Action items Add action items to close the loop on open questions or discussion topics: 33 incomplete Re-validate the priorities in the DSE roadmap 34 incomplete Review June commitments and understand requirements for experiments Decisions Type /decision to record the decisions you make in this meeting: e968e7cc-0a12-44f3-8fdc-165573f70769 DECIDED f3f42d19-b3dc-4140-bff0-3b43d389147e,False
1986953580,page,current,/x/bIFudg,2021-04-16 Data Science Initiatives,Date Participants List meeting participants using their @mention names: Goals We try to achieve following objectives Build clarity around data science initiatives Discuss bottlenecks/risks Collectively agree on the deliverable Discussion topics Time Item Presenter Notes 10 min Review Data Science Roadmap Sanjaya Data Science & Engineering Roadmap 10 min Food ETA Testing - approach & timelines Pulara 10 min Review Action items Sanjaya Action items Add action items to close the loop on open questions or discussion topics: 33 incomplete Re-validate the priorities in the DSE roadmap 34 incomplete Review June commitments and understand requirements for experiments Decisions Type /decision to record the decisions you make in this meeting: e968e7cc-0a12-44f3-8fdc-165573f70769 DECIDED f3f42d19-b3dc-4140-bff0-3b43d389147e,False
1986986247,page,current,/x/BwFvdg,Data Infrastructure Rationalization,"Driver Approver Contributors Stakeholder Objective Summarize the objective in one sentence Due date Key outcomes Report on infrastructure status, growth requirements and the environment landscape Status NOT STARTED Problem Statement The project would focus on reviewing the infrastructure usage, identify bottleneck and understand the growth requirements. Also it would analyze the requirements around setting up and environment landscape including dev, staging and production. Scope Review and report on the current infrastructure usage on the data platform. Explore opportunities for scaling down and requirements for scaling up Prepare a 5 year projection based on the current usage and the growth requirements Establish the environment landscape (dev, staging and prod) in par with the practices used by other teams Milestones and deadlines Plot out milestones and deadlines using the table below. Put each milestone on its own row, @mention milestone owners, and type /date to quickly add deadlines. Milestone Owner Deadline Status Review and report on the current infrastructure usage on the data platform. Explore opportunities for scaling down and requirements for scaling up e.g., Finalize designs for v1 NOT STARTED Prepare a 5 year projection based on the current usage and the growth requirements NOT STARTED Establish the environment landscape (dev, staging and prod) in par with the practices used by other teams NOT STARTED Reference materials Add links to relevant research and any other key documents https://docs.google.com/spreadsheets/d/1J7w3yR8ZYprPYWENWHTkQiKttLP9PJqkXN-5sig3KQ4/edit#gid=0",False
1987313730,page,current,/x/QgB0dg,Meeting Notes,This section is used to keep track of the meeting notes for all data team meetings.,False
1987707119,page,current,/x/7wB6dg,Meeting notes (2),"4adbdf04-fbbe-4bac-a5c6-a9bdf035d02f com.atlassian.confluence.plugins.confluence-business-blueprints:meeting-notes-blueprint Create meeting note Incomplete tasks from meetings BDDS 10 space:BDDS incomplete meeting-notes Decisions from meetings space = ""BDDS"" and label = ""meeting-notes"" All meeting notes 4adbdf04-fbbe-4bac-a5c6-a9bdf035d02f com.atlassian.confluence.plugins.confluence-business-blueprints:meeting-notes-blueprint meeting-notes Set meeting agendas, take notes, and share action items with your team. Meeting notes BDDS Create meeting note meeting-notes",False
1988296844,page,current,/x/jACDdg,Operational Reports Optimization,"Driver Approver Contributors Stakeholder Objective Optimize Operational Reports Due date Key outcomes Operational Reports can be executed with acceptable efficiency Quality assurance process available for rolling out reports Best practices and tools established around report design Alternative options evaluated for Mode Status NOT STARTED Problem Statement Operational reports are an important Data product produced and maintained by the data team and therefore ensuring the efficiency and the quality of this product is the main focus of this project. Currently following challenges are identified. Some reports fail or takes large amount of time to complete The resource usage during peak hours is very high and there is no opportunity for scalability The production environment is used for report design and it is extremely inefficient to experiment during peak hours The reports developed do not go through any verification or quality assurance process It is questionable if the ROI is delivered using Mode as the tool for building reports Solution Direction During the project we would try to establish the following Establish ownership of the reports and identify those that are no longer required Review the required reports and identify the opportunities for optimization. E.g. optimize the query or move the heavy lifting to a data pipeline. Ensure that manual reports only contains a direct data fetch or a simple join. Provide a development environment for report design Introduce a quality assurance process (similar to code reviews) for reports rolled-out Streamline the data requests and report requests (can we introduce a service desk?). We need to move away from emails Experiment KNIME analytics platform ( https://www.knime.com/open-source-vs-legacy ) for data exploration Evaluate alternative options for Mode (e.g. Apache Superset used by AirBnB) Milestones and deadlines Plot out milestones and deadlines using the table below. Put each milestone on its own row, @mention milestone owners, and type /date to quickly add deadlines. Milestone Owner Deadline Status Establish ownership of the reports and identify those that are no longer required Started Blue Optimize Required Reports Not Started Provide a development environment for report design Not Started Introduce a quality assurance process Not Started Streamline the data requests and report requests Not Started Experiment KNIME analytics platform Not Started Evaluate alternative options for Mode Not Started Reference materials Add links to relevant research and any other key documents",False
2017329283,page,current,/x/gwA_e,Analysis of the existing reports,Frequently used reports Scheduled Regional Activity ---> Region Performance By District (daily-regional) Food & Market Place ---> Foodie - Daily Summary (daily) Food & Market Place ---> Market Place - Daily Summary (daily) Customer Support Activity ---> Passenger Complaint Team KPI Report (daily) Customer Support Activity ---> New CRM Tickets For Agent Performance (daily) Promo Activity ---> Promo Usage Summary With Parameter (daily-taximodel wise) Not Scheduled Finance Operations ---> Region Details with Parameters Food & Market Place ---> CSV Bulk with time stamp,False
2017525796,page,current,/x/JABBe,Technical Docs,"This section is used to create technical specifications for the developments done by the data team. Technical specifications should be brief and include, Overview of the problem being solved Plan of implementation - what you are changing or adding new in ingestion pipelines, etl pipelines and microservices. Include diagrams where necessary . Non functional requirements (NFR) - state if there is any and how these would be addressed Reference to delivery artifacts such as API specifications, deployment instructions, how to guides etc. Plan for testing - state how you plan to test",False
2017525807,page,current,/x/LwBBe,ETL for calculating the daily cancellation count and updating the CRM tables,"Overview Provides counts of passenger, driver (driver support) and system user cancellations over the periods namely, last day, last 7 days, last 30 days, last 6 months (180 days) Plan of implementation A new ETL pipeline need to be created to get the counts of following types of cancelations. passenger cancellations: can be obtained from the one of the tables, passenger_log_archive, fact_trip, fact_unique_trip or fact_rides. The trips with only the statuses after_accepted or before_accepted has to be filtered out. The counts will seperatey be given to these statuses. The descriptions on the statuses can be found in the table dim_trip_statuses. driver (driver support) cancellations: There are two ways in which this type of cancelations can happen. Driver cancels through driver support and the rehailing takes place to a different driver, which can be found out with kafka event system rejected with type after accepted . The records are in the trip_flow table. Join records with travel status 8 in the trip_flow table with the hailing_driver_support_reason_log table in order to verify whether it is a driver support cancellation or not. system user cancellations: Join the records with travel status 8 in the trip_flow table with the hailing_driver_support_reason_log table to get the system user cancelations. It can be identified with two modes of cancelations which are “dispatcher cancel“ and “passenger portal cancel“ The date ranges required are last day, last 7 days, last 30 days and last 6 months (180 days). As aggregation of all the data for a given date range on a daily basis can be inefficient, intermediate tables for each date range will be maintained so that the relevant counts of yesterday will be added and the counts of day before given date range will be subtracted. A new flow needs to be added in the dp-crm-vtiger-export project along with the relevant hive and sql table creation scripts and hive ingestion script. Proposed output table (CancellationCounts) schema in Vtiger DB: column type comment driverId int Id of the driver TravelStatus VARCHAR (50) trip status which is mapped from the dim_trip_statuses table cancellationCount int number of cancellations in the given category dateRange VARCHAR (25) past number of days considered in which the cancellations occured cancellationType VARCHAR (25) Type of cancelation: Passenger, Driver Support or System User Plan for testing Hive queries of the ETL can be tested from the dev environment. As there is no dev access for Vtiger DB, the end to end testing needs to be done in the prod environment.",False
2017525894,page,current,/x/hgBBe,Report Specification - Template,Purpose Business Owners Departments Manual/Scheduled Frequency Scheduled time Approximate Runtime Sample report Output Data Schema Table columns and data types Source table information Source tables where the report data is extracted from Report Logic Describe how the report is built,False
2017788030,page,current,/x/fgBFe,Report Specifications,,False
2019459220,page,current,/x/lIBee,CleverTap Event Integration Technical Specification,"Overview The purpose of this document is to detail the logical steps of processing and filtering needed to formulate the required CleverTap events as of the shared document , as well as provide documentation for the implementation plan publishing missing information to capture all the CleverTap events. This document will not focus on events that do not fall under the data services domain. The technical document detailing the logic for calculating the CleverTap profile attributes can be found here . Following is the yet to be integrated CleverTap events that require capturing. Details on each event will be provided in relevant sections. Event name Information that needs to be captured / event Properties Tier_jump Current tier - Platinum Previous tier - Gold Tier_fall Current tier - Platinum Previous tier - Gold about_jump Current tier - Silver Next tier - Gold about_fall Falling tier - Silver about_secure Current tier - Gold Event Formulation This section describes the logic required for the curation of the required CleverTap events. This section only describes the events that have not been integrated as of April 23, 2021. Tier Jump (Tier_jump) Tier Jumps are tier transitions that are from a tier to a higher tier, or from the highest tier (Platinum) to itself when the relevant tier points threshold is reached. Tier Jump events can be curated as follows: For each user u who has fired a tier_renewal event e { if ( e.body.renewal_type = “JUMPED” OR e.body.old_tier = e.body.new_tier = “Platinum” ) { TierJumpEvent ← (Create Tier jump event for user u ) TierJumpEvent.previous_tier ← e.body.old_tier TierJumpEvent.current_tier ← e.body.new_tier } } Tier Fall (Tier_fall) Tier Falls are tier transitions that are from a tier to a lower tier or the same tier (The only exception being the highest tier, Platinum). This happens when the target points threshold is not reached during the tier loyalty period. Tier Fall events can be curated as follows: For Each user u who has fired a tier_renewal event e { If ( e.body.renewal_type = “DROPPED” OR ( e.body.renewal_type = “NOCHANGE” AND ` e.body.new_tier != “Platinum” ) ) { TierFallEvent ← (Create Tier fall event for user u ) TierFallEvent.previous_tier ← e.body.old_tier TierFallEvent.current_tier ← e.body.new_tier } } About to Jump (about_jump) About to jump is a type of event that gets triggered for users who have secured their current tier. This gets triggered when the user’s current points reaches a predefined threshold respective to the target tier points threshold and the remaining days to tier expiry is within a predefined days boundary. For Each user u who has fired a threshold_reached event e , and daily_snapshot event d { If ( e.body.threshold_type = “ABOUT_TO_JUMP” ) { AboutJumpEvent ← (Create Tier jump event for user u ) AboutJumpEvent.current_tier ← d.body.current_tier AboutJumpEvent.next_tier ← e.body.target_tier } } About to Fall (about_fall) About to fall is a type of event that gets triggered for users who have not secured their current tier. This gets triggered when the user’s current points reaches a predefined threshold respective to the current tier points threshold and the remaining days to tier expiry is within a predefined days boundary. For Each user u who has fired a threshold_reached event e , and daily_snapshot event d { If ( e.body.threshold_type = “ABOUT_TO_FALL” ) { AboutFallEvent ← (Create Tier jump event for user u ) AboutFallEvent.falling_tier ← d.body.highest_secured_tier } } Non functional requirements (NFRs) Add minimal execution impact on the existing Customer Loyalty jobs. Reference to delivery artifacts <> Plan for testing The newly added logic will be manually tested using a handcrafted dataframe(s) to output the desired/correct dataframes.",False
2022670341,page,current,/x/BYCPe,2021-04-23 Data Science Initiatives,Date Participants List meeting participants using their @mention names: Goals We try to achieve following objectives Build clarity around data science initiatives Discuss bottlenecks/risks Collectively agree on the deliverable Discussion topics Time Item Presenter Notes 5 min Review Data Science Roadmap Sanjaya Data Science & Engineering Roadmap 5 min Food Promotions - status update Pulara 5 min Food ETA Testing - performance evaluation Sanjaya 5 min Fulfilment failures - discussion Sanjaya 10 min Review Action items Sanjaya Action items Add action items to close the loop on open questions or discussion topics: 33 complete Re-validate the priorities in the DSE roadmap 34 incomplete Review June commitments and understand requirements for experiments Decisions Type /decision to record the decisions you make in this meeting: e968e7cc-0a12-44f3-8fdc-165573f70769 DECIDED f3f42d19-b3dc-4140-bff0-3b43d389147e,False
2022998121,page,current,/x/aYCUe,Corporate ID Wise Credit Limit List,"Overview This task is to build a data pipeline to ingest data from the companyinfo table in Eztaxi mysql database to Hive platform. Plan of implementation Schema of the Companyinfo table Column Type Comment id int(100) Auto Increment company_cid int(100) company_domain varchar(500) customer_app_url varchar(500) driver_app_url varchar(500) company_app_name varchar(250) company_app_description longtext company_meta_title varchar(255) company_meta_keyword varchar(255) company_meta_description varchar(255) company_email_id varchar(250) company_phone_number varchar(20) company_currency_format varchar(11) company_created_date timestamp [ CURRENT_TIMESTAMP ] company_update_date timestamp [ 0000-00-00 00:00:00 ] company_currency varchar(11) payment_method varchar(1) company_paypal_username text company_paypal_password longtext company_paypal_signature longtext company_tagline varchar(50) company_copyrights varchar(250) company_logo varchar(500) company_favicon text company_facebook_key varchar(100) company_facebook_secretkey varchar(100) company_facebook_share text company_twitter_share text company_google_share text company_linkedin_share text company_notification_settings int(11) In seconds company_tax double cancellation_fare enum('1','0') [ 0 ] 0 -NO,1- Yes company_sms_enable enum('1','0') [ 0 ] 1-Enabled,2-Disabled passenger_setting int(3) [ 2 ] 1 - Server will select the nearest taxi and dispatch, 2 - Passenger able to select the taxi company_api_key varchar(200) company_time_zone varchar(100) home_page_title varchar(500) home_page_content longtext default_unit enum('1','0') [ 1 ] 0-KM , 1-MILES skip_credit_card enum('1','0') [ 1 ] 1-Skip , 0-No-Skip fare_calculation_type enum('1','2','3') [ 3 ] 1 => Distance, 2=> Time, 3=> Distance / Time credit_limit int(100) [ 0 ] credit_remark varchar(250) NULL email_status enum('0','1','2','3') [ 0 ] 0 - Clear Limit | 1 - 80% of limit is exceeded | 2 - 90% of limit is exceeded | 3 - 100% of limit is exceeded active_deactive_reason varchar(500) NULL Task can be performed by adding a new workflow to the available dp-hailing-raw project. In this task all the columns in the company table are going to be ingested and loaded to the hive platform. Changes to the db_hailing_row project Workflow to ingest yesterday data branch - fb-companyinfo-tables SQL script to ingest data from database src/main/sqoop -> companyinfo_table_query.sql Hive script to load data to hive table src/main/hive -> hive_companyinfo_table.hql Add the companyinfo table to dev.gradle & prod.gradle in dev.gradle, under coporates key in table_sets in prod.gradle, under coporates key in table_sets Workflow to ingest data for a given time range branch - fb-one-time-companyinfo-ingestion SQL script to ingest data from database src/main/sqoop -> companyinfo_table_qury.sql Hive script to load data to hive table src/main/hive -> hive_companyinfo_table.hql Add the companyinfo table to dev.gradle & prod.gradle in dev.gradle, under coporates key in table_sets in prod.gradle, under coporates key in table_sets added the start_date and end_date variable to workflow.gradle and sqoop_table.sh",False
2037710872,page,current,/x/GAB1eQ,Data extraction of inactive drivers,Overview The objective is to develop a data pipeline and that extracts the list of drivers who hasn't done any trip for last 180 days. Frequency - daily Project - dp-pd-driver-tagger Proposed output table schema - hive_db_stage.inactive_drivers Column Name Data Type Comment driverid INT id of the driver date_time STRING date_time partition External location - wasbs://hive-db-stage@hdinhpdevshrprimpkme.blob.core.windows.net/daily_kpi/inactive_drivers Plan of implementation data sources - hive_db_stage.fact_daily_driver_kpi hive_db_stage.dim_driver Required fields from the source table - hive_db_stage.fact_daily_driver_kpi driverid dispatchedtripcount mobiletripcount datekey Required fields from the source table - hive_db_stage.dim_driver driverid status Find the drivers who have done at least 1 mobile trip or dispatched trip for the past 180 days from fact_daily_driver_kpi . Get the sum of each of those drivers trip counts Right join the above data frame with all the drivers who aren’t already permanently deactivated from dim_driver . Extract the rest of the drivers who haven’t done any trips for the past 180 days from the joined data frame. Write the output to the destination table with the previous day partition and copy the data to GCP in csv format. Plan for testing The hive query can be tested from the dev environment. SRS Link SRS - DATA - Automate the process of permanently deactivating drivers,False
2048098346,page,current,/x/KoATeg,Extracting Incentives Information Technical Specification,"Overview The purpose of this document is to propose the technical solution for the above-titled requirement. The requirement of this task is explained through the SRS / BRS . The following information should be provided through Data dump: Driver ID His Eligible Incentive name Remaining time/Expire Period Completion progress and relevant values If additional incentives available that information also should be provided. If multiple incentives are available, incentives should be sorted based on the incentive which has the highest progress. If there aren't any values to be provided, '0' should be provided. Assumptions The driver is eligible for one or many weekly incentive schemas. Proposed Driver Incentive Table schema Column Name Data Type Comment driverid int Id of the driver taximodel int ID of the taxi model strategyname string Name of the strategy used incentivetarget int Minimum target of trips to be achieved incentiveamount decimal(18,2) Driver incentive amount to be awarded expiredate datetime last day incentive eligibility is valid for minqualifiedtarget int Minimum qualified target of the driver maxachievedtarget int Maximum achieved target payableincentiveamount decimal(18,2) Driver incentive amount to be awarded totaltrips int Total trips totallegitimatetrips int Total legitimate trips fraudtrips int Total fraud trips of the driver itctrips int Total Immediate trip Cancellations hfctrips int Total Hire Fee Incentive Trips rpptrips int Total road pick up trips completionratiotarget decimal(5,2) Minimum required completion ratio achievedcompletionratio decimal(5,2) Maximum achieved completion ratio systemrejectedtrips int Timed out trip count manualrejectedtrips int Manually rejected trip count status int - updateddatetime datetime - Plan of Implementation Create an Intermediate table and export table using the sqoop loader. Source Tables hive_db_stage.incentive_achievements hive_db_stage.incentive_qualifiers Required fields from source table : hive_db_stage.incentive_achievements driverid taximodel minqualifiedtarget maxachievedtarget payableincentiveamount totaltrips totallegitimatetrips fraudtrips itctrips hfctrips rpptrips completionratiotarget achievedcompletionratio systemrejectedtrips manualrejectedtrips Required fields from source table : hive_db_stage.incentive_qualifiers driverid strategyname incentivetarget incentiveamount eligibilityperiod Expire date calculate using the eligibility period. And the status will be 0. Hive Table Loction 'wasbs://hive-db-stage@hdinhpdevshrprimpkme.blob.core.windows.net/incentives/incentives_information' Non-Functional Requirements (NRF) Reference to Delivery Artifacts Dependency Workflow : Plan for Testing",False
2052161880,page,current,/x/WIFReg,How to Articles,This section contains informative articles.,False
2053275921,page,current,/x/EYFieg,Containerization of Data Services,"This document outlines the process of containerizing services using docker and deploying these services in Azure Kubernetes Cluster (AKS) Code Repositories Docker Utils Repo https://git.mytaxi.lk/pickme/data-services/docker-utils This repository contains the common utilities required for dockerization. For example a base docker image is available which could be extended to create the individual docker images for the microservices. Kube Deployments Repo https://git.mytaxi.lk/pickme/data-services/kube-deployments This repository contains the kubernetes deployment YAML files for each environment Architecture Overview - containerized deployment The data required by the microservices are populated in the HBase cluster which sits in the HDInsight VNet. The microservices deployed would be inside the AKS VNet and therefore VNet to VNet tunnel is setup to enable connectivity. Reference https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager-portal In the stage 1 the docker images would be build locally and pushed to container registry. The Kubernetes deployments would also be triggered manually. Containerization Steps First step is to install docker in your local environment. Follow the instructions here to install the appropriate version for your environment. The individual service images would be built based on the base docker image. To build the base docker image follow the below steps. git clone https://git.mytaxi.lk/pickme/data-services/docker-utils.git cd docker-utils/base-images/ sudo docker-compose up #Verify that the base image is created sudo docker images Next step is to create the Dockerfile for the microservice. Follow the below steps. cd <project_home> vi Dockerfile #In most cases simply creating an image on the base image would be sufficient. #Enter the following line in the Docker file FROM docker.pickme.lk/base-service:1.0.0 Create a docker-compose.yaml file and enter the following details. Use appropriate values for the container_name and the image. Note that the ARCHIVE_NAME should be the name of the archive created during the build process without the version number. version: '3' services: deliveryeta: container_name: deliveryeta build: context: . args: - ARCHIVE_NAME=mserv-delivery-eta image: docker.pickme.lk/deliveryeta:latest ports: - ""8010:8082"" Finally create a shell script named docker-start.sh to build the code, build the docker image and start the container. This should have the following content. #!/usr/bin/env bash #clean sbt clean #compile sbt compile #build archive sbt dist #build docker image sudo docker-compose build #start container sudo docker-compose up Pushing containers to container registry Login to the docker registry sudo docker login dsregistrypickme.azurecr.io Use the credentials available in the container registry portal credentials page Tag the image sudo docker tag docker.pickme.lk/deliveryeta:latest dsregistrypickme.azurecr.io/deliveryeta:1.0.3 Push the image to the registry sudo docker push dsregistrypickme.azurecr.io/deliveryeta:1.0.3 Deploying the Services Note : the AKS deployment requires access to the Azure account and hence would only be done by the authorized users. Following are some of the prerequisites for connecting to AKS. Install Azure cli as mentioned here https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt If you are provided with a service principal, use the following command to login to Azure Cli az login --service-principal -u <name url> -p <password> --tenant <tenant> Install kubectl using the following commands az aks install-cli Connect to the kubernetes cluster using following commands az aks get-credentials --resource-group myResourceGroup --name myAKSCluster Verify the connectivity through kubectl get nodes The deployment yaml files are available in the kube-deployments repo. Follow the steps below for deployment. git clone https://git.mytaxi.lk/pickme/data-services/kube-deployments # Navigate to the deployment file related to the service/services # Deploy kubectl apply -f deliveryeta-deployment.yaml # Undeploy kubectl delete -f deliveryeta-deployment.yaml # To find the public ip of the Load Balancer kubectl get service deliveryeta --watch Useful Docker commands # List Docker images sudo docker images # Remove Docker image sudo docker rmi <image name> # List running containers sudo docker container ls # View logs sudo docker logs <container name> # Navigate to a container sudo docker exec -it <container name> bash Useful Kubectl commands # List nodes kubectl get nodes # List pods kubectl get pods # Monitor logs kubectl logs <pod name> --follow # Navigate to a container kubectl exec -it <pod name> bash",False
2063106110,page,current,/x/PoD4eg,Model retraining V2.0.0,"Accept to arrive Basic filtration All trips completed between 23rd Sep and 22nd Oct were filtered based on following basic logics. removed the trips which have waiting time greater than or equal to actual travel time ( TimeRatio >= 1) removed the trips which have average speed over 100 km/h removed the trips which have actual distance over 20 km removed the trips which are not happened with 1,2,3 or 4 taxi models Distribution of continuous features Actual Distance 2. Average Speed 3. Distance Ratio 4. Time Ratio Distribution of discrete features After the analysis, remove the outlier based on following features. AverageSpeed (LF: 0.008 km/h, UF: 27.428 km/h) TimeRatio (LF: 0, UF: 0.87) DistanceRatio (LF: 0.47, UF: 1.39) ActualDistance (LF: 0.001 km, UF: 3.385 km) Start to end Basic filtration All trips completed between 23rd Sep and 22nd Oct were filtered based on following logics. removed the trips which have waiting time greater than or equal to actual travel time ( TimeRatio >= 1) removed the trips which have average speed over 100 km/h removed the trips which have actual distance over 200 km removed the trips which are not happened with 1,2,3 or 4 taxi models Distribution of continuous features Actual Distance 2. Average Speed 3. Time Ratio 4. Distance Ratio Distribution of discrete features After the analysis, remove the outlier based on following features. AverageSpeed (LF: 9.647 km/h, UF: 36.632 km/h) TimeRatio (LF: 0, UF: 0.52) DistanceRatio (LF: 0.73, UF: 1.22) ActualDistance (LF: 0.001 km, UF: 19.965 km) Features considered for modeling DayOfWeek HourBin StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance Results To measure the accuracy of the predicted travel time, MAE (Mean absolute error) and RMSE (Root mean square error) was used. MAE measures the average magnitude of the errors in a set of predictions without considering their direction. This takes the average over the test sample of the absolute difference between prediction and actual observation where all individual differences have equal weight. RMSE on the other hand is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation. Hence it gives more weight to larger errors than smaller ones. XGBoost algorithm Model Feature combinations Estimator Accept to arrive Start to end Train (s) Test (s) Train (s) Test (s) XGBoost Treat as numerical features. DayOfWeek HourBin StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance RMSE 169.613 169.450 371.587 377.669 MAE 117.161 117.629 232.631 236.209 Average (ActualTravelTime) 366.249 365.886 1220.963 1222.520 Average (PredictedTravelTime) 366.248 366.341 1156.895 1158.110 Median (ActualTravelTime) 322.0 322.0 1023.0 1026.0 Median (PredictedTravelTime) 350.666 350.814 1002.223 1005.107 Standard deviation (ActualTravelTime) 221.556 219.991 790.785 790.577 Standard deviation (PredictedTravelTime) 139.605 139.434 656.310 655.923 Treat as categorical features. DayOfWeek HourBin Treat as numerical features. StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance RMSE 169.911 169.768 377.888 383.735 MAE 117.437 117.893 237.510 240.805 Average (ActualTravelTime) 366.249 365.886 1220.963 1222.520 Average (PredictedTravelTime) 366.248 366.333 1156.940 1157.961 Median (ActualTravelTime) 322.0 322.0 1023.0 1026.0 Median (PredictedTravelTime) 350.948 351.032 1003.436 1005.435 Standard deviation (ActualTravelTime) 221.556 219.991 790.785 790.577 Standard deviation (PredictedTravelTime) 139.322 139.122 650.646 650.034 Treat as categorical features. DayOfWeek (One-hot encoded) HourBin (One-hot encoded) StartHexID (Label encoded) EndHexID (Label encoded) Treat as numerical features. OSRMDistance RMSE 170.425 170.082 380.436 385.991 MAE 117.785 118.171 240.986 244.182 Average (ActualTravelTime) 366.249 365.886 1220.963 1222.520 Average (PredictedTravelTime) 366.247 366.327 1163.344 1164.499 Median (ActualTravelTime) 322.0 322.0 1023.0 1026.0 Median (PredictedTravelTime) 351.107 351.307 1009.413 1011.386 Standard deviation (ActualTravelTime) 221.556 219.991 790.785 790.577 Standard deviation (PredictedTravelTime) 138.724 138.536 653.351 652.972 Treat as cyclical features. DayOfWeek DayOfWeekSin DayOfWeekCos HourBin HourBinSin HourBinCos Treat as numerical features. StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance (Normalized using log(1+x)) RMSE 169.304 169.220 370.448 377.405 MAE 116.949 117.490 232.164 236.088 Average (ActualTravelTime) 366.249 365.886 1220.963 1222.520 Average (PredictedTravelTime) 366.248 366.326 1156.881 1157.991 Median (ActualTravelTime) 322.0 322.0 1023.0 1026.0 Median (PredictedTravelTime) 350.536 350.703 1001.134 1003.636 Standard deviation (ActualTravelTime) 221.556 219.991 790.785 790.577 Standard deviation (PredictedTravelTime) 139.855 139.622 657.089 656.378 Other tried algorithms Model Feature combinations Estimator Accept to arrive Start to end Train (s) Test (s) Train (s) Test (s) CatBoost Treat as categorical features. DayOfWeek HourBin Treat as numerical features. StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance RMSE 171.510 170.244 358.285 362.503 MAE 118.329 118.277 237.242 239.179 Average (ActualTravelTime) 366.249 365.886 1220.963 1222.520 Average (PredictedTravelTime) 366.060 366.212 1220.640 1221.703 Median (ActualTravelTime) 322.0 322.0 1023.0 1026.0 Median (PredictedTravelTime) 351.128 351.236 1052.164 1055.072 Standard deviation (ActualTravelTime) 221.556 219.991 790.785 790.577 Standard deviation (PredictedTravelTime) 138.842 138.636 701.043 700.309 LightGBM Treat as categorical features. DayOfWeek HourBin Treat as numerical features. StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance RMSE 169.649 169.212 379.900 380.979 MAE 117.067 117.520 254.604 254.909 Average (ActualTravelTime) 366.249 365.886 1220.963 1222.520 Average (PredictedTravelTime) 366.249 366.318 1220.950 1221.915 Median (ActualTravelTime) 322.0 322.0 1023.0 1026.0 Median (PredictedTravelTime) 350.293 350.288 1066.240 1067.473 Standard deviation (ActualTravelTime) 221.556 219.991 790.785 790.577 Standard deviation (PredictedTravelTime) 140.090 139.834 649.887 649.572 According to the above results, when training the XGBoost model, we decided to treat DayOfWeek and HourBin feature as cyclical while StartHexCenterLatitude, StartHexCenterLongitude, EndHexCenterLatitude , EndHexCenterLongitude, OSRMDistance features as numerical. Model Hyper-parameter tuning Hyperparameters are certain values or weights that determine the learning process of an algorithm. In tree-based models like XGBoost, hyperparameters include things like the maximum depth of the tree and the number of trees to grow. For our models, we fine-tuned two hyperparameters using grid search and 5 fold cross validation. The ranges of possible values that we considered for each are as follows: parameters = { 'learning_rate': [0.1], #Fixed learning rate 'max_depth': [5, 6, 7, 8, 9, 10], 'n_estimators': [200, 250, 300, 350, 400, 450] } Other XGBoost parameters were set to default values. Accept to arrive model n_estimators vs neg_root_mean_squared_error Start to end model n_estimators vs neg_root_mean_squared_error We found that best results were given to the accept-to-arrive model by 450 decision tree estimators and maximum depth of 10. Also best results were given for the start-to-end model by 450 decision tree estimators and maximum depth of 9. But since there is no significant improvement between maximum depth of 8 and above mentioned best values, we used maximum depth of 8 for the model training. Final Model Evaluation Using the above best parameter combination, we trained the final model and model evaluation metrics are mentioned below. Model Feature combinations Estimator Accept to arrive Start to end Train (s) Test (s) Train (s) Test (s) XGBoost Treat as cyclical features. DayOfWeek DayOfWeekSin DayOfWeekCos HourBin HourBinSin HourBinCos Treat as numerical features. StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance Size of the dataset 1,097,808 274,453 1,017,280 254,321 RMSE 155.567 167.500 325.860 359.536 MAE 108.883 116.166 219.210 236.681 Average (ActualTravelTime) 366.249 365.886 1220.963 1222.520 Average (PredictedTravelTime) 366.233 366.482 1220.930 1222.656 Median (ActualTravelTime) 322.0 322.0 1023.0 1026.0 Median (PredictedTravelTime) 348.190 348.862 1050.468 1054.111 Standard deviation (ActualTravelTime) 221.556 219.991 790.785 790.577 Standard deviation (PredictedTravelTime) 144.413 142.844 706.409 704.735 Performance comparison using validation set (152,393) is mentioned below. Model RMSE (s) MAE (s) Accept to arrive Start to end Accept to arrive Start to end Improved Model 188.870 340.463 121.716 241.255 Previous Model 225.167 405.913 144.146 273.918 OSRM 317.197 631.316 241.865 483.428 According to above results we can see the about 16% improvement of both start-to-end model and accept-to-arrive model. Bench-marking For the bench-marking, we used a validation set of data of the time period between 23rd of october and 28th of october 2020 and compared with the trained model predictions with OSRM estimations and OLD model predictions. Accept to arrive model Mean Absolute Error variation along with distances Root mean square Error variation along with distances Start to end model Mean Absolute Error variation along with distances Root mean square Error variation along with distances",False
2063204484,page,current,/x/hAD6eg,Model retraining V2.1.0,"Accept to arrive Basic filtration All trips completed between 1st Jan 2021 and 31st March 2021 were filtered based on following basic logics. Removed the trips which have waiting time greater than or equal to actual travel time ( TimeRatio >= 1) Removed the trips which have actual travel time less than 0 (Due to start time and end time recording errors) Removed the trips which have average speed over 100 km/h Removed the trips which have actual distance over 20 km Removed the trips which are not happened with 1,2,3 or 4 taxi models Distribution of continuous features Actual Distance 2. Average Speed 3. Distance Ratio 4. Time Ratio Distribution of discrete features After the analysis, remove the outlier based on following features. AverageSpeed (LF: 3.215 km/h, UF: 33.508 km/h) TimeRatio (LF: 0, UF: 0.87) DistanceRatio (LF: 0.56, UF: 1.34) ActualDistance (LF: 0.001 km, UF: 3.844 km) Start to end Basic filtration All trips completed between 1st Jan 2021 and 31st March 2021 were filtered based on following logics. Removed the trips which have waiting time greater than or equal to actual travel time ( TimeRatio >= 1) Removed the trips which have actual travel time less than 0 (Due to start time and end time recording errors) Removed the trips which have average speed over 100 km/h Removed the trips which have actual distance over 200 km Removed the trips which are not happened with 1,2,3 or 4 taxi models Distribution of continuous features Actual Distance 2. Average Speed 3. Time Ratio 4. Distance Ratio Distribution of discrete features After the analysis, remove the outlier based on following features. AverageSpeed (LF: 10.173 km/h, UF: 37.233 km/h) TimeRatio (LF: 0, UF: 0.52) DistanceRatio (LF: 0.75, UF: 1.2) ActualDistance (LF: 0.001 km, UF: 21.6 km) Features considered for modeling DayOfWeek DayOfWeekSin DayOfWeekCos HourBin HourBinSin HourBinCos StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance Final Model Evaluation To measure the accuracy of the predicted travel time, MAE (Mean absolute error) and RMSE (Root mean square error) was used. MAE measures the average magnitude of the errors in a set of predictions without considering their direction. This takes the average over the test sample of the absolute difference between prediction and actual observation where all individual differences have equal weight. RMSE on the other hand is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation. Hence it gives more weight to larger errors than smaller ones. Latest trained model (V2.1.0) evaluation metrics are mentioned below. Model Feature combinations Estimator Accept to arrive Start to end Train (s) Test (s) Train (s) Test (s) XGBoost Treat as cyclical features. DayOfWeek DayOfWeekSin DayOfWeekCos HourBin HourBinSin HourBinCos Treat as numerical features. StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRMDistance Size of the dataset 3,153,044 788,261 3,021,213 755,304 RMSE 164.008 166.807 327.684 338.848 MAE 112.262 113.924 215.862 220.773 Average (ActualTravelTime) 375.325 375.392 1234.158 1233.765 Average (PredictedTravelTime) 375.321 375.390 1234.153 1233.110 Median (ActualTravelTime) 331.0 330.0 1027.0 1026.0 Median (PredictedTravelTime) 356.044 356.069 1051.906 1049.701 Standard deviation (ActualTravelTime) 232.243 231.567 810.621 811.828 Standard deviation (PredictedTravelTime) 158.624 158.400 736.104 736.004 Bench-marking For the bench-marking, we used a validation set of data of the time period between 1st of April 2021 and 30th of April 2021 and compared with the trained model predictions with OSRM estimations and OLD model (model V2.0.0 ) predictions Performance comparison using validation set (1,538,613) is mentioned below. Model RMSE (s) MAE (s) Accept to arrive Start to end Accept to arrive Start to end Latest Model (V2.1.0) 190.288 362.909 126.350 237.073 Previous Model (V2.0.0) 209.800 386.532 147.132 253.157 OSRM 338.635 825.027 264.028 630.913 According to above results we can see the about 9% improvement of the accept-to-arrive model and 6% improvement of the start-to-end model with respect to the previous model (V2.0.0) . Accept to arrive model Mean Absolute Error variation along with distances Root mean square Error variation along with distances Start to end model Mean Absolute Error variation along with distances Root mean square Error variation along with distances",False
2080440398,page,current,/x/TgABf,Incentives Generalization,"Overview Objective of this implementation is to build a generic framework for calculating incentives and analyzing performance of the incentives. Incentives are expected to boost the supply by encouraging drivers and riders to complete more trips/deliveries. Scope Currently flash, food and hailing incentives have been launched and following are the expected outcomes of the full implementation. Support weekend and daily incentives through the incentives engine. Provide an API to configure incentives (prioritized for one day incentives) Move entire incentive calculation to the incentives engine (currently hailing and food are standalone jobs) At a conceptual level, following are the key changes into the current functionality Introduction of the concept of incentive category - daily/weekly/oneday Effective from and to dates Ability to configure applicable driver cohorts The proposed UI for daily incentive calculation is shown below. Plan of Implementation The table structure to store incentives rules are show below Please see https://app.diagrams.net/#G1HgT3RvnDgYmwSqgyDp8YjVt4ahwI2kAB The main implementation tasks would involve Introducing a new entity incentives which would define the incentive ref - auto generated reference which encapsulate the incentive type, category , dates and a sequence number title - title produced by the UI effectivefrom and effectiveto - effective date range. For one day incentives this would be the same date servicetypes drivercohort - as specified from the UI for oneday incentives. Could be null in which case it would be applicable to all remarks - as specified from the UI daysconsidered - how many days are considered for determining the applicable strategy. This is not applicable for one day incentives so the value may be 0 incentivecategory - weekly, weekend, daily incentivetype - food, hailing etc. Existing incentive_strategies table would remain the same except that the incentivetype is moved out to incentive table. This table is not applicable for one day incentives given that the concept of limit based strategies are not applicable. However to make calculation generalized an entry would be added here where range start would be 0 and rangeend would be a large number. incentive_strategy_target table would remain the same The API development would involve providing CRUD operations on the incentives data. The upload of driver cohorts would be initially through a manual upload Changes to the Data Pipelines Changes to the API External References BRS - BRS - Driver Portal for Oneday Incentives (taxi & food) SRS - [DriverPortal] SRS - One-day Incentives Upload Capability",False
2080441189,page,current,/x/ZQMBf,Ingesting the driver cohort data from the Eztaxi DB,"Overview Objective of this task is to implement data pipelines to ingest yesterday data in dr_cohort, dr_cohort_driver, dr_cohort_types tables. Plan of Implementation Implementing three pipelines to ingest data from each tables dr_cohort, dr_cohort_driver, dr_cohort_types. These tables are in the Eztaxi DB. Schema of the dr_cohort Column Name Data Type Comment id int(11) Auto Increment name varchar(50) service_type int(11) NULL cohort_type int(11) description text NULL created_date datetime NULL modified_date datetime NULL status char(1) NULL [ A ] ‘A’ , ‘D’ Schema of the dr_cohort_types Column Name Data Type Comment id int(11) Auto Increment The id of the cohort type. This is the primary key of the table. name varchar(255) cohort name. type_key varchar(255) cohort key. created_at datetime [ CURRENT_TIMESTAMP ] The created time of the cohort type. updated_at datetime NULL The updated time of the cohort type. status char(1) NULL [ A ] status of the cohort type. Schema of the dr_cohort_driver Column Name Data Type Comment id int(11) Auto Increment int(11) NULL driver_id int(11) NULL created_date datetime NULL modified_date datetime NULL status char(1) NULL [ A ] ‘A’ , ‘D’ Plan for testing Pipelines can be tested by executing the workflows in dev environment and comparing the output.",False
2080473089,page,current,/x/AYABf,2021-05-07 Data Science Initiatives,Date Participants List meeting participants using their @mention names: Goals We try to achieve following objectives Build clarity around data science initiatives Discuss bottlenecks/risks Collectively agree on the deliverable Discussion topics Time Item Presenter Notes 5 min Review Data Science Roadmap Sanjaya Data Science & Engineering Roadmap 5 min Establish an implementation framework for all projects till August release Sanjaya 10 min Review Action items Sanjaya Action items Add action items to close the loop on open questions or discussion topics: 33 complete Re-validate the priorities in the DSE roadmap 41 complete Prioritize Search in the Roadmap 34 incomplete Review June commitments and understand requirements for experiments Decisions Type /decision to record the decisions you make in this meeting: e968e7cc-0a12-44f3-8fdc-165573f70769 DECIDED f3f42d19-b3dc-4140-bff0-3b43d389147e,False
2080899252,page,current,/x/tAAIf,Customer Loyalty State Processor Flow,"Overview The purpose of this document is to give an understanding of what the activities of the logic are, and the flow of it that defines the customer loyalty state processor; which is the job that is responsible for the customer loyalty related logic from the data services perspective. Customer loyalty state processor is a daily job that updates the customer’s loyalty state at the end of each day by accumulating that day’s trip points to the current points and does any required tier promotions or expiries. It also creates event bodies based on the passenger loyalty Kafka topic and stores them in the HIVE, which will be later fetched by a Kafka producer and published to the topic. Activity Flow The customer loyalty state processor has the following high level activities (which applies to each customer) in the order shown below. Fetch metadata related to tiers Tier name Tier cost Target tier name Duration for tier Fetch customer’s current loyalty state Current tier Current points Target tier Highest secured tier Expiry date Fetch today’s customer accumulated trip points (Excluding anomalies) Add today’s accumulated trip points to current points ( Current points ← Current points + today’s points ) Based on the new current points or the expiry date, update the customer’s tier accordingly Tier promotions (User has reached points threshold for target tier) Update user’s current tier to the user’s target tier ( Current tier ← Target tier ) Subtract the target tier cost (taken from tier metadata) from the current points ( Current points ← Current points - Target tier cost ) Update user’s target tier to be the target tier of the new current tier based on the tier metadata. Update Highest secured tier to lowest BLUE tier. Set expiry date to the new current tier’s tier duration taken from tier metadata ( Expiry date ← today + current tier duration ) Create tier_renewal event and save in event_log Tier expiry (User has not reached target tier and expiry date is reached) Update user’s current tier to user’s highest secured tier ( Current tier ← Highest secured tier ) Subtract the highest secured tier cost (taken from tier metadata) from the current points ( Current points ← Current points - Highest secured tier cost ) Update user’s target tier to be the target tier of the new current tier based on the tier metadata. Update Highest secured tier to lowest BLUE tier. Set expiry date to the new current tier’s tier duration taken from tier metadata ( Expiry date ← today + current tier duration ) Create tier_renewal event and save in event_log Based on current points, update the highest secured tier fields of customers Update customer’s Highest secured tier to the tier corresponding to the largest tier cost (taken from tier metadata) that is still less than the current points Create tier_secured event and save in event_log Evaluate other thresholds scenarios that have been reached based on the current points About to Jump (User has secured his current tier and has obtained at least 95% of the target tier cost in their current points) Create threshold_reached event of type ABOUT_TO_JUMP and save in event_log About to Fall (User has not secured his current tier but has achieved at least 95% of the current tier cost in their current points and there is only 14 days left to tier expiry) Create threshold_reached event of type ABOUT_TO_FALL and save in event_log Save customer’s final state to HIVE after processing and log final state in Kafka Create daily_snapshot event and save in event_log",False
2092827387,page,current,/x/_wK_f,Analysis of Real-time Data Pipelines,Current Architecture Raw Ingestion Details Raw Ingestion Use Case Details Use Cases,False
2101313863,page,current,/x/R4E-fQ,Vtiger PassengerDetail Technical Specification,"Overview The purpose of this document is to detail the requirements of the Vtiger.PassengerDetailUpload and the implementation changes to the existing Vtiger.PassengerDetail export job. Requirements Consider only app accounts excluding roadside hires for trip metrics If there is only active mobile accounts for a phone number the latest created one have to be selected If all the accounts for the a phone number is deactivated, latest created account have to be selected If a phone number has active and deactivate accounts the latest active one have to be select If a phone number has both app and cooperate account and the cooperate account is active cooperate data have to be join with app account CompanyName and DepartmentName should be combined from cooperate using mobile number UpdatedDateTime - the most recent updated date from either app or cooperate (Requirement already satisfied) CreatedDateTime - value for app account (Requirement already satisfied) Plan of implementation This section will detail the implementation changes to accommodate the above requirements. The implementation changes will be applied to the export query itself (from HIVE to SQL) instead of adjusting the PassengerDetailUpload view. Instead of the current Left join of dim_passenger with other tables, run the following filters and windowing in the order given below. This may require subqueries. Filter in the passenger accounts corresponding to normal app hires (createdby = 1) and cooperate hires (createdby =6) Filter in only active cooperate accounts (userstatus = 'A') RANK() the table partitioning by phone number, account type and account status (phone, createdby, and userstatus) and ordering by descending created date (createddatesk) Filter records where the above rank is 1 Use a LEAD window function to get cooporate passengerid corresponding to app passenger id as a column (AS co_pid). Partition by phone number (phone) and order by account type (createdby) Use a LEAD window function to get the account type of the leading tuple corresponding to app passenger id as a column (AS lead_createdby). Partition by phone number (phone) and order by account type (createdby) Filter in only the app passenger Ids (createdby = 1) Filter the accounts that are active (userstatus = 'A') OR if that account is inactive, filter in the accounts whose account type is different from the leading account type or leading account type is NULL. Join the above relation with dim_cooperate_passenger (using co_pid and passengerid) Non functional requirements (NFRs) Current Vtiger jobs take some time to finish. This added changes should add minimal execution time. By filtering active accounts before joining with other tables, the reduction of overall execution time is also expected. Reference to delivery artifacts Plan for testing The added changes will be tested using data fetched from the production cluster for the relevant tables. Initial testing will be done using data coming user created tables.",False
2102068133,page,current,/x/pQNLfQ,Weekend Incentives,"Overview Existing implementation of the weekly incentives is outlined below. driver-incentives-qualifier-job This logic is executed by a qualifier job that runs weekly. Logic considered is outlined in the diagram below. driver-incentives-daily-calculator-job Plan of Implementation Weekend Incentives The idea would be to introduce an incentive category (weekly,weekend) to strategy and target tables. The same qualifier jobs would be run with a configuration which indicates whether it is a weekly incentive or weekend. This configuration would drive, Loading of the days scope (weekdays or weekends) for fetching the rides val week_day_keys = ""SELECT datekey, WeekOfYear "" + ""FROM "" + Config.STAGE_DB + "".dim_date "" + ""WHERE `date` < CAST(FROM_UNIXTIME(UNIX_TIMESTAMP("" + ""'"" + Config.CURRENT_DATE_PARTITION + ""' , '"" + Config.DATE_FORMAT + ""')) AS Date) "" + >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ""AND isweekday = 1 "" + >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ""ORDER BY datekey desc "" + ""LIMIT "" + Config.PAST_DAYS_CONSIDERED Loading the appropriate incentive strategies and targets for the incentive category // Query to get incentive strategies val incentive_strategies = ""SELECT "" + ""StrategyName, "" + ""TaxiModel, "" + ""RangeStart, "" + ""RangeEnd "" + ""FROM "" + Config.STAGE_DB + "".driver_incentive_strategy "" + ""WHERE TaxiModel IN ("" + Config.TAXI_MODELS + "") <<<<<<<<<<<<<<<<< filter for category >>>>>>>>>>>>>>>>>>"" // Query to get incentive targets val incentive_targets = ""SELECT "" + ""StrategyName, "" + ""TaxiModel, "" + ""TargetTrips, "" + ""IncentiveAmount, "" + ""CompletionRatio AS CompletionRatioTarget "" + ""FROM "" + Config.STAGE_DB + "".driver_incentive_strategy_target "" + ""WHERE TaxiModel IN ("" + Config.TAXI_MODELS + "") <<<<<<<<<<<<<<<<< filter for category >>>>>>>>>>>>>>>>>>"" Incentive category would also be written to the final results set val driver_weekly_incentive_new_df = driver_trips_summary .join(driver_fraud_trips_df, driver_trips_summary(""DriverId"") === driver_fraud_trips_df(""FraudDriverId""), ""left"") .na .fill(0) .withColumn(""PastDaysConsidered"", lit(Config.PAST_DAYS_CONSIDERED)) .withColumn(""date_time"", lit(Config.CURRENT_DATE_PARTITION)) .withColumn(""EligibilityPeriod"", lit(Config.INCENTIVE_VALIDITY_PERIOD)) >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> incentive category as a column here >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> .select( col(""DriverId""), col(""TaxiModel""), col(""StrategyName""), col(""QualifyingTrips"").alias(""TotalLegitimateTrips""), col(""FraudTripCount"").alias(""FraudTrips""), col(""ITCTrips""), col(""HFCTrips""), col(""RPPTrips""), col(""MaximumTrips""), col(""PastDaysConsidered""), col(""TargetTrips"").alias(""IncentiveTarget""), col(""IncentiveAmount""), col(""EligibilityPeriod""), col(""CompletionRatioTarget""), col(""date_time"") ) Following changes would be required in the driver-incentives-daily calculator-job Incentive category should be passed as a parameter for the job Loading of driver trips should consider weekends or weekdays depending on the incentive category val driver_trips = ""SELECT "" + ""fr.TripId AS TripId, "" + ""fr.DriverId AS TripDriverId, "" + ""fr.PassengerId AS TripPassengerId, "" + ""fr.TaxiModelId AS TripTaxiModel, "" + ""COALESCE(DropDateSK, CreateDateSK) as DropDateSK, "" + ""fr.tripfare AS TripFare, "" + ""fr.paymentmethod AS PaymentMethod "" + ""FROM "" + Config.STAGE_DB + "".fact_rides fr "" + ""WHERE fr.TaxiModelId IN ("" + Config.TAXI_MODELS + "") AND "" + ""fr.TravelStatus = 1 AND "" + ""COALESCE(CreateDateSK, DropDateSK) > CAST(REGEXP_REPLACE(("" + max_inc_target_date + ""),'-','') AS INT) AND "" + ""COALESCE(CreateDateSK, DropDateSK) <= CAST(REGEXP_REPLACE(('"" + Config.CURRENT_DATE_PARTITION + ""'),'-','') AS INT) "" + ""ORDER BY fr.DriverId"" Loading of driver kpis should consider weekends or weekdays depending on the incentive category val driver_kpi = ""SELECT "" + ""fddk.DriverId AS KPIDriverId, "" + ""dtm.mappingtaximodelid AS KPITaxiModel, "" + ""SUM(fddk.ITCTripCount) AS ITCTripCount, "" + ""SUM(fddk.HireFeeTripCount) AS HireFeeTripCount, "" + ""SUM(fddk.RoadPickupTripCount) AS RoadPickupTripCount "" + ""FROM "" + Config.STAGE_DB + "".fact_daily_driver_kpi fddk "" + ""INNER JOIN ("" + driver_primary_taxi + "") dtm "" + ""ON fddk.DriverId = dtm.mappingdriverid "" + ""WHERE fddk.TaxiModelId IN ("" + Config.TAXI_MODELS + "") AND "" + ""fddk.Datekey >= dtm.mappingcreatedatesk AND "" + ""fddk.DateKey > CAST(REGEXP_REPLACE(("" + max_inc_target_date + ""),'-','') AS INT) AND "" + ""fddk.DateKey <= CAST(REGEXP_REPLACE(('"" + Config.CURRENT_DATE_PARTITION + ""'),'-','') AS INT) "" + ""GROUP BY fddk.DriverId, dtm.mappingtaximodelid "" + ""ORDER BY fddk.DriverId"" Loading of trip_flow should consider weekends or weekdays depending on the incentive category val trip_flow = ""SELECT tf.rejected_driver_id AS RejectedDriverId, "" + ""dtm.mappingtaximodelid AS RejectedTaxiModel, "" + ""tf.rejection_type AS RejectionType, "" + ""COUNT(tf.rejection_type) AS RejectionCount "" + ""FROM "" + Config.STAGE_DB + "".trip_flow tf "" + ""INNER JOIN ("" + driver_primary_taxi + "") dtm "" + ""ON tf.rejected_driver_id = dtm.mappingdriverid "" + ""WHERE tf.vehicle_type IN ("" + Config.TAXI_MODELS + "") "" + ""AND tf.rejection_type IN ('"" + Config.MANUAL_REJECTION_TRIP + ""', '"" + Config.SYSTEM_REJECTION_TRIP + ""') "" + ""AND CAST(REGEXP_REPLACE((tf.created_date),'-','') AS INT) >= dtm.mappingcreatedatesk "" + ""AND CAST(REGEXP_REPLACE((tf.created_date),'-','') AS INT) > CAST(REGEXP_REPLACE((("" + max_inc_target_date + "")),'-','') AS INT) "" + ""AND CAST(REGEXP_REPLACE((tf.created_date),'-','') AS INT) <= CAST(REGEXP_REPLACE(('"" + Config.CURRENT_DATE_PARTITION + ""'),'-','') AS INT) "" + ""GROUP BY tf.rejected_driver_id, dtm.mappingtaximodelid, tf.rejection_type "" Loading of qualifiers should take into consideration the incentive category val driver_incentive_targets = ""SELECT "" + ""driverid AS DriverId, "" + ""taximodel AS TaxiModel, "" + ""phonenumbers AS PhoneNumbers, "" + ""strategyname AS StrategyName, "" + ""incentivetarget AS Target, "" + ""incentiveamount AS Amount, "" + ""CompletionRatioTarget, "" + ""QualifierId "" + ""FROM "" + Config.STAGE_DB + "".driver_incentive_qualifiers "" + ""WHERE date_time IN ("" + max_inc_target_date + "")""",False
2109177869,page,current,/x/DYC3fQ,2021-05-14 Data Science Initiatives,Date Participants List meeting participants using their @mention names: Goals We try to achieve following objectives Build clarity around data science initiatives Discuss bottlenecks/risks Collectively agree on the deliverable Discussion topics Time Item Presenter Notes 5 min Architecture Simplification Sanjaya 10 min Search & the elastic search Sanjaya 5 min Rider Hailing ETA Sanjaya/Pulara 10 min Food Promotions Pulara Action items Add action items to close the loop on open questions or discussion topics: 33 complete Re-validate the priorities in the DSE roadmap 41 complete Prioritize Search in the Roadmap 34 incomplete Review June commitments and understand requirements for experiments Decisions Type /decision to record the decisions you make in this meeting: e968e7cc-0a12-44f3-8fdc-165573f70769 DECIDED f3f42d19-b3dc-4140-bff0-3b43d389147e,False
2110390694,page,current,/x/pgHKfQ,Use Cases,"Surge Stage Job Code Repo : dp-rt-surge-job Heatmap Jobs Code Repo : dp-rt-surge-job Job Name From Kafka Topic To Kafka Topic Description CoreSurgeDowngradedJob surge_stage heat_changed filter source_type is not null Aggregated by agg_type and cell over 5 mins period, sliding during 30 sec, trigger every 10 secs CoreSurgeNonDowngradedJob surge_stage heat_changed filter source_type is not null Aggregated by agg_type and cell over 5 mins period, sliding during 30 sec, trigger every 10 secs DataPlatformSurgeDowngradedJob surge_stage surge_ingest filter dp_type is not null window = 5 min, 30 sec sliding aggregated by dp_type & cell trigger every 10 secs DataPlatformSurgeNonDowngradedJob surge_stage surge_ingest no filter window = 5 min, 30 sec sliding aggregated by vehical_type & cell trigger every 10 secs Average Waiting Time at Merchants Code Repo : dp-average-waiting-time-job Sources Destination Actions hive_db_raw_rt.job_arrived_at_pickup hive_db_raw_rt.job_order_picked_up hive_db_raw_rt.job_job_confirmed hive_db_stage.average_waiting_time Calculate wait time = pickedup - arrived Average wait time over merchants Driver Heartbeat Code Repo : dp-driver-heartbeat-enriching-job Sources Destination Actions hive_db_raw_rt.driver_shift_status_driver_shift_status_changed hive_db_raw_rt.driver_locations_driver_location_changed hive_db_stage.driver_status_incremental_frame hive_db_stage.shift_status_incremental_frame hive_db_stage.driver_status_incremental_frame hive_db_stage.driver_heart_beat Populate shift incremental data frame from drivers'shift data and previous shift status incremental data Populate drivers' trip data frame from trip created, trip started, trip completed, trip cancelled, trip rejected by driver event data and previous driver status incremental data Join above two frames to aggregate drivers' trip data and driver's shift data Join the above data frame with drivers'location data and using the joined frame derive drivers'status (A,B,F). NRT Driver Utilization Code Repo : dp-nrt-driver-utilization-job Sources Destination Actions hive_db_stage.driver_heart_beat hive_db_raw_rt.trip_trip_rejected_by_driver hive_db_raw_rt.trip_driver_assigned hive_db_raw_rt.trip_trip_completed hive_db_raw_rt.job_job_completed hive_db_stage.nrt_driver_utilization Customer Join Location Code Repo : dp-customer-join-location-job Sources Destination Actions hive_db_stage.dim_passenger hive_db_raw_rt.passenger_driver_requested hive_db_stage.customer_join_location On each day, per passenger calculate the hexbin, lat,lng for the first location Daily Region-wise Demand Supply Code Repo : dp-rt-region-taxi-supply-demand-job Flow Sources Destination Actions NoDriverFoundPipeline hive_db_raw_rt.trip_no_driver_found hive_db_raw_rt.trip_driver_pool_no_driver_found hive_db_raw_rt.trip_trip_created hive_db_stage.dim_hex_bin hive_db_stage.dim_taxi_model hive_db_stage.daily_no_driver_found No driver found events aggregated per minute, hour, district and taximodel final schema - hour, minute, district, taxi_model_description, count RegionBasedDemandPipeline hive_db_raw_rt.passenger_driver_requested hive_db_stage.dim_hex_bin hive_db_stage.dim_taxi_model hive_db_stage.daily_region_based_demand Driver request events aggregated per minute, hour, district and taximodel final schema - hour, minute, district, taxi_model_description, count RegionBasedExcessSupplyPipeline hive_db_stage.driver_heart_beat hive_db_stage.dim_hex_bin hive_db_stage.dim_taxi_model hive_db_stage.daily_region_based_excess_supply Heart beat events with shift_status “I” and status “F” aggregated per minute,hour, district and taximodel final schema - hour, minute, district, taxi_model_description, count RegionBasedSupplyPipeline hive_db_stage.driver_heart_beat hive_db_stage.dim_hex_bin hive_db_stage.dim_taxi_model hive_db_stage.daily_region_based_excess_supply Heart beat events with shift_status “I” aggregated per minute,hour, district and taximodel final schema - hour, minute, district, taxi_model_description, count RegionBasedTripCompletePipeline hive_db_raw_rt.trip_trip_completed hive_db_raw_rt.trip_trip_created hive_db_stage.dim_hex_bin hive_db_stage.dim_taxi_model hive_db_stage.daily_region_based_trip_complete Trip completed events aggregated per minute,hour, district and taximodel final schema - hour, minute, district, taxi_model_description, count DSD Visibility Code Repo : dp-dsd-visibility-job Flow Sources Destination Actions hourlyDsdVisibility hive_db_raw_rt.passenger_driver_requested hive_db_stage.dim_hex_bin hive_db_stage.dsd_visibility per dsd, hour and vehicle model calculate the available vehicle count and the no vehicle count passengerDsdVisibility hive_db_raw_rt.passenger_driver_requested hive_db_stage.dim_hex_bin hive_db_stage.passenger_hourly_dsd per passenger,dsd, hour,date_time and vehicle model calculate the available vehicle count and the no vehicle count Trip Flow Code Repo : dp-batch-trip-flow-job Sources Destination Actions hive_db_raw_rt.trip_trip_created hive_db_raw_rt.trip_driver_assigned hive_db_raw_rt.trip_trip_received_by_driver hive_db_raw_rt.trip_trip_rejected hive_db_raw_rt.trip_trip_accepted hive_db_raw_rt.trip_driver_arrived hive_db_raw_rt.trip_trip_started hive_db_raw_rt.trip_trip_ended hive_db_raw_rt.trip_trip_completed hive_db_raw_rt.trip_trip_cancelled hive_db_stage.trip_flow create a flat table of trip data aggregated by trip_id and driver_id Service level parameters Code Repo : dp-service-level-parameter-job Sources Destination Actions hive_db_raw_rt.trip_trip_created hive_db_raw_rt.trip_driver_assigned hive_db_raw_rt.trip_trip_rejected_by_driver hive_db_raw_rt.trip_trip_accepted hive_db_raw_rt.trip_driver_arrived hive_db_raw_rt.trip_trip_started hive_db_raw_rt.trip_trip_completed hive_db_raw_rt.trip_trip_cancelled hive_db_stage.service_level_parameter Calculate following for the past day average_acceptance_time accepted_trips all_requests average_eta_time date_time TBD dp-rt-region-taxi-supply-demand-job",False
2110652867,page,current,/x/wwHOfQ,Raw Ingestion,Raw Trip Ingestion Code Repo : dp-rt-trip-raw Destination db : hive_db_raw_rt Job Name Kafka Topic Destination Table tripDriverArrivedRaw driver_arrived trip_driver_arrived tripDriverAssignedRaw driver_assigned trip_driver_assigned tripDriverPoolNoDriverFoundRaw driver_pool_no_driver_found trip_driver_pool_no_driver_found tripDriverSelectedRaw driver_selected trip_driver_selected tripNoDriverFoundRaw no_driver_found trip_no_driver_found tripTripAcceptedRaw trip_accepted trip_trip_accepted tripTripCancelledRaw trip_cancelled trip_trip_cancelled tripTripCompletedRaw trip_completed trip_trip_completed tripTripCreatedRaw trip_created trip_trip_created tripTripEndedRaw trip_ended trip_trip_ended tripTripFareUpdatedRaw trip_fare_updated trip_trip_fare_updated_new tripTripNotificationTimedOutRaw trip_notification_timed_out trip_trip_notification_timed_out tripTripReceivedByDriverRaw trip_received_by_driver trip_trip_received_by_driver tripTripRejectedByDriverRaw trip_rejected_by_driver trip_trip_rejected_by_driver tripTripStartedRaw trip_started trip_trip_started tripTripUpdatedRaw trip_updated trip_trip_updated Raw Passenger Ingestion Code Repo : dp-rt-passenger-raw Destination db : hive_db_raw_rt Job Name Kafka Topic Destination Table passengerDriverRequestedRaw passenger_driver_requested passenger_driver_requested Raw Driver Location Change Ingestion Code Repo : dp-rt-driver-locations-raw Destination db : hive_db_raw_rt Job Name Kafka Topic Destination Table driverLocationsDriverLocationChangedRaw driver_locations driver_locations_driver_location_changed Raw Job Ingestion Code Repo : dp-rt-job-raw Destination db : hive_db_raw_rt Job Name Kafka Topic Destination Table JobArrivedAtPickupRaw arrived_at_pickup job_arrived_at_pickup JobJobAcceptedRaw job_accepted job_job_accepted JobJobCancelledRaw job_cancelled job_job_cancelled JobJobCompletedRaw job_completed job_job_completed JobJobConfirmedRaw job_confirmed job_job_confirmed JobJobCreatedRaw job_created job_job_created JobJobDeclinedRaw job_declined job_job_declined JobJobEndRaw job_end job_job_end JobJobOrderReadyRaw job_order_ready job_job_order_ready JobJobPreptimeExpiredRaw job_preptime_expired job_job_preptime_expired JobJobTimedOutRaw job_timed_out job_job_timed_out JobOrderPickedUpRaw order_picked_up job_order_picked_up JobQueueAcceptedRaw queue_accepted job_queue_accepted JobQueueDeclinedRaw queue_declined job_queue_declined Raw Heatmap Ingestion Code Repo : dp-rt-surge-raw Destination db : hive_db_raw_rt Job Name Kafka Topic Destination Table SurgeRawIngest surge_ingest heat_changed Raw Driver Selection Code Repo : dp-rt-driver-selection-raw Destination db : hive_db_raw_rt Job Name Kafka Topic Destination Table DriverSelectionRaw driver_selection_debug driver_selection Raw Trip ETA Code Repo : dp-rt-trip-eta-raw Destination db : hive_db_raw_rt Job Name Kafka Topic Destination Table tripETARaw trip_eta trip_eta,False
2137948224,page,current,/x/QIBufw,Incentives Enhancements,,False
2140241977,page,current,/x/OYCRfw,Miscellaneous,Under this all investigation related to any new business concepts will be documented.,False
2141159428,page,current,/x/BICffw,2021-05-21 Data Science Initiatives,Date Participants List meeting participants using their @mention names: Goals We try to achieve following objectives Build clarity around data science initiatives Discuss bottlenecks/risks Collectively agree on the deliverable Discussion topics Time Item Presenter Notes 5 min ESP and the way forward Sanjaya 10 min Driver Churn Pulara/Sanjaya Current status Driver segmentation 5 min Search Optimizations Sanjaya/Pulara Improve relevance through tags Derive relevance through search patterns Action items Add action items to close the loop on open questions or discussion topics: Decisions Type /decision to record the decisions you make in this meeting: e968e7cc-0a12-44f3-8fdc-165573f70769 DECIDED f3f42d19-b3dc-4140-bff0-3b43d389147e,False
2141454347,page,current,/x/CwCkfw,Driver churn prediction,"The main goal for this predictive problem is to identify which drivers would churn. The dataset has 2853 drivers. Used 80% sample for training and 20% of the data to measure accuracy. Additionally, the analysis aims to identify a criteria for churn and identifying factors that could affect churn. The study was done considering the following sections. Driver profiling CRM data analysis and feature building Incentives analysis Working pattern recognition Driver segmentation Moving average analysis Model building Churn factor analysis Driver segmentation using factors related to churn factor The summary of the whole analysis is as follows.",False
2152235017,page,current,/x/CYBIg,CRM data analysis and feature building,"Dataset CRM tickets and tokens from 23rd of December - 23rd of March **Before data preparation and cleaning CRM tickets CRM tokens no . of unique tickets/tokens 443508 8807 No. of instances 480142 8807 No. of drivers 33697 5156 CRM tickets There were a lot of missing values for some attributes in CRM tickets and the attributes with missing values are as follows. variable % of missing values (considering tickets) ticketno 100% tripid 48.68% driverid 26.09% crmpassengerid 1.66% passengerid 56.90% modifiedby 89.96% updatedtime 89.96% updatedatesk 89.96% direction 30.44% location 29.83% description 16.46% solution 91.447% raisedby 0.76% initialhandler 3.764% initialprofile 1.57% The variable ‘ticketno’ was removed since all the values are missing. There were tickets with missing driverids. The driverid is a very important attribute since that is the variable which captures the driver related to the ticket. For the tickets with missing driver ids, initially, the missing driverids were obtained from ‘fact_rides’ table considering the trip id of the ticket. 17.39% of the missing driverids were imputed using that approach. Rest of the tickets with missing driverids were dropped since there was no other way to impute the value. Only the hailing trips were considered for the analysis. Thus, the tickets related to delivery rides were discarded initially. The approach used to select only the hailing drivers/trips related tickets is as follows. The taximodel ids can be used to distinguish between the hailing drivers and the other drivers. The taxi models which are used for hailing are 1, 2, 3, 4, 6, 10 and 18. First, the drivers which have done rides using taxi models other than hailing taxi models were filtered from the fact_rides table. These selected drivers were considered as the drivers related to the delivery side. Next, the tickets which were raised by the drivers which are not in the delivery drivers list were filtered out. After filtering out the hailing driver/ trips related tickets, still there were remaining delivery (food/marketplace) related tickets. Thus, a selected number of tripids were cross checked with the fact rides table to identify the anomalies. Anomalies observed in the driverids and tripids related to the tickets and reasons for remaining delivery related tickets There were trips with missing driver id in the fact rides table. But, the driver id is available in a raised ticket for the particular tripid. Ex: tripid - 266566003 This is a delivery related trip. This was not captured since the driver id was not available in the fact ride table. Order follow ups - Some tickets have been created to follow up the problems related to previous rides which were completed before the considered time period. Thus, the driverid was not captured in the delivery driver list obtained from the fact rides table. These tickets can be removed by merging the fact rides table with the trip created date. There were remaining 67 food category trips and 28 trips after removing the delivery related tickets. Some of the food (38 tickets) and truck (25 tickets) category related tickets are order follow ups as explained above. Thus those can be removed. After removing the food and truck category related tickets, still there were tickets with titles related to food and market place rides. Those titles were captured considering a NLP approach using the keywords food, marketplace, delivery and order. Finally, food and marketplace delivery related tickets were captured considering the keywords in the descriptions. Dealing with other missing values The variables ‘modifiedby’ , ‘updatedtime’ and ‘updatedatesk’ have a similar number of missing values. These missing values belong to the tickets with only one instance in the considered dataset and modifying/ updating information are missing. Else, these tickets have been handled by the person who initially created the ticket. Entries with missing passenger id - most of the tickets have been raised by drivers. Most of the tickets are about driver profile, driver app and payments which are not related to any passengers. The variable solution is missing mostly (~10.2%) for trip cancellation requests. Also. the solution is missing mostly, if the ticket was handled by the call centre team (22.87%) . The variable ‘raised by’ is very important to know whether the ticket was raised by the driver or the passenger. Thus, the tickets are dropped if the value is missing. (Data loss - 0.76%). Most of the tickets missing raised by variable are complaints (33.64%). Considering all missing values - When considering the initial handler, the missing values are higher if the ticket was initially handled by the call centre team ( 20.41% ). Descriptive analysis There 17 types of categories and most of the tickets are about driver profile, requests, inquiries and complaints. Even though the tickets are originally grouped under 17 categories, in some cases the titles tend to fall into different categories due to the nature of the complaint. Hence, it is important to categorize similar tickets into one category using a different approach. Thus initially, clustering and LDA (Latent Dirichlet allocation) approaches was used in order to segment the title. However, both these methods weren’t able to segment the tickets in a meaningful way. Hence, a manual title breakdown was carried out to categorize them. The manual title breakdown is as follows. Title breakdown Category Titles falls under the selected category Trip fare not credited Driver Payment Not Updated Bank Payment Not Updated Self Settlement Not Banked Card payment declined and cash not paid Did Not Pay - Cash Card transaction ended as cash Unable to end transaction Corporate Cash in to Credit Settlement Recovery or Settlement Plan Settlement Plan Request - Branch Operation Subscription waive off Diriya Settlement Plan Driver Care Temporally Reverse Changes in account, vehicle or taximodel, phone number Vehicle or Driver change approval New Feature Related Mobile Number Update Gaining information / understanding the process E-Bill Request Lost Device Procedure Info Registration - Branch Details Vehicle Change Procedure Driver Insurance Related Driver Change Procedure Account Balance Incentive or Bonus Related Information Trip or Cancellation Followed Up Driver Income Statement Request Training Schedule Vehicle Breakdown Payment Locations Dispatcher Hire Follow Up Feedback and Suggestions Trip related information (fare, location) Hire Pickup Location Related Cash Manual Calculations Corporate Manual Calculations Hire Payment Type Related Corporate Highway or Paging Add Base Fare and Packages Mistakes made by driver Payment Made to Incorrect Driver ID​ Outstanding balance Payment Reminder Technical (App/Device) Unable To Start the trip Unable To End Hire App Heat Map Not Working Wrong Driver Location Driver Push Notification Related Information Unable to Accept Hires Meter Not Working Driver Login Related Issues Directional Hire issues Miscellaneous Driver Not Working Informed Profile Picture Registration - Online registered call back request Risk Call Drop Call Closed complaint Inquiry/followup No Answer Missed Call Inquiry/Callback Inquiry Call Transferred Driver unable to work due to being blocked Account Status - Active Registration - Other Account Status - Blocked Payments Done Still Blocked Driver Reactive Device Reissue Complaints about the drivers ITC Corporate Driver Refuse Corporate Poor Overall Discipline Overcharged by Driver Cancellations or Bookings of trips Trip Cancellation Request Corporate Bookings Corporate Booking Created Finally, the above main categories were grouped as follows. Driver dissatisfaction Trip fare not credited, Trip fare, location, Driver complaints, Mistakes made by driver, Technical (App/Device), Cancellations or Bookings of trips outstanding payments Outstanding balance related to blocked Settlement, Driver unable to work due to being blocked related to information Changes in account, vehicle or taximodel, phone number Gaining information / understanding the process driver behavior Complaints about the drivers Features extracted from tickets and tokens Customer complaints Driver complaints No hire complaints Driver behavior complaints Information related tickets Settlement tickets Driver dissatisfaction Walk in frequency Limitations The driver ID is missing in some passenger complaints. Thus, all the complaints on the driver by passenger cannot be tracked accurately. The tickets which should be fall under drivers have been raised under passengers due to various reasons. These tickets can affect the customer complaints count and driver complaints count The titles are not accurate all the time",False
2170716247,page,current,/x/V4BigQ,Customer Loyalty logic change requirements,"Overview The purpose of this document is to provide a brief description of the changes that should be made to the currently functioning customer loyalty system. The proposed changes will be applied to the loyalty points accumulation and expiry and the overall progression of Platinum tier customers. Platinum tier customer logic requirements In the currently deployed customer loyalty system, the logic for a platinum tier customer is the same as the scenario of a customer of any other tier reaching their target tier, where once they do reach their target tier, the points are immediately deducted and reset on their points journey with a new expiry date given (6 months duration) from that date. The desired logic for a platinum tier customer is as follows. Even though the target tier for a platinum customer is the platinum tier, once a platinum tier customer reaches the platinum points threshold again, his points are not deducted immediately unlike the scenario of reaching the target tier while being a non platinum customer. The customer is allowed to further accumulate his points (After firing a tier secured event) until his current expiry date is reached. Only on the date of expiry, the platinum tier cost (600) will be deducted from his points to maintain his tier for the next 6 months (new expiry date) Based on remaining points after deduction, the a tier secured event will be fired if the current point balance is enough to secure a tier higher than BLUE.",False
2179924035,page,current,/x/QwDvgQ,Customer Loyalty development changes Technical Spec,"Overview The purpose of this document is to detail the implementation changes that need to be done to incorporate the Customer Loyalty logic change requirements as detailed in https://pickme.atlassian.net/wiki/pages/resumedraft.action?draftId=2170716247 . Plan of Implementation Platinum customer loyalty progression Currently, when a platinum customer reaches a platinum tier points threshold, it is treated similar to a tier promotion where the approach followed for other tiers is taken. Instead the platinum customer should be able to accumulate until his expiry ends. To do this, we include the following changes. Incorporate the condition in the tier promotions logic where if the customer’s current tier is platinum, he is to be excluded from the promotion activities The remainder of the current loyalty processing flow as shown in Customer Loyalty State Processor Flow of tier expiry calculation followed by tier securing calculation will ensure the satisfaction of the change requirements. Non functional requirements (NFRs) The changes should affect the running time and resource usage of the current job in a minimal way Reference to delivery artifacts <> Plan for testing The changes will be manually tested using custom created dataframes for the different scenarios.",False
2183332003,page,current,/x/owAjgg,How to Use JMeter for Performance & Load Testing,"Overview This document outlines the process of performance and load testing using Apache JMeter. What is Performance and load testing? Performance tests are performed to test the performance of web servers, databases and networks. Load tests are performed to determine how many users the system can handle. JMeter JMeter is an open source desktop Java application designed to test load and measure performance. It can be used to analyze and evaluate the performance of web applications and other services. Installation Install Apache JMeter on Ubuntu Install Java (If you have not already installed). Download the latest stable version of Apache JMeter. wget http://www.gtlib.gatech.edu/pub/apache/jmeter/binaries/apache-jmeter-5.4.1.tgz Extract the downloaded JMeter file. tar xf apache-jmeter-5.4.1.tgz Navigate to the bin directory. cd apache-jmeter-5.4.1/bin Run the jmeter.sh script to install JMeter. ./jmeter.sh Create a Test Plan in JMeter and Generate a Dashboard Report Here, I am going to do a performance analysis on Delivery ETA API service using 150 users. Step 1: Launch the JMeter application. The following screen will appear after launching JMeter. Step 2: Provide a name for your test plan and save it. Then you will see the following screen. Step 3: Add an HTTP Request Defaults . This element is used to set default values for HTTP request in our test plan. Select Test Plan (Delivery ETA Progress Screen API Test Plan) , then Right-click it Go to Add > Config Element > HTTP Request Defaults In HTTP Request Defaults , under the Web Server section, fill in the Server Name or IP field with the name or IP address of the web server you want to test. Setting the server here makes it the default server for the rest of the items in this Test Plan . Step 4: Add an HTTP Header Manager. This element is used to set default header values for HTTP request in our test plan. Select Test Plan (Delivery ETA Progress Screen API Test Plan) , then Right-click it Go to Add > Config Element > HTTP Header Manager In HTTP Header Manager , You can add a header name and value using the add button at the bottom of the window. Setting the headers here makes it the default headers for the all requests in this Test Plan . Step 5: Add a Thread group. Select Test Plan (Delivery ETA Progress Screen API Test Plan) , then Right-click it Go to Add > Threads (Users) > Thread group The thread group has three particularly important properties and you need to set them for doing a load test. Number of Threads (users) - Number of users using the platform Ramp-up period (seconds) - Time JMeter should take to start the total number of threads Loop count - How many times the test would be executed Step 6: Add an HTTP Request. Select Thread group , then Right-click it Go to Add > Sampler > HTTP Request Provide a name for your sample request and save it. In HTTP Request , under the HTTP Request section , fill in the Path that you want each thread (user) to request (No need to specify the server as it is already defined in the HTTP Request Defaults ). Select POST or Get from the drop down according to the your sample request. If it is a POST request , add post body into Body Data. Note: Body data and parameters could not be entered at once, As a way to do that, I used the url path with the parameters of my HTTP request. If you want to add more HTTP requests as part of your test, repeat these steps. Each thread will execute all the requests in this test plan. Step 6: Add a Listener. This element is used to get the results of load test. There are different types of listeners and you can add any type of listener you want. Here, I have used the Summary Report as a listener. Select Thread group , then Right-click it Go to Add > Listener > Summary Report If you want to write the results to csv, you can provide a path to the FIlename . Step 7: Run the Test Plan. Run the test plan using Green Start Arrow below the main menu. Then you will be able to see the test results in the Summary Report section as follows. Step 8: Generate a Dashboard Report. Add ' Simple Data Writer ' from the listeners as in step 6 and set the location to save the test results as follows. 2. Run your Test Plan. 3. Go to Tools (In main Menu) > Generate HTML report Then following screen will appear. Result file (csv or jtl) : The file path where the result is saved. user.properties file: This file can be found in the JMeter bin directory. Output directory: The output directory where you want to save the report. Configure the above file paths and click the Generation Report button at the bottom of the window. Then you will get a report as follows. JMeter in Non-GUI Mode The GUI mode of JMeter is perfect for adding and editing new configuration elements, thread groups and samplers as described above. However, the GUI mode have a limitation which slows down the CPU utilization while running the test plans. If we are running multiple listeners in our test plan to visualize the test results in different ways, it affects the JMeter performance. To overcome such a situation, the test plan should be run in the non-GUI mode. Then JMeter will be able to driver more request rate. Run your test plan in non GUI Mode and Generate a report Step 1 : Install the JMeter on the machine where you want JMeter to be installed (Please refer the installation steps mentioned above ) Step 2: Create a test plan using the JMeter GUI mode that runs on your local machine and save it. Then you will get JMX file that contains the your test plan. Step 3: Copy the JMX file to the remote server where you installed JMeter using the 'scp' command. Step 4: Go to the server you installed JMeter and navigate into JMeter’s bin folder. Step 5: Run the following command to run your test plan. jmeter -n -t delivery_eta_progress_screen_test_plan.jmx -l log.jtl -n : This specifies JMeter is to run in cli mode -t : [name of JMX file that contains the Test Plan] -l : [name of JTL file to log sample results to] Step 6 : After completing the test, copy the generated JTL file into your local machine using the 'scp' command. Step 7 : Generate a report using the JMeter GUI mode that runs on your local machine and generated JTL file. ( Please proceed from sub-step 3 of ‘ Step 8: Generate a Dashboard Report ’ mentioned above)",False
2183463082,page,current,/x/qgAlgg,ETA Accuracy Analysis Pipeline,"Overview Build a pipeline to calculate the mean squared error for ETA predictions each day. this task involves few steps Extract the prediction data from the log files Calculate the Actual values from the fact_food_orders_log using the status id Plan of Implementation Extract the prediction data from the log files 2021-05-13 17:20:58.265 +0530 - [pool-20-thread-1] [INFO] - from . [CORRID=NRTHexLevelInfoGuavaCacheLoad] HBase record retrieval delay in ms: 12 2021-05-13 17:20:59.442 +0530 - [pool-20-thread-1] [INFO] - from . [CORRID=765801ec-369f-4fbb-9e4c-e3a09df295d5] Delivery ETA Payload: {""orderId"": 523, ""statusId"": 12, ""estimatedETA"": 586.8041572570801, ""etabreakdown"": {""prepETA"": 32.5449104309082, ""driverAcceptETA"": 311.9693908691406, ""driverArrivalETA"": 234.28985595703125, ""driverWaitingETA"": 8}} is successfully generated for feature map: {""orderId"":523,""statusId"":12,""weekday_sin"":0.8660254037844386,""weekday_cos"":0.5000000000000001,""hour_sin"":0.3984010898462414,""hour_cos"":-0.917211301505453,""minute_sin"":-0.9968120070307502,""minute_cos"":0.07978610555308265,""average_order_count_merchant"":48.0,""average_order_cancellations"":48.0,""restaurant_count"":6.0,""prep_time_7days_average"":6,""prep_time_90days_average"":7,""volume"":5,""service_group_code_MARKET_PLACE"":2,""dsd"":0,""average_order_count"":48.0,""average_driver_count"":48.0,""acceptance_time_7days_average"":9,""acceptance_time_90days_average"":8,""arrival_time_90days_average"":4,""arrival_time_7days_average"":5,""driver_waiting_time"":8,""NRT_15_order_count"":5.0,""NRT_completed_orders"":6.0,""NRT_average_prep_time"":7.0,""NRT_order_count"":6.0,""NRT_driver_count"":6.0}. 2021-05-13 17:20:59.442 +0530 - [pool-20-thread-1] [INFO] - from . [CORRID=765801ec-369f-4fbb-9e4c-e3a09df295d5] Merchant confirmed Stage analyzing is applied for order id: 523 and status id :12 The log file contains various details as shown in above log. But for our task we only need the records which contain the prediction fields. A line with prediction fields can identify by checking for the text “ Delivery ETA Payload “ in each line in log file. Once found a relevant line, regex is used to get the JSON string. there are two parts to extract from a line. Regex prediction fields “{.*{.*}}” features used to predict “{.*}” join the two JSON strings and parse it to get the JSON object. Finally a JSON object as shown below return from a line in log file. {'orderId': 523, 'statusId': 12, 'estimatedETA': 586.8041572570801, 'weekday_sin': 0.8660254037844386, 'weekday_cos': 0.5000000000000001, 'hour_sin': 0.3984010898462414, 'hour_cos': -0.917211301505453, 'minute_sin': -0.9968120070307502, 'minute_cos': 0.07978610555308265, 'average_order_count_merchant': 48.0, 'average_order_cancellations': 48.0, 'restaurant_count': 6.0, 'prep_time_7days_average': 6, 'prep_time_90days_average': 7, 'volume': 5, 'service_group_code_MARKET_PLACE': 2, 'dsd': 0, 'average_order_count': 48.0, 'average_driver_count': 48.0, 'acceptance_time_7days_average': 9, 'acceptance_time_90days_average': 8, 'arrival_time_90days_average': 4, 'arrival_time_7days_average': 5, 'driver_waiting_time': 8, 'NRT_15_order_count': 5.0, 'NRT_completed_orders': 6.0, 'NRT_average_prep_time': 7.0, 'NRT_order_count': 6.0, 'NRT_driver_count': 6.0, 'prepETA': 32.5449104309082, 'driverAcceptETA': 311.9693908691406, 'driverArrivalETA': 234.28985595703125, 'driverWaitingETA': 8} Then a dataframe can be obtained from the list of JSON objects by using the pyspark createDataFrame() function. ETA APIs can called in different status of same order. So there can be few raws for same order id but different status id. In the prediction there are four ETA breakdowns as prepETA, driverAcceptETA, driverArrivalETA, driverWaitingETA. prepETA value is -1 for the Predictions after food preparation . Similarly other ETA values also will be -1 for the prediction of later status for same order id. 2. Calculate the Actual ETA values from the fact_food_orders_log using the status id fact_food_orders_log table in hive_db_stage is used to find the actual ETA values for each orders placed. each raw of this table consist of an event related to order such that merchant confirmed, order ready, accepted, arrived at pickup point, etc… There can be more than one record for same orderid and statusid. In such a case picked the raw with latest time stamp 'orderlogid', 'orderid', 'statusid', 'latitude', 'longitude', 'address', 'distance', 'createddatetime', 'createddate', 'updateddatetime', 'updateddate', 'editreason', 'date_time' Create a pivot table from the above table only using the orderid , statusid and createddatetime columns. fact_order_table = fact_order_table[[""orderid"", ""statusid"",""createddatetime""]]. fact_order_table.groupBy(""orderid"").pivot(""statusid"").agg(first(""createddatetime"")) The table after pivoting as follow, | orderid| 0| 1| 11| 12| 13| 14| 15| 16| 17| 18| 3| 4| 6| 8| 9| +---------+----+-------------------+----+-------------------+----+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+ |140783637|null|2020-11-02 07:06:05|null|2020-11-02 07:06:24|null| null|2020-11-02 07:06:05| null| null|2020-11-02 07:08:16|2020-11-02 08:30:26|2020-11-02 08:44:32|2020-11-02 08:48:14|2020-11-02 08:59:15|2020-11-02 08:59:24| In the chain of event in an order if order prepared ( status 18) not fired within a predefined time a prep time expire event is fired ( status 17 ). Because of this impute the null values in column 18 with timestamp values in column 17. pivoted_table.withColumn(""prepActual"", col(""18"").cast(""long"") - col('12').cast(""long"")) pivoted_table.withColumn(""driverAcceptActual"", col('3').cast(""long"") - col('18').cast(""long"")) pivoted_table.withColumn(""driverArrivalActual"", col('4').cast(""long"") - col('3').cast(""long"")) pivoted_table.withColumn(""driverWaitingActual"", col('6').cast(""long"") - col('4').cast(""long"")) Then the actual value can calculated from taking the difference of specific columns as above. 3. Joining the table to analyse accuracy In this step the tables derived in above steps ( predicted and actual ) are joined and calculate the squared errors. pivoted_table.join(data_predictions, pivoted_table.orderid==data_predictions.orderId, ""inner"") Code snippet below shows the square error calculation. joined_table.withColumn(""driverArrivalMSE"", (col(""driverArrivalActual"") - col(""driverArrivalETA""))**2 ) joined_table.withColumn(""driverAcceptMSE"", (col(""driverAcceptActual"") - col(""driverAcceptETA""))**2 ) joined_table.withColumn(""driverWaitingMSE"", (col(""driverWaitingActual"") - col(""driverWaitingETA""))**2 ) Final the table is inserted to the eta_accuracy_analysis_table in hive_db_stage as a new partition. Column Type Comment orderid bigint orderid statusid int status of a particular order prepactual bigint Actual preparation time driveracceptactual bigint Actual driver accept time driverarrivalactual bigint Actual driver arrival time driverwaitingactual bigint Actual driver waiting time prepeta bigint Predicted preparation time ( -1 status ids other than 12 ) driveraccepteta bigint Predicted driver accept time ( -1 status ids other than 12, 18 ) driverarrivaleta bigint Predicted driver arrival time( -1 when status id = 4 ) driverwaitingeta bigint Predicted driver waiting time prepmse bigint Error of preparation time driverarrivalmse bigint Error of driver accept time driveracceptmse bigint Error of driver arrival time driverwaitingmse bigint Error of driver waiting time Executing the Pipeline Currently deployed version - fb-dataproc Azkaban project - http://34.124.240.84:8081/manager?project=dp-log-reader-recovery Above version is compatible for both daily run and bulk recovery. Daily Execution Can execute without any flow parameter setup Recovery for a given date range After copying the log files to the location appropriately execute the flow with below flow parameters. start - 2022-07-31 end - 2022-08-05 ex -: http://34.124.240.84:8081/executor?execid=73024#",False
2187362328,page,current,/x/GIBggg,Churn prediction - (model building),"The main focus of this predictive problem was to identify which driver would churn in the upcoming months. A dataset of 2853 regular drivers was used for the analysis and used 80% of the data to training and 20% to test the accuracy. A classification modelling approach was used in order to predict whether a driver is either churned or active. The driver data was collected considering the regular drivers who were active during 1st November 2020 to 31st January 2021. The driver churn was defined considering the data of the month of February and March. The definition for churn is as follows. “ A regular driver (from November 1st January 2020 to 31st of January 2021) who is inactive in February and March is a churned driver “ The following classification models were fitted in order to predict the churn. Logistic regression Random forest GB Classifier KNN The GB Classifier had the best performance out of the models fitted. The accuracy of the GB Classifier in 3 different windows are as follows. Accuracy Precision Recall F1- score Considered period - November 1 - January 31, Churned period - February 1 - February 28 train 0.98 0.96 1.0 0.98 test 0.97 0.94 1.0 0.97 Considered period - November 15 - February 15, Churned period - Feb 16 - March 16 train 0.97 0.95 0.99 0.97 test 0.97 0.94 1.0 0.97 Considered period - December 1 - February 28, Churned period - March 1 - March 31 train 0.95 0.93 0.98 0.95 test 0.95 0.92 0.98 0.95 The detailed report is attached below.",False
2188312922,page,current,/x/WgFvgg,Subscription pricing model v1,,False
2190966919,page,current,/x/h4CXgg,Incentives portal API,"Revision History Date Version Change Request Date Description Author 2021-06-03 1.0.0 N/A Initial version Imasha Ariyarathna References Reference Version Date Remarks N/A N/A N/A N/A 1 7 Introduction Incentives Information API provides easy and quick access to upload any kind of incentives. And this service allows all CRUD operations as well as the Search. Incentives Information Services From here onwards, this document describes the Incentives Information API end-points. Create Incentive Request Schema The following table includes the request schema of the Create Incentives REST API. URL http://<endpoint_addr>/driverincentiveportal/iserv/v1/driver/incentives Description Service to create new incentive. Request Method POST Request Body Format application/json Note 1: The request must specify the “Content-Type” header with the value “application/JSON”. Note 2: Please see section Request Body for POST request body schema. Response Schema (Success) application/JSON Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/JSON Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Request Body Parameters Parameter Name State Type Example Values Input Field Type Remarks incentiveTitle Mandatory String ""New Year Incentive"" Free Form Text Unique Title incentiveType Mandatory String ""Hailing"" ""Food"" Values from Specified List incentiveCategory Optional String ""Daily"" ""Weekly"" ""Weekend"" Values from Specified List effectiveFrom Mandatory String ""2021-04-10"" Date Date format should be YYYY-MM-dd effectiveTo Optional String ""2021-04-17"" Date If not set this value will replace with the effective from date. Consider as One Day Incentive startTime Optional String ""2021-04-17 08:00:00"" Date-Time endTime Optional String ""2021-04-17 14:00:00"" Date-Time serviceGroup Mandatory String ""Taxi"" ""Food"" ""Truck"" ""Parcel"" ""Market Place"" Values from Specified List service Mandatory String ""Tuk"",""Car"",""Mini”,""Flex"",""VIP"",""Van”,""Bike"",""Budget”, ”Mini Van”,”Food” etc Values from Specified List driverCohort Optional String - Values from Specified List If not set, it will replace as ""All"" rewardsBy Mandatory String ""LKR"" ""MVR"" Values from Specified List rewardsBasedOn Optional String ""Trip"" ""Earnings"" Values from Specified List completionRatio Optional Double 80.0 strategyTargets Mandatory Array [{<strategy 1 data>}, {<strategy 2 data>}] Free Form Text. See sample request below. In case of 1 day incentives this would be a single entry where as in other types of incentives this would be multiple entries incentiveMetadata Mandatory Array [{<metaKey>,<metaValue>}] Free Form Text. See sample request below. district Optional String ""Kandy"" ""Colombo” Values from Specified List remarks Optional String """" Free Form Text daysConsidered Optional Integer 28 Free Form Text Default Value 1 updatedBy Mandatory String ""Imasha"" Logged User Name strategyTargets Structure Parameter Name State Type Example Values Input Field Type Remarks strategyName Optional String ""Strategy 1"" Free Form Text If not set it will replace as ""Strategy 1"" Consider as One Day Incentive rangeStart Optional Integer 10 Free Form Text Default Value 0 rangeEnd Optional Integer 20 Free Form Text Default Value 0 ""sequence"" Mandatory Integer 0,1,2,3... Generated Value target Mandatory Integer 25 Free Form Text reward Mandatory Double 500.00 Free Form Text incentiveMetadata Structure Parameter Name State Type Example Values Input Field Type Remarks metaKey Mandatory String ""SinahalaTitle"" Free Form Text metaValue Mandatory String ""ටෙස්ට්"" Free Form Text Request Body The request body of the Incentives Information (Create Incentive) POST API will need to follow the below JSON schema. json { ""incentiveTitle"": ""New Year Incentive"", ""incentiveType"": ""daily"", ""incentiveCategory"": ""Hailing"", ""effectiveFrom"": ""2021-04-10"", ""effectiveTo"": ""2021-04-17"", ""startTime"": ""1970-01-01 08:00:00"", ""endTime"": ""1970-01-01 12:00:00"", ""serviceGroup"": ""Rides"", ""service"": ""TukH"", ""driverCohort"": ""-"", ""rewardsBy"": ""LKR"", ""rewardsBasedOn"": ""Trip"", ""completionRatio"": 80.0, ""strategyTargets"": [ { ""strategyName"": ""Strategy 1"", ""rangeStart"": ""0"", ""rangeEnd"": ""10"", ""rewards"": [ { ""sequence"":0, ""target"": 10, ""reward"": 500 }, { ""sequence"":1, ""target"": 20, ""reward"": 1000 }, { ""sequence"":2, ""target"": 30, ""reward"": 1500 } ] }, { ""strategyName"": ""Strategy 2"", ""rangeStart"": ""10"", ""rangeEnd"": ""20"", ""rewards"": [ { ""sequence"":0, ""target"": 20, ""reward"": 1000 }, { ""sequence"":1, ""target"": 30, ""reward"": 1500 }, { ""sequence"":2, ""target"": 40, ""reward"": 3000 } ] } ], ""incentiveMetadata"":[ { ""metaKey"": ""TestTitleS"", ""metaValue"" : ""ටෙස්ට්"" }, { ""metaKey"": ""TestTitleE"", ""metaValue"" : ""தமிழில் தட்டச்சு"" } ], ""district"": ""Kandy"", ""remarks"": ""nothing"", ""daysConsidered"": 28, ""updatedBy"": ""Imasha"" } Update Incentive Request Schema The following table includes the request schema of the Update Incentives REST API. URL http://<endpoint_addr>/driverincentiveportal/iserv/v1/driver/incentives ?incentiveid= {incentiveid} Description Service to update an existing incentive. Request Method PUT Request Body Format application/json Note 1: The request must specify the “Content-Type” header with the value “application/JSON”. Note 2: Please see section Request Body for POST request body schema. Response Schema (Success) application/JSON Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/JSON Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Example Values Remarks ""incentiveid"" Mandatory Long 1001 Request Body Parameters All fields of the structure can be updated before the effective-from date. See “Create Incentives” body structure for details. Request Body The request body of the Incentives Information (Update Incentive) POST API will need to follow the below JSON schema. { ""incentiveTitle"": ""New Year Incentive"", ""incentiveType"": ""daily"", ""incentiveCategory"": ""Hailing"", ""effectiveFrom"": ""2021-04-10"", ""effectiveTo"": ""2021-04-17"", ""startTime"": ""1970-01-01 08:00:00"", ""endTime"": ""1970-01-01 12:00:00"", ""serviceGroup"": ""Rides"", ""service"": ""TukH"", ""driverCohort"": ""-"", ""rewardsBy"": ""LKR"", ""rewardsBasedOn"": ""Trip"", ""completionRatio"": 80.0, ""strategyTargets"": [ { ""strategyName"": ""Strategy 1"", ""rangeStart"": ""0"", ""rangeEnd"": ""10"", ""rewards"": [ { ""sequence"":0, ""target"": 10, ""reward"": 500 }, { ""sequence"":1, ""target"": 20, ""reward"": 1000 }, { ""sequence"":2, ""target"": 30, ""reward"": 1500 } ] }, { ""strategyName"": ""Strategy 2"", ""rangeStart"": ""10"", ""rangeEnd"": ""20"", ""rewards"": [ { ""sequence"":0, ""target"": 20, ""reward"": 1000 }, { ""sequence"":1, ""target"": 30, ""reward"": 1500 }, { ""sequence"":2, ""target"": 40, ""reward"": 3000 } ] } ], ""incentiveMetadata"":[ { ""metaKey"": ""TestTitleS"", ""metaValue"" : ""ටෙස්ට්"" }, { ""metaKey"": ""TestTitleE"", ""metaValue"" : ""தமிழில் தட்டச்சு"" } ], ""district"": ""Kandy"", ""remarks"": ""nothing"", ""daysConsidered"": 28, ""updatedBy"": ""Imasha"" } Delete Incentive Request Schema The following table includes the request schema of the Delete Incentives REST API. URL http://<endpoint_addr>/driverincentiveportal/iserv/v1/driver/incentives ?incentiveid= {incentiveId} Description Service to delete existing incentive. Request Method DELETE Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/JSON Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Example Values Remarks ""incentiveid"" Mandatory Long 1001 Response Schema (Success): Incentives POST/PUT/DELETE API HTTP 200(OK) response schema of Incentives Creation API will follow the base JSON response structure indicated below. { ""message"" : (String), ""status"" : ""200 OK"", ""path"" : ""/driverincentiveportal/iserv/v1/driver/incentive"" } Please see section Response Schema (Error): Incentives API for Error responses from Incentives. Fetch Incentives Request Schema The following table includes the request schema of the fetch Incentives REST API. URL http://<endpoint_addr>/driverincentiveportal/iserv/v1/driver/incentives ?incentiveid= {incentiveid} Description Service to fetch an existing incentive by incentive id. The call returns the full incentive structure. Request Method GET Request Body Format N/A Response Schema (Success) application/JSON Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/JSON Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Example Values Remarks ""incentiveid"" Mandatory Long 1001 Response Schema (Success) HTTP 200(OK) response schema of Fetch Incentives API will follow the base JSON response structure indicated below. { ""incentiveId"":1001 ""incentiveTitle"": ""New Year Incentive"", ""incentiveType"": ""daily"", ""incentiveCategory"": ""Hailing"", ""effectiveFrom"": ""2021-04-10"", ""effectiveTo"": ""2021-04-17"", ""startTime"": ""1970-01-01 08:00:00"", ""endTime"": ""1970-01-01 12:00:00"", ""serviceGroup"": ""Rides"", ""service"": ""TukH"", ""driverCohort"": ""123"", ""driverCount"": ""15"", ""rewardsBy"": ""LKR"", ""rewardsBasedOn"": ""Trip"", ""completionRatio"": 80.0, ""strategyTargets"": [ { ""strategyName"": ""Strategy 1"", ""rangeStart"": ""0"", ""rangeEnd"": ""10"", ""rewards"": [ { ""sequence"":0, ""target"": 10, ""reward"": 500 }, { ""sequence"":1, ""target"": 20, ""reward"": 1000 }, { ""sequence"":2, ""target"": 30, ""reward"": 1500 } ] }, { ""strategyName"": ""Strategy 2"", ""rangeStart"": ""10"", ""rangeEnd"": ""20"", ""rewards"": [ { ""sequence"":0, ""target"": 20, ""reward"": 1000 }, { ""sequence"":1, ""target"": 30, ""reward"": 1500 }, { ""sequence"":2, ""target"": 40, ""reward"": 3000 } ] } ], ""incentiveMetadata"":[ { ""metaKey"": ""TestTitleS"", ""metaValue"" : ""ටෙස්ට්"" }, { ""metaKey"": ""TestTitleE"", ""metaValue"" : ""தமிழில் தட்டச்சு"" } ], ""district"": ""Kandy"", ""remarks"": ""nothing"", ""daysConsidered"": 28, ""createdDate"": ""2017-08-22"", ""createdBy"": ""Thamal"" ""lastUpdatedDate"": ""2017-08-23"", ""updatedBy"": ""Thamal"" } ""incentiveId"" Long ""incentiveTitle"" string ""incentiveType"" string ""incentiveCategory"" string ""effectiveFrom"" string ""effectiveTo"" string ""startTime"" string ""endTime"" string ""serviceGroup"" string ""service"" string ""driverCohort"" string ""driverCount"" int ""rewardsBy"" string ""rewardsBasedOn"" string ""completionRatio"" double ""strategyTargets"" array object ""strategyName"" string ""rangeStart"" int ""rangeTo"" int ""rewards"" array object ""sequence"" int ""target"" int ""reward"" double ""incentiveMetadata"" array object ""metaKey"" string ""metaValue"" string ""district"" string ""remarks"" string ""daysConsidered"" int ""createdDate"" string ""createdBy"" string ""lastUpdatedDate"" string ""updatedBy"" string Please see section Response Schema (Error): Incentives API for Error responses from Incentives. Filter Incentives Request Schema The following table includes the request schema of the fetch Incentives REST API. URL http://<endpoint_addr>/driverincentiveportal/iserv/v1/driver/incentives/filter ?page= {page} &size= {size}&incentiveTitle={title}&status={status}&serviceGroup={serviceGroup}&service={service}&effectiveFrom={effectiveFrom}&effectiveTo={effectiveTo} Description Service to fetch existing incentives with pagination. Only limited information about each incentive is returned in the call. Request Method GET Request Body Format N/A Response Schema (Success) application/JSON Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/JSON Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Example Values Remarks ""page"" Mandatory Integer 1 Page Number ""size"" Mandatory Integer 2 Incentive count per page ""incentiveTitle"" Optional String ""New Year Incentive"" ""serviceGroup"" Optional String ""Rides"" ""service"" Optional String ""Tuk"" ""status"" Optional String ""Active"" ""Pending"" ""Completed"" ""effectiveFrom"" Optional String ""2021-04-10"" ""effectiveTo"" Optional String ""2021-04-20"" Response Schema (Success) HTTP 200(OK) response schema of Fetch Incentives API will follow the base JSON response structure indicated below. { ""content"": [ { ""incentiveId"": 20210628072020293, ""incentiveType"": ""daily"", ""incentiveCategory"": ""Hailing"", ""incentiveTitle"": ""Test4"", ""effectiveFrom"": ""2017-08-22"", ""effectiveTo"": ""2017-08-22"", ""service"":""Tuk"", ""serviceGroup"": ""Rides"", ""status"": ""Completed"" ""driverCohort"": ""null"", ""driverCount"":0, ""rewardsBy"": ""LKR"", ""rewardsBasedOn"": ""Trip"", ""remarks"": ""nothing"", ""daysConsidered"": 1, ""createdDate"": ""2017-08-22"", ""createdBy"": ""Thamal"" ""lastUpdatedDate"": ""2017-08-23"", ""updatedBy"": ""Thamal"" }, { ""incentiveId"": 4, ""incentiveType"": ""daily"", ""incentiveCategory"": ""Hailing"", ""incentiveTitle"": ""Test5"", ""effectiveFrom"": ""2017-08-22"", ""effectiveTo"": ""2017-08-22"", ""service"":""Tuk"", ""serviceGroup"": ""Rides"", ""status"": ""Completed"" ""driverCohort"": ""null"", ""driverCount"":0, ""rewardsBy"": ""LKR"", ""rewardsBasedOn"": ""Trip"", ""remarks"": ""nothing"", ""daysConsidered"": 1, ""createdDate"": ""2017-08-22"", ""createdBy"": ""Thamal"" ""lastUpdatedDate"": ""2017-08-23"", ""updatedBy"": ""Thamal"" } ], ""pageable"": { ""sort"": { ""sorted"": false, ""unsorted"": true, ""empty"": true }, ""pageNumber"": 0, ""pageSize"": 2, ""offset"": 0, ""paged"": true, ""unpaged"": false }, ""last"": true, ""totalElements"": 2, ""totalPages"": 1, ""first"": true, ""sort"": { ""sorted"": false, ""unsorted"": true, ""empty"": true }, ""size"": 2, ""number"": 0, ""numberOfElements"": 2, ""empty"": false } } content array Contains the incentives objects. object incentiveId Long incentiveType string incentiveCategory string incentiveTitle string effectiveFrom string effectiveTo string service string serviceGroup string status string driverCohort string driverCount int rewardsBy string rewardsBasedOn string remarks string daysConsidered integer ""createdDate"" string ""createdBy"" string ""lastUpdatedDate"" string ""updatedBy"" string ""pageable"" object Pagination Information totalElements integer Total Number of Objects totalPages integer Total Number of Pages size integer Size of the Page number integer Page Number numberOfElements integer Number of Elements in the Page Please see section Response Schema (Error): Incentives API for Error responses from Incentives. Request Headers: Incentives API The following table illustrates the Request Headers that must be included in the Incentives POST/PUT API. Header Name State Possible Value Description Content-Type Mandatory application/JSON The value MUST be application/JSON X-EVOD-DRIVER-PORTAL-INCENTIVES-API-KEY Mandatory dirverportal API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as the value will be changed in a breach. X-EVOD-DRIVER-PORTAL-INCENTIVES-API-SECRET Mandatory feaee65a-2f54-4b18-8a7e-ea409a2e0d72 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as the value will be changed in a breach. X-CORRELATION-ID Optional 223e1203-0237-4770-89f6-31d65843f4c1 If provided, the given Correlation Id will be logged with all log messages logged by the Incentives Information API Service in executing this particular request. This will enable deeper issue analysis from source to incentives information service backend. If the same Correlation Id is tracked on the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, the Incentives Information API Service will generate its own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Schema (Error): Incentives API Error responses from Incentives API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Structure Client error responses and the server error responses from Incentives API will be respective HTTP 4xx and HTTP 5xx responses. The response body is included in the following JSON schema. json { ""errors"": [ { ""correlationId"": ""196d4252-51c7-49d8-b38c-c8138d2f5122"", ""code"": ""DATA-4000"", ""message"": ""Failed to process the request."", ""developerMessage"": ""NotAllowedException"" } ] } errors array An array containing one or more errors. correlationId string A UUID is unique to each request. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes on the consumer side. developerMessage string A more detailed technical error message if needed.A more detailed technical error message if needed Please see section Error Codes for error code messages and corresponding HTTP codes. Error Codes The following tables illustrate the possible error codes and corresponding error messages. Client-Side Error Codes Error Code Error Message Corresponding HTTP Code DATA-1000 Missing Query Parameters 400 DATA-1200 Malformed Request Body Parameters 400 DATA-1300 Missing authentication details. 401 DATA-1400 Modifications not allowed after effective from date 405 DATA-1401 Deletion not allowed after effective from date 405 DATA-4000 Client error. Please send an email to l2support@pickme.lk with the EVOD-DRIVER-PORTAL-INCENTIVES response header value. Any valid client error code as defined in link 10.4 Client Error section. Server-Side Error Codes As per best practices in not exposing the exact error that occurred on the server-side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code DATA-5000 Server error. Please send an email to l2support@pickme.lk with the EEVOD-DRIVER-PORTAL-INCENTIVES response header value. Any valid server error code as defined in Link 10.5 Server Error section.",False
2190999652,page,current,/x/ZACYgg,Promotion framework related work,"Promotion monitoring and evaluation framework Existing promotion investigation Lapsed promotion metric building Analyzing Lapse Promotions Build an ingestion pipeline to capture Lapse promotions details. (3 sprints) Establish a method to tag all customers with promotions that were sent in the past 6 months. For this we need to first ingest promo push information from CleverTap. Segmentation of customers to Control/Treatment groups for promo effectiveness measurement is already completed. All segments will have equal number of customers with similar AOV, order frequency and loyalty tiers. There is a limitation in this segmentation as described below: Problem: Due to lack of visibility and control of promotions sent to customer segments, it is not possible to 100% guarantee that all the evaluated metrics are solely influenced by the selected lapse promotion. Solution: For promotions given after the control/treatment segmentation, we should either exclude or include all segments. For promotions given before segmentation, we can only control this once a mechanism for promotion tagging is incorporated. Evaluation metrics: With these evaluation metrics we intend to answer the following questions. During promo period: Is the increment in order volume and AOV compared to the control group better? Was there an indication of promo push instigating an indirect purchase? Reactivation without promo usage. What is the fraction of customers pushed to a higher AOV due to promotion? Percentage of customers going above min order value compared with control. What was the cost of the promotion? Post promo behavior: Did the treatment group return to status quo? AOV, per day average order count compared with platform performance for the month. Did the control group behave similarly to the treatment group during the month following the promotion? Promo success indicator: Per rupee spent gain/loss = (Actual Purchase – Expected Purchase) – burn / total burn Limitations in the analysis: There is no control over proximity-based platform wide promotions. Such as a Burger King Promotion visible to customers who only fall under its delivery radius vs customers who are outside of it. Therefore, this can affect the outcome of the promo performance metrics. Assumption: Since the data team cannot control which promotions are given to the lapse based customers in the upcoming month, it is assumed that all promotions are either sent or withheld from control/treatment group in the month after Lapse promo end. Experimentation 1 - Lapsed promo multi channel performance",False
2190999680,page,current,/x/gACYgg,Vendor density analysis,"Summary Revenue can be increased by both increasing the number of drivers and merchants in identified areas such as Thimbirigasyaya, etc. For example, there is high demand in Thimbirigasyaya, but both the number of merchants and drivers are low in this area. Although, from an absolute merchant count point-of-view Thimbirigasyaya has the highest number of merchants. Looking at job cancellations most of these are due to driver or merchant issues. In areas such as Maharagama and Dehiwala orders have been cancelled in large volumes due to the unavailability of drivers. Focus should be given to increase the driver availability/sign-up in these areas or drivers can be re-allocated to these areas. Highest sales are in Groceries followed by Bakery and Vegetables. The next popular categories are Elephant House and Litro Gas. In comparison to Colombo district, the Gampaha district has a substantial merchant count, however, this does not seem to convert into sales in similar proportions to that of Colombo. This may be due to the lack of awareness of the services offered via PickMe and a promotion campaign targeting Gampaha district may yield higher sales. When looking at population per hexabin there are areas that do not have any merchants although the population is high. For instance, in Gampaha district itself Minuwangoda, Meerigama and Dompe does not have any merchants. Similarly, the Kaththankudi DS has 14,000+ population per hex but does not have a single merchant – Sathosa is available in this area. Despite introducing our services across the island, the highest sales volumes are from Colombo, Gampaha and Kandy during a 30-day period. The next highest is in Kalutara with 592 sales which is well below that of Kandy. All other districts combined have only yielded 110 sales during the lockdown period – this may be due to the lack of understanding of services offered by PickMe. Kurunegala and Galle has a reasonable number of merchants, but they don’t show enough sales. Data All the findings below are based on new merchants onboarded during the lockdown period from 20 th March to 10 th April 2020. The analysis can be improved further by by looking at the number of active PickMe users (consumers) in these areas to understand the data and sales patterns observed in this report further. District and DS division level presence of merchants: We have been able to onboard merchants from 12 districts within this lockdown period which includes districts such as Jaffna, Badulla, Matara, Hambanthota and Matale. However, the number of merchants in these areas are limited to 1 or 2. As can be seen in figure 1, 68% of all newly onboarded merchants are from the Colombo district (286) which is significantly higher when compared with other districts. Figure 1: District level merchant distribution The DS division level presence can be seen in figure 2. Within Colombo districts the highest onboarding has occurred in Thimbirigasyaya DS division with the next highest being in Colombo, Dehiwala and Kaduwela areas. From Gampaha district, most merchants were onboarded in Wattala closely followed by Katana. Figure 2: DS division level merchant distribution Suggestions for merchant onboarding locations based on population density [1] I will be introducing a term called vendor density subsequentially. To understand the dynamics of vendor density lets first look at the following example. DS division Population No. of hexbins Population per hex Dehiwala (Colombo) 93680.49 38 2465.0 Pathadumbara (Kandy) 95191.74 63 1511.0 Sainthamarathu (Ampara) 28545.95 38 751.0 You can see that both Dehiwala and Pathadumbara has similar population. But area spanning Pathadumbara is about 1.7 times more than that of Dehiwala. Therefore, despite the similarity in population we can see that Pathadumara will need to have a higher number of merchants to service the area when compared with Dehiwala. This applies vice versa as well, if we take Dehiwala and Sainthamarathu in Ampara district, these are similar in area but Dehiwala has 3 times more in populations, therefore despite being of same size Dehiwala requires more merchants when compared with Sainthamarathu. By taking account of the population per hexbin in figure 3, we can highlight the following DS divisions to be areas where we should think about onboarding new merchants to increase sales. Figure 3: Areas with high population density Vendor density based on DS divisions. The vendor density has been calculated using the following equation. Notice that in calculating the vendor density we have not only taken the population of the DS division but has also considered the area covered by that DSD, which both play a role in understanding the number of merchants required to cater these areas. I have included snap of the vendor density Gampaha district DS divisions in figure 4. Figure 4: Population density in Gampaha district You can see that most DS divisions with high population per hexbin has already merchants catering these areas but Minuwangoda does not have any merchants yet. Current vendor density distribution by DS division is as in figure 5. We can notice that the highest vendor density is seen in Kaduwela, second being Dehiwala and third KFG & G.Korale Kandy. Figure 5: Vendor density by DS division and district What one might notice is that Thimbirigasyaya despite having the highest number of new onboard merchants when taking in to account its population the number of merchants available is much less than that of Dehiwala. In Dehiwala even (figure 6) with a low number of new onboard merchants since population is low the vendor density is much higher. Figure 6 In figure 7, you can find the DS division vice breakdown of available merchants stacked by few main categories. The secondary y-axis shows the vendor density overlap over these DS divisions. Figure 7: Merchant distribution by main categories Note that this diagram does not take into consideration the service radius of merchants, therefore there will be merchants in neighboring DS divisions who are able to supply for missing categories. For instance, Magaragama DS division does not have any Merchants selling ‘gas’, but you will notice Litro Gas (Boralesgamuwa) services this area which from Kesbewa DS division. Currently available merchant categories are: Groceries, Vegetables, Dairy (Milk), Baby Care, Beverages (Juices & Drinks), Seafood (fish), Gas (Litro Gas), Meat, Personal care, Cleaners (detergent), Spices, Fruits, Bakery, Snacks, Coconuts, Fresh Produce, Frozen Foods, Ice cream, Pet care, Stationary Correlation between sales and vendor density For this segment total sales volumes from 11 th April 2020 to 10 th May 2020 have been considered, this consists of 30 days’ worth of data during lockdown period. District level order distribution. As can be seen 90% of all sales occurs within the Colombo district. Close to 6% from Gampaha district and the rest are from Kandy, Kalutara and Galle. Figure 8: Order volume by district Something we can notice immediately is that despite having considerably high number of merchants available in Gampaha and Kandy district the sales does not seem proportional to the presence of these merchants when compared to sales in the Colombo district. Let us look in further at DS division level: Figure 9: Order volume by DS division and corresponding vendor density Correlation between vendor density and number of orders can be seen in figure 10. The correlation coefficient of 0.77 indicates a strong positive linear relationship between the vendor density to sales. Figure 10: Regression plot This indicates that increasing the vendor density based on geospatial and population demographics can directly increase order count and sales. Order volume by tag level Figure 11: Order volume by categorical tags Notice that most orders are under groceries, second highest being Bakery and Vegetables followed by Elephant House, Litro Gas and Sathosa. Note that groceries include some vegetable and Sathosa deliveries. Lack of fulfillment based on cancellation reason Distribution of orders bases job status Figure 12: Completed orders and reasons for cancellations with order volume You will notice that from all orders placed during this 30-day period 14% of the orders have not been fulfilled. A closer look at the cancellation reasons will show that out of all cancellations (31,589) about 13% (4,228) which includes both merchant declined and auto cancelled are due to rejections from merchants’ end. Job cancelled occurs due to 3 reasons as per current set up, where it is either due to passenger cancellations, no driver found (hence de queued) and by monitoring portal. When looking closely most of these job cancelled are due to monitoring portal cancellations by passenger request, rider unavailability, restaurant being closed, item unavailability, order delays, upon merchant request, vehicle breakdown, rider requests (device not working, no pass, wrong order, too far). All queue declined cancellations are because the queue is full as we were unable to find drivers to release the queue. Therefore, as a summary we can see that 13% of cancellations are due to merchants, 45% cancellations are inability to find drivers and 42% of the rest will be due to passenger cancellations or cancellations via monitoring portal. Looking at the cancellations by DS division: Figure 13: Order cancellation volume by DS division It was noted that the correlation coefficient between the order volume and the cancellations is 0.95 which indicates a strong positive correlation between order volume and cancellations which is expected. Also, there is a positive correlation between the total cancellations and vendor density where most of the contribution is coming due to queue declined and job cancelled. Again, this is something expected as there is high demand in areas with high vendor density therefore the chance of queue declined, and job cancelled are higher. Cancellations based on DS division for each cancellation can be found separately in the appendix for further reference. Average price distribution to understand what price range most customers are making purchases on, Figure 14: Order volume by order value It can be noticed that most of the orders are between Rs. 600 to Rs 1800 where the average is at Rs.1630 and 50% of the population make orders worth less than is Rs.1400. Which can be noticed in figure 15. Figure 15: Order value (price) distribution This consists the price breakdown in DS division level. It can be noticed that there is no difference depending on the location on the average order amount. Most orders in all DS divisions are between Rs. 1000 – Rs.2000 and the next highest volume is for orders less than Rs.1000. The only change for this happens in the Colombo DS division where the second highest number of orders are for orders between Rs.3000 – Rs.5000. Full report. [1] Population density here refers to population per hexbin.",False
2191097958,page,current,/x/ZoCZgg,Passenger value metrics,"Using historical data from June 2019 This document discusses the groundwork to analyze PickMe passenger Recency, Frequency and Monetary values to introduce passenger segmentation based on three different approaches. Introduction The purpose of this report is to convey the findings of the RFM (Recency, Frequency, Monetary) analysis completed for the month of July 2019. Understanding customer segments based on parameters such as, current RFM value of customer, demographics, purchasing patterns and future customer value will provide the ability for the business to initiate more targeted campaigns which will aim at simulating each customer segment with the right kind of stimulus. The initial analysis as per the operation teams request will be to identify the customer segments based on their Recency, Frequency and Monetary (RFM) value. Recency: How recently did the customer purchase? i.e. Duration since last purchase (this could be interpreted as the last visit day or the last login time to the PickMe app, but for the purpose of this analysis recency was considered to be the last day the passenger hailed and completed a ride) Frequency: Number of completed rides within July 2019. Monetary: The total fare of all the trips completed within July 2019. Several approaches can be used to segment the data. Segmentation based on business input. Segmentation based on ML clustering, using K-means algorithm Segmentation using the RFM_score as a basis. Out of the proposed methods, I have empirically proven instead of the 27 segments used in approach one, we are able to use a much succinct segmentation of 4 clusters based on approach 2 and 3. This is further described in the Results section of this document. The main advantage of this process is to be able to adopt different marketing strategies for different customer segments. Moreover, clustering customers into different groups improves the quality of the recommendation and helps decision makers identify market segments more clearly and therefore develop more effective strategies Detailed document is attached Key take-aways Approach 2 and 3 provides a more granular segmentation of four, compared to the method given by the business that results in 27 segments. These segments are more directly actionable compared to the 27 segments. However, the combination of these segmentations can be used hand in hand to understand passenger behaviors in detail. The distribution of passengers in approach 1 results in an uneven distribution of passengers across the 27 segments with some having less than 50 passengers. Therefore, it will become difficult to use these segments to create value for passengers in an effective manner. Approach 2 and 3 both provide four segments and the categorization of customers is loosely similar. The RFM score-based approach provides a lower number of customers in the best segments (gold & silver) where monetary and frequency values are high compared to the K-means method where the passenger count is almost double. K-means segmentation provides a more even distribution of passengers across the segments compared to the RFM_score-based approach. K-means clustering method’s lower segment has approximately 100,000 passengers with an average monetary value of approximately Rs. 400 compared to the 120,000 passengers with an average monetary value of approximately Rs. 600 in the RFM_score-based method.",False
2191163505,page,current,/x/cYCagg,Subscription price modeling v2,,False
2191196305,page,current,/x/kQCbgg,LightFM vs ALS testing,"Evaluation of LightFM vs Spark MLlib s Alternate Least Square method for personalized recommendations. LightFM One of the key importance in the LightFM is that the model does not suffer from the cold start problem since it allows building hybrid recommender system. This is a special kind of recommender that uses both collaborative filtering and content-based filtering for making recommendations. This means that LightFM can use the normal user-item interactions for making predictions for known users . In the case of new users , it can make predictions if it knows some additional information about these new users. This additional information could be features like gender, age, etc, and must be fed to the algorithm during training. However, in the case of making personalized recommendations for PickMe food, since we do not have any new user demographics in our arsenal, we will not be using this ability of new user recommendations in our deployment. Therefore, this will perform only the pure Collaborative Filtering without using any user-features or item-features. ALS (Alternate least square estimation) ALS is a matrix factorization algorithm and it runs itself in a parallel fashion. I have used the ALS implementation in Apache Spark MLlib which is built for large-scale collaborative filtering problems. The objective function ALS uses is L2 regularization. ALS minimizes two loss functions alternatively; it first holds user matrix fixed and runs gradient decent with item matrix; then it holds the item matrix fixed and runs gradient decent with user matrix. ALS runs its gradient decent in parallel across multiple partions of the underlying training data from a cluster of machines. The hyper parameter tuning was carried out using cross-validation. Most important hyper-parameters in ALS: maxIter: the maximum number of iterations to run (defaults to 10) rank: the number of latent factors in the model (defaults to 10) regParam: the regularization parameter in ALS (defaults to 1.0) Tuning results by grid search CV Best model had close to zero regularization with 20 latent factors. Evaluating Recommender Systems RMSE or MAE – We will perform a test/train split of 80-20 and check with how much accuracy does the recommender system performs. Recall - here 80% of users’ past orders were submitted to the RS, and then, for each test user, I submit only a few purchases and ask the RS to predict the rest. I have hidden several (2 or 3) purchased items and ask the RS for 10 items. Catalog coverage – here I have taken my test users and have asked for recommendation for each of them and put all the recommended items together. Then I have divided the size of this large set of different items by the total number of items in the entire catalog. Instead of the test/train split I can perform a 10 fold cross validation (which I have not attempted) LightFM ALS RMSE 0.15688 0.03288 Recall at K 0.72 0.75 Catalog coverage 72% 78% Both models perform closely, however, ALS takes more time in terms execution time.",False
2195095581,page,current,/x/HYDWgg,ETA for home screen,"Delivery time can be estimated before placing an order based on various factors. This analysis aims to estimate the delivery time from each restaurant to a certain location so that the customer can see it on the app's home screen before placing a food order. The delivery time can be defined as the time from the order created moment to the delivery (status 8) of the food order. This time can be varied depending on the merchant and the customer location. Dataset All completed food delivery orders from 31st October 2020 to 31st January 2021 was used for the analysis. Trainset: 31st October 2020 - 22nd January 2021 Validation set: 22nd of January - 31st of January Historical data: for 30 days - 30th of September - 30th of October Historical data: for 90 days - 30th of July- 30th of October Test set: 12th of February - 21st of February Data cleaning The following data preparation steps were done before the analysis. Dropped multiple statuses for one food order (eg: multiple driver acceptance times, multiple arrival times, etc) and selected the maximum time for each status Imputed missing values in order ready status with prep time expired status Removed orders with missing driver acceptance, arrival and order pickup since those are passenger pickups (data loss - 0.12%) Test merchants were removed from the analysis (data loss - 0.02%) Outliers were removed considering the delivery time (data loss - 7.08%) Outliers were removed considering the AOV (data loss - 5.92%) Missing values of driver count were imputed using 0 considering there are no drivers **Total data loss after cleaning and removing outliers: 6.07% Feature engineering The below list of features was used to predict the delivery time. Average delivery time based on Merchant\Weekday\Hour\DropHex (considering 30 days before the order date) Average times based on Merchant for following times: 7days prep time (status 17/18 - status 12) acceptance time (status 3 - status 17/18) arrival time (status 4 - status 3) waiting time (status 6 - status 4) delivery time (status 8 - order created) en-route time (status 8 - status 6) Pick up DSD of merchant Drop DSD for customer Driver supply average weekday\hour\bin at the level (considering 30 days before the order date) Weekday (as a cyclical feature) Hour (as a cyclical feature) Haversine Distance (in Km) Missing values imputation Some features were created considering different levels such as merchant, weekday, hour and drophex. There can be missing values under several features whenever the data on each of the above levels is missing. For example, the average delivery time feature was calculated on the merchant, weekday, hour and drop hex levels considering the data of the past 30 days. If there are no past orders from merchant A to customer B on a Friday within the past 30 days, an average value for that day cannot be calculated. Thus, the following imputation procedure was introduced to avoid such scenarios. Average times based on Merchant\Weekday\Hour\DropHex: 30 days The average time was calculated considering four different levels. The following table denotes all the instances where there can be a missing value. The first row of the table denotes the instance where the information on an hour is missing. drophex Weekday hour Impute by* 1 1 0 A 1 0 1 B 1 0 0 B 0 1 1 D 0 1 0 C 0 0 1 D 0 0 0 E** The average values for the imputation can be calculated as follows. A - merchant drophex weekday B - merchant drophex C - merchant weekday D - merchant hour E - merchant 2. Average times based on Merchant: 7days If the value is missing for 7 days features, it means there are no orders for the merchant before 7 days from the order date. Thus, those merchants can be considered as new merchants. Considering test set data : ( 12th February 2021 - 21st February 2021 ) variable Missing values No. of restaurants associated with the orders 7 days features 5809 1109 3. Driver count / average driver count : Missing values were imputed by zero considering the driver supply as zero in the considered instance. Descriptive analysis The response variable considered in the analysis was the delivery time which is the time between the order created time and the order delivered to the customer. This was calculated by considering the ‘ order created date time ’ and the ‘ order delivered time ’ (status id 8). The distributions of the delivery time before and after removing the outliers are as follows. Before removing outliers After removing outliers mean 40.83 34.17 St. Dev 31.38 15.45 IQR 24 21 Lower fence -13.0 -9.5 Upper fence 83.0 74.5 90th percentile 67.0 56.0 Correlation analysis The below plot shows the correlation among the selected variables for the analysis. Only the average delivery time is highly correlated with the response variable delivery time. Other variables are slightly but positively correlated with the delivery time. Advanced analysis XGBoost model The XGBoost model was fitted considering the below parameters. 'Colsample_bytree': 0.9, 'eta': 0.10, 'eval_metric': 'rmse', 'max_depth': 7, 'min_child_weight': 12, 'objective': 'reg:linear', 'subsample': 1.0 The accuracy of the model is as follows. Test data based on 10% random split Train Test RMSE 11.88 11.71 MAE 9.22 9.32 Finally, the XGBoost model was tested considering pooled and unpooled data and the dataset for that was obtained considering all the completed food orders from 12th February 2021 to 21st of February 2021. There were 17676 pooled orders and 49117 unpooled orders after the removal of outliers. The distributions of the pooled and unpooled data are as follows. Before removing outliers After removing outliers Pooled data Unpooled data Pooled data Unpooled data Mean 38.26 40.18 35.06 31.35 St. Dev 20.46 30.36 14.86 14.06 IQR 22 24 20 19 Lower fence -8.0 -13.0 -6.0 -7.5 Upper fence 80.0 83.0 74.0 68.5 90th percentile 62.0 66.0 56.0 51.0 Test data based on one week post pooled order deployment Pooled data Unpooled data All data RMSE 12.42 10.91 11.35 MAE 9.35 8.69 8.87 The error is comparatively higher for pooled food orders than for unpooled orders. Limitations Only considered the orders which belong to the food vertical. The delivery time is not predicting for new merchants When the features are missing (new merchant) the default delivery time will be set as 45-55 mins The maximum distance between the customer and the merchant should be less than 13.5km",False
2197750518,page,current,/x/9gL-gg,Retrospectives,"com.atlassian.confluence.plugins.confluence-software-blueprints:retrospectives-blueprint Add Retrospective BDDS com.atlassian.confluence.plugins.confluence-software-blueprints:retrospectives-blueprint retrospective Title Date,Participants",False
2197783088,page,current,/x/MIL-gg,ETA for progress screen,"Delivery Time Prediction Delivery time is the time from the order created moment until the delivery of the order to the customer. The ability to predict the food delivery time is an important factor for customers, merchants and the business as well. An accurate prediction can improve customer satisfaction and help the supply side by dispatching the delivery partners at the right time. The delivery time was predicted by dividing it into several phases as preptime, driver acceptance time, driver arrival time, driver waiting time and driver enroute time. Each phase can be described as follows. Preptime - Time between merchant accepted and order ready (status 17/18 - status 12) Driver acceptance time - Time between order ready and driver accepted (status 3 - status 17/18) Driver arrival time - Time between driver accepted and driver arrived (status 4 - status 3) Driver waiting time - Time between driver arrived and order picked up (status 6 - status 4) Enroute time - Time between order picked up and driver arrived at the destination (status 8 - status 6) **For all events the timestamp of the latest event was considered. Data preparation The following analysis was done considering all the completed food orders from 31st October 2020 and 29th November 2020 obtained from fact_unique_food_orders table. There were 260467 completed food orders during the considered time period and data cleaning and removal of outliers was done prior to the analysis. The steps carried out in the data cleaning stage and the descriptive analysis can be found here . Before outlier removal Predictor Mean St. Dev IQR Upper Fence Lower Fence 90th percentile Prep time 12.43 25.90 8 22 -10 20.00 Acceptance time 8.90 13.37 15 37.5 -22.5 27.41 Arrival time 5.81 7.05 5 14.5 -7.5 13.25 Waiting time 3.06 6.38 3 7.5 -4.5 9.68 Enroute time 12.77 9.38 9 29.5 -6.5 23.65 Total time 44.82 39.45 26.16 89.17 -26.16 77.90 Note: Outliers of the data were removed separately for each model Descriptive statistics The below chart denotes the descriptive statistics of each of the times taken at each stage and the total delivery time after removing the outliers. Delivery time prediction model (Until order pickup) The final delivery time prediction model was created by aggregating several models which are the prep time model, driver acceptance time model, drive arrival time model and driver waiting time. The first three models were trained by using XGBoost regression and a static waiting time was used in order to define the driver waiting time. Note: the enroute time will not be discussed in this analysis Initially, the above models were trained at the time where the merchant accepted the order. The below flow chart denotes how each time is predicted by the models and the final aggregation. More information can be found here . Prep time prediction model The prep time prediction model predicts the time a merchant takes to prepare the order. This time would be the time from merchant accepted time to the order ready/prep time expired stage. The model was fitted after data preprocessing and removing the outliers and the data loss is 6.19%. An XGBoost regression model with the following features was used in order to predict the prep time with an RMSE of 4.63 minutes. More information on prep time prediction can be found here . Dsd Time (as a cyclical feature) - hour, minute Weekday (as a cyclical feature) Service group code Merchant density (hexbin level/ Neighbouring hex level ) Order related features Volume (number of total items) NRT features - (2 hrs before merchant accept the order, merchant level Number of accepted orders NRT features - (1 hr before merchant accept the order, merchant level) NRT average prep time NRT features - (15 mins before merchant accept the order, merchant level) Number of orders placed in last 15 minutes (including not completed orders) Historical features - (using 3 months data, merchant level - (August - October)) Average orders - weekday [hour, min]: merchant level Average cancellations - weekday [hour, min]: merchant level Average prep time (merchant level) Weekly average preptime 90 days average preptime 2. Driver acceptance time model This model predicts the time a driver takes to accept an order. This is the time from the order ready (prep time expired) stage to the driver accepting stage. The model was fitted after data preprocessing and removing the outliers and the data loss is 6.40%. An XGBoost model was used in order to predict the acceptance time with an RMSE of 6.38 minutes. The model features are as follows. Further information on the driver acceptance time model can be found here . Dsd Time (as a cyclical feature) - hour, minute Weekday Service group code Merchant density NRT features - (15 mins before merchant accept the order) NRT Driver count ( hexbin level/ neighbouring hex level) NRT order count (merchant level/ hexbin level/ neighbouring hex level) Historical features - (Using 3 months historical data (August - October) -In 15 minute time intervals in a certain weekday) Average driver count (hexbin level/ neighbouring hex level) Average order count (merchant level/ hexbin level/ neighbouring hex level) Average acceptance time (merchant level) Weekly average acceptance time 90 days average acceptance time 3. Driver arrival time model The driver's arrival time is the time a driver takes to arrive at a merchant. The model was fitted after data preprocessing and removing the outliers and the data loss is 6.24%. An XGBoost model was used to predict the driver arrival time with an RMSE of 3.24 minutes using the following features. Further information on the driver arrival time model can be found here . Dsd Time (as a cyclical feature) - hour, minute Weekday Service group code Merchant density NRT features - (15 mins before merchant accept the order) NRT Driver count ( hexbin level/ neighbouring hex level) NRT order count (merchant level/ hexbin level/ neighbouring hex level) Historical features - (Using 3 months historical data (August - October) - In 15 minute time intervals in a certain weekday) Average driver count (hexbin level/ neighbouring hex level) - used only November data considering the availability Average order count (merchant level/ hexbin level/ neighbouring hex level) Average arrival time (merchant level) Weekly average arrival time 90 days average arrival time 4. Driver waiting time A modelling procedure was not used for the waiting time prediction and a static time of 3 minutes was used as the driver waiting time for each food order. Mean St. Dev IQR Upper fence Lower fence 90th percentile 3.06 6.38 3 7.5 -4.5 9.68 Summary results Train dataset: 31st oct 2020 - 31st Jan 2020 (Random Split ~ test set: 65,000 orders) After outlier removal Predictor Mean St. Dev IQR Upper Fence Lower Fence 90th percentile RMSE (test) RMSE (train) Prep time 7.24 6.24 8 22 -10 16.71 4.63 3.80 Acceptance time 5.17 7.18 9.15 22.98 -13.63 15.61 6.38 5.19 Arrival time 4.47 3.43 4.57 13.22 -5.06 9.63 3.24 3.04 Waiting time 1.96 2.27 3.6 9.18 -5.21 9.68 - - Enroute time 11.75 6.09 9.43 30.82 -6.92 23.65 - - Total time (aggregated results)** 30.26 12.52 17.57 64.77 -5.51 48.5 8.89 - Testing Train dataset new: 1st Jan 2021 - 11th Feb 2021 After outlier removal (test set: Feb Data ~ 85,500, non pooled ~ 64,700) Predictor Mean St. Dev IQR Upper Fence Lower Fence 90th percentile RMSE (train) RMSE (val) RMSE (test all data ) RMSE (test non pooled) Prep time 8.25 6.98 10.33 28.43 -12.89 20.00 4.35 4.62 4.63 4.70 Acceptance time 3.63 5.39 6.05 15.20 -9.00 15.12 4.56 4.79 4.87 4.85 Arrival time 0.05 2.70 3.50 9.9 -5.20 7.35 2.55 2.69 2.58 2.53 Waiting time 3.67 5.19 3.93 10.47 -5.27 9.57 - - - - Enroute time 11.7 7.12 8.2 27.23 -5.57 20.45 - - - - Total time (aggregated results)** 18.80 10.77 14.86 47.67 -11.77 33.43 - - 8.72 8.64 Note: RMSE was recorded at the merchant accepted time **Total time: Time from merchant acceptance to order pickup Summary of the result Limitations Only completed food order data were used The model has assumed there will be no pooled orders and does not encapsulate any scenario with regard to pooling Food orders with multiple driver cancellations were removed All models were trained at merchant accepted time Prep Time was calculated based on the merchants The decrease in the driver supply due to weather changes is not captured in the ETA New merchants who join after the model training date will not be seeing any ETA, until the next iteration. However, they will see an ETA based on averages of similar restaurants in the vicinity. Outliers were removed considering separate models and time durations after the removal of outliers is as follows Time Minimum duration (minutes) Maximum duration (minutes) Preptime 0.00 25.81 Driver acceptance time 0.00 30.47 Driver arrival time 0.05 15.15 Driver waiting time 0.00 9.70 Future work Food categorization and understanding their impact on the prep time. I.e. Some food types will take a longer time to prepare than others even within the same restaurant. There could be changes in the same food category prepared by different restaurants behaving differently. This work needs to follow post-item-level tagging. Considering orders with multiple driver cancellations. There are several driver cancellations happening with some food orders, this scenario needs to be modelled accordingly by taking into consideration the process involved in re-hailing a rider in the event of cancellation. Include driver supply with free status drivers. What we have considered for this model is the total available riders in the previous 15 min in the restaurant location for rider supply. However, this captures information of free, busy and active status riders. It might be appropriate to omit any riders on active and busy status to understand the actual supply. Incorporating weather-related data. For cold start problems, for example, the addition of new merchants after model training we will need to substitute values from similar merchants. Include average features considering merchant, weekday and hour (for prep time, arrival time and acceptance time)",False
2218622992,page,current,/x/EIA9h,2021-06-11 Data Science Initiatives,Date Participants List meeting participants using their @mention names: Goals We try to achieve following objectives Build clarity around data science initiatives Discuss bottlenecks/risks Collectively agree on the deliverable Discussion topics Time Item Presenter Notes 10 min Churn Prediction - haling and food Pulara Churn prediction work extending to Food 15 min Dynamic Pricing - open discussion Pulara/Sanjaya Reference - https://downloads.hindawi.com/journals/mpe/2020/5620834.pdf 5 min Search Optimizations Sanjaya Elastic Search Improvements Action items Add action items to close the loop on open questions or discussion topics: Decisions Type /decision to record the decisions you make in this meeting: e968e7cc-0a12-44f3-8fdc-165573f70769 DECIDED f3f42d19-b3dc-4140-bff0-3b43d389147e,False
2220032289,page,current,/x/IQFTh,Customer Incentives,"Overview Objective of this implementation is to build a generic framework for calculating incentives for customers and analyzing performance of the incentives. Incentives are expected to boost the ride and order counts by encouraging customers to complete more trips/deliveries. Scope Maintain the incentive target configurations Calculate the progress of the incentive depending on the achievement Provide an API to send incentive progress details for passenger back-end Once the targets are met, qualified promotions for that customer must be activated in the promo engine Note: Incentive applicable customer cohort and their targets will be defined by the business. Plan of implementation Newly introduced tables The table structure to store customer incentive target configurations are shown below. (Not a partition table) DB Name: hive_db_stage Table Name: customer_incentive_targets Table Schema: Id - unique ID customer_cohort - The name of the customer cohort to whom the rule applies target_loyalty_points_count - Target of loyalty points count start_date - Target start date end_date - Target end date promo_id - Id of the promotion that will be activated when the target is achieved created_date - Rule created date 2. The table structure to store the customer cohorts details. (Not a partition table) DB Name: hive_db_stage Table Name: customer_cohorts Table Schema: Id - unique ID customer_id - Id of the customer customer_cohort - The name of the cohort to which the customer belongs created_date - customer cohort created date 3. The table structure to store the incentive qualified customer details. (partition table, daily executed) DB Name: hive_db_stage Table Name: customer_incentive_qualifier Table Schema: Id - unique ID customer_id - Id of the customer customer_cohort - The name of the cohort of the customer belongs to target_loyalty_points_count - Target of loyalty points count start_date - Target start date end_date - Target end date promo_id - Id of the promotion that will be activated when the target is achieved date_time - The day before qualification job runs (partition column) 4. The table structure to store the incentive progress details. (partition table, daily executed) DB Name: hive_db_stage Table Name: customer_incentive_progress Table Schema: Id - unique ID customer_id - Id of the customer customer_cohort - The name of the cohort of the customer belongs to target_loyalty_points_count - Target of loyalty points count start_date - Target start date end_date - Target end date promo_id - Id of the promotion that will be activated when the target is achieved achieved_loyalty_points_count - Achieved loyalty point count is_eligible - whether the customer is eligible or not for the promo date_time - The day before job runs (partition column) Existing tables The table structure to store the daily loyalty points of each customer. (Partition table) DB Name: hive_db_stage Table Name: customer_loyalty_daily_points Table Schema: customer_id - customer ID points - Points calculated for the corresponding date date_time - Partition date The main implementation tasks would involve, Introducing a customer_incentive_targets table to store the customer incentive target configurations and initially this table will be populated manually by inserting the incentive target details provided by the business. Introducing a customer_cohorts table to store the customer cohorts details and initially this table will be populated manually by inserting the customer cohort details provided by the business. Introducing a customer_incentive_qualifier table to store the incentive qualified customer details and this will be populated by using customer_incentive_targets and customer_cohorts tables. And this job is planned to run daily and only not expired incentives will be added into daily partition. Introducing a customer_incentive_progress table to store the incentive progress details and this will be populated by using customer_incentive_qualifier (newly introduced) and customer_loyalty_daily_points (existing) tables. And this job is planned to run daily. Identify the customers who met the targets and update the promo engine by sending a post request to the end point of the promotion engine for eligible customers. Design a API to send incentive progress details Design the HBase table to hold the required data for the API Implement the Customer Incentive Progress API External References SRS: https://pickme.atlassian.net/wiki/spaces/BC/pages/1273594053/SRS%2B-Passenger%2BLoyalty%2B-%2BPerformance%2BOffers%2B-%2BRevenue%2BTarget BRS: BRS - Passenger Loyalty - Performance Offers - Revenue Target",False
2230386937,page,current,/x/_QDxh,Passenger Loyalty Performance Offers API,"Revision History Date Version Change Request Date Description Author 2021-06-15 1.0.0 N/A Initial version Suchitha Dehigaspitiya 2021-06-22 1.0.1 2021-06-22 Simplify endpoint optional parameters to a single categorical string value instead of date range. Rename query parameters for consistency. Suchitha Dehigaspitiya 2021-07-15 1.0.2 2021-06-24 Make error response to match with the standard error response set in guidelines Suchitha Dehigaspitiya 2021-07-20 1.0.3 2021-07-15 Response field name change request Suchitha Dehigaspitiya 2021-07-29 1.0.4 2021-07-16 Change API throttling error response to 429 HTTP Status Suchitha Dehigaspitiya 2021-08-02 1.0.5 2021-07-29 Change API response field naming scheme from underscore convention to camel case convention Suchitha Dehigaspitiya Introduction Passenger Loyalty Performance Offers API is one of the APIs hosted in the PickMe data backend under Enterprise View of Customer (EVoC) API domain. This API will offer functionality of fetching the customer’s progress towards achieving the performance targets set by the business. https://git.mytaxi.lk/pickme/data-services/mserv-customer-incentives Request Schema Following table includes the request schema of the Passenger Loyalty Performance Offers API. URL Description API to fetch a customer’s state and progress in relation to the performance targets set based on customer segmentation Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Request Headers: Following table illustrates the Request Headers that must be included in the Passenger Loyalty Performance Offers API. Header Name State Example Value Description X-EVOC-CUST-INC-API-KEY Mandatory androidprod API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-EVOC-CUST-INC-API-SECRET Mandatory feaee65a-2f54-4b18-8a7e-ea409a2e0d72 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-CORRELATION-ID Mandatory 223e1203-0237-4770-89f6-31d65843f4c1 If provided, the given Correlation Id will be logged with all log messages logged by the Passenger Loyalty Performance Offers API in executing this particular request. Query Parameters: Parameter Name State Type Values Description Remarks customerID Mandatory String One valid customer/passenger ID The ID of the passenger whose details are required Only a single customer ID should be provided availability Optional String “all” | “ongoing” | “ scheduled ” This parameter will be used to describe whether to return the progress and details about the ongoing incentives for the customer, scheduled incentives or all. By default the value will be “ongoing”. If an invalid string value is given, the endpoint will give an error response. Response Schema HTTP 200(OK) response for this API endpoint will follow the JSON structure shown below Response Headers: Header Name Description EVOC-CUST-INC-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. json { ""payload"": { ""customerID"": (string), ""incentivesPayload"": [ { ""customerCohort"": (string) , ""targetLoyaltyPointsCount"": (double), ""startDate"": (string YYYY-MM-DD), ""endDate"": (string YYYY-MM-DD), ""promoCode"": (string) , ""achievedLoyaltyPointsCount"": (double), ""isEligible"": (boolean) }, ... ] } } payload array Contains the entire response payload. This array can contain the customers performance progress in relation to his targets. customerID string ID of the desired customer incentivesPayload string An array containing the incentives for customer customerCohort string The identifier of the customer segment defined by the business targetLoyaltyPointsCount double The target amount of loyalty points set by the business for the corresponding segment startDate string Starting date for the performance targets in the YYYY-MM-DD format endDate string Ending date for the performance targets in the YYYY-MM-DD format promoCode string The ID of the promo to be given to customer if customer is eligible fpr the promo achievedLoyaltyPointsCount double The amount if customer loyalty points obtained within the offer period (between start_date and end_date) isEligible boolean true if customer is eligible for promo, false if otherwise Error Response Schema Error responses from Passenger Loyalty Performance Offers API can be two folds, Client errors (4xx codes). Server errors (5xx codes). The following shows the error response of the API which follows the standard and schema as specified by the Error Handling Guidelines . json { ""errors"": [ { ""correlationId"": <string>, ""code"": <string>"", ""message"": <string>, ""developerMessage"": <string> } ] } errors array Array containing one or more errors. correlationId string A UUID unique to each request. If not sent in request, a new will be generated with data team prefix. code string Error code unique to the occurred error. message string A user friendly error message developerMessage string A more detailed technical error message if needed Client Errors: All client initiated error responses from Passenger Loyalty Performance Offers API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. Following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code DATA-10001 Invalid value for incentives type. Argument should be either “all”, “ongoing” or “scheduled” 400 DATA-10002 Missing authentication details. 401 DATA-10003 Throttling limit reached. Try again in a few seconds. 429 Server Errors: Error Code Error Message Corresponding HTTP Code DATA-20001 Error fetching user from HBase 500 Other Errors: Error Code Error Message Corresponding HTTP Code DATA-50001 An error occurred when processing your request 500",False
2230583306,page,current,/x/CgD0h,Deploying Data Services in Kubernetes,"This document describes the process for deploying data services in Staging and Live environments. This is a temporary arrangement until the build process is automated. Following would be the sequence of actions for deployment. Data services are dockerized. Data team would build the final docker image and upload to the docker registry in Azure. The docker image version related to the deployment would be indicated in DR along with the service name. In case of a new service, the memory and cpu requirements would also be indicated in the DR. DevOps team would need to deploy the services from a jumpbox allocated to the environment. Jumpbox login details are given below. Deployment Steps Login to the jumpbox using the details given below ssh -i <key_file> aksjumpbox@ <jumpbox ip> Navigate to the <script location>. The deployment yaml files would be available in this location. Select the correct yaml file related to the service and modify the image version or any other parameters specified in the DR Deploy the relevant service using the kubectl commands kubectl delete -f deliveryeta-deployment.yaml kubectl apply -f deliveryeta-deployment.yaml Check the deployment kubectl --namespace data-services get pods Jumpbox Details Staging jumpbox ip 10.80.4.227 key file aksdevjumpbox.pem (please request from data team) script location: /home/azureuser/deployments/dev Ingress Controller IP 10.80.4.231 Live jumpbox ip (accessible via openvpn) 10.88.4.227 key file aks-prod-jumpbox_key.pem (please request from data team) script location: /home/aksjumpbox/deployments/prod Ingress Controller IP 10.88.4.241",False
2247722044,page,current,/x/PIT5hQ,ETL for Driver Referrals Reporting,"Overview Driver Referral program is awarding some bonus to to an agent/ Driver Partner for introducing a new driver to pickme. To be eligible to get the bonus Invitee should registered with pickme and complete number of trips within the expiry time. The objective is to implement a daily raw data ingestion pipeline to ingest data from referral_log table in driver db and build a ETL pipeline to give more insights of referrals weekly. Raw data ingestion from referral _log Frequency - daily Project - dp-driver-db-raw ETL pipeline of Driver Referral Frequency - daily Project - dp-hailing-legacy-staging Plan of Implementation Raw data ingestion from referral _log In the raw ingestion pipeline it insert the data created yesterday or updated yesterday to a new hive partition in hive_db_raw database. Schema of the referral log Column Type Comment id int Auto Increment referred_by_driver_id int NULL Referred by Driver ID referred_by_driver_name varchar(45) NULL Referred by Driver Name referral_code varchar(45) NULL Referral code of Referee invitee_driver_id int NULL Referred to driver-ID invitee_driver_name varchar(45) NULL Referred to Driver Name invitee_phone_number varchar(45) NULL Referred to driver phone no invitee_service_id int NULL Referred to driver service id invited_date datetime NULL Invite sent time completed_trip_count int NULL Invitee completed trip count at EOD last_trip_on date NULL Last trip date expiry_date date NULL Expiry date for goal completion status varchar(45) NULL INVITE_SENT, REGISTERED, CLAIMED thres_required_trip_count int NULL Required trip count for goal completion thres_reward_bonus float NULL Bonus reward amount created_time datetime NULL Invite sent time activation_time datetime NULL Invitee driver activated time claimed_date datetime NULL Reward claimed timestamp updated_at datetime [now()] Timestamp of row last update ETL pipeline of Driver Referral There are two tables which are populated via the ETL job. fact_referral_log This pipeline has a daily frequency and consider records in last 100 partitions of driver_referral_log table. Algorithm apply the following logic to latest record of each id and populate the yesterday partition of fact_referral_log table Referred by driver-ID - available in the referral_log table Referred by driver status - should take from the dim driver table in stage. can do a left join with referral log table Referred by Driver Primary Service-ID - should take from the driver_active_primary_taxi_model table in hive_db_stage. Referred to driver-ID - available in the referral_log table Referred to driver status - should take from the dim driver table in stage Referred to Driver Primary Service-ID - available in the referral_log table Referral code used - available in the referral_log table Referred Time-Stamp (Timestamp of invitee joined ) - available in the referral_log table ( activation_time) Threshold Trip count - available in the referral_log table (thres_required_trip_count ) Invitee Trip Count - from aggregated_driver_referral_trips table yesterday partition Goal completion status (Pending, Completed, Expired) - should derive from the fields completed_trip_count, thres_required_trip_count, expiry_date available in referral_log table. COALESCE(completion_status,completion_status_t) AS completion_status Goal Completion Amount - available in the table ( thres_reward_bonus ) Invitee goal Completed/Expired Time-Stamp - if it is pending null if completed get the drop timestamp of thres_required_trip_count of that driver else give the expired time stamp Goal expiring timestamp - available in the referral_log table Claimed Status : Claimed/Not Claimed - available in the referral_log table if status in referral log is claimed, Claimed Status is also Claimed otherwise Not Claimed Claimed timestamp : If claimed, the claimed timestamp. Else NULL Based on status - available in the table 2. fact_unique_referral_log Mode report use this table for reporting purpose. the latest record of each id which available in last 100 partitions of fact_referral_log table available in this table.",False
2315386931,page,current,/x/MwACig,Export data dumps from Prod HBase and import data dumps into Dev HBase,"Step 1: Export a table from production HBase into HDFS hbase org.apache.hadoop.hbase.mapreduce.Driver export [table-name] [hdfs-path] Ex: hbase org.apache.hadoop.hbase.mapreduce.Driver export batch_hex_level wasbs://hive-db-stage@hdinprdshrprimpkme.blob.core.windows.net/hbase-dump/deliveryeta/batch_hex_level Step 2: Copy dump files from prod HDFS into prod local file system hadoop fs -copyToLocal [hdfs-path] [local-path] Ex: hadoop fs -copyToLocal wasbs://hive-db-stage@hdinprdshrprimpkme.blob.core.windows.net/hbase-dump/deliveryeta/batch_hex_level/* /home/prd-hbase-ssh/hbase-dump/deliveryeta/batch_hex_level Step 3: Copy dump files from production local file system to dev local file system scp [prod-local-file-path] [usrname]@[address]:[dev-local-file-path] Ex: scp /home/prd-hbase-ssh/hbase-dump/deliveryeta/batch_hex_level/* dev-hbase-ssh@hbs-dev-prim-pkme-ssh.azurehdinsight.net:/home/dev-hbase-ssh/hbase-dump/deliveryeta/batch_hex_level/ Step 4: Copy dump files from dev local file system to dev HDFS location hadoop fs -copyFromLocal [local-path] [hdfs-path] Ex: hadoop fs -copyFromLocal /home/dev-hbase-ssh/hbase-dump/deliveryeta/batch_hex_level/* wasbs://hive-db-stage@hdinhpdevshrprimpkme.blob.core.windows.net/hbase-dump/deliveryeta/batch_hex_level Step 5: Create HBase table in dev Ex: create 'batch_hex_level', {NAME => 'mf', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '2', COMPRESSION => 'NONE', TTL =>'2592000', BLOCKSIZE => '65536', IN_MEMORY =>'true', BLOCKCACHE => 'true', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'false'} enable 'batch_hex_level' Step 6: Import a table from a HDFS dump into existing HBase table hbase org.apache.hadoop.hbase.mapreduce.Driver import [table-name] [hdfs-path] Ex: hbase org.apache.hadoop.hbase.mapreduce.Driver import batch_hex_level wasbs://hive-db-stage@hdinhpdevshrprimpkme.blob.core.windows.net/hbase-dump/deliveryeta/batch_hex_level/",False
2340356221,page,current,/x/fQB-iw,Customer Analytics API,"Revision History Date Version Change Request Description Author 2020-07-16 1.0.0 N/A Initial Version Gihan Prabudda References Reference Version Date Remarks SRS N/A N/A N/A Introduction Merchant Customer Analytics APIs hosted in the PickMe data backend under Enterprise View of Merchant (EVoM) API domain. This API will be integrated into the Merchant Analytics Web portal where merchants can check the history of their customers. Currently, the API is facilitated as a RESTful API. Request Schema The following table includes the request schema of the customer analysis REST API. URL http://<endpoint_addr>/iserv/v1/evom/{ merchantId }/customer analytics Description Service to request customers' buying history based on merchant id and other filtering parameter Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2 : Please see section response schema for response body schema Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Values Description Remarks {merchantId} Mandatory Path Any valid merchant id Id of the merchant whose customers’ buying history is requested None type Optional String all (default) nc rc List of customer types being requested. Multiple types can be specified as a comma separated list. E.g. type=nc, rc URL encoded: type=nc %2C rc all - indicates to include all types nc- new customers rc- return customers Note 1: Listed are only the current supported types, and new types will be added based on the new user types business comes up with. Note 2: Multiple types can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 3: Any unknown types will be silently ignored by the API (i.e. no error will be thrown). If all specified types are unknown, API will return an empty type array. Note 4: If ‘all’ is specified among other types in the comma separated list of types, ‘all’ will be given priority and any other specific types will be ignored. I.e. response will be generated as though only ‘all’ or no type is specified in the request. kpi Optional String all (default) aov aoc ucc List of kpi(s) being requested. Multiple types can be specified as a comma separated list. E.g. kpi=aov, aoc, ucc URL encoded: kpi=aov %2C aoc %2C ucc all - indicates to include all types (default) aov- average order values for the given period of time Aoc- average order count for the given period of time Ucc- unique customer count Note 1: Listed are only the current supported types, and new types will be added based on the new kpis business comes up with. Note 2: Multiple types can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 3: Any unknown types will be silently ignored by the API (i.e. no error will be thrown). If all specified types are unknown, API will return an empty kpi array in the response payload. Note 4: If ‘all’ is specified among other types in the comma separated list of types, ‘all’ will be given priority and any other specific types will be ignored. I.e. response will be generated as though only ‘all’ or no type is specified in the request. Note 5: If data not available for requested KPI, value will be send as 0 period Optional String 1|7 (default) 7|14 1|30 30|60 Period for the kpi(s) being requested. <startDay>|<endDay> Multiple types can be specified as a comma separated list. E.g. period=1|7,7|14,1|30 URL encoded: period=1 %7C 7 %2C 7 %7C 14 %2C 1 %7C 30 1|7 - kpis from day 1(yesterday) to day 7. 7|14 - kpis from day 7 to day 14. 1|30 - kpis from day 1(yesterday) to day 30. 30|60 - kpis from day 30 to day 60. Note 1: Listed are only the current supported periods, and new periods will be added based on the new periods business comes up with. Note 2: Multiple periods can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 3: Any unknown periods will be silently ignored by the API (i.e. no error will be thrown). If all specified types are unknown, API will return 422 error code Request Header The following table illustrates the Request Headers that must be included in the Merchant Analytics API. Header Name State Example Value Description Content Type Mandatory application/json Value must be application/json X-EVOM-MERCHANTANALYTICS-API-KEY Mandatory maprod API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-EVOM-MERCHANT ANALYTICS-API-SECRET Mandatory 3de88065-95f7-4159-9b48-464172b78043 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 9de32d9c-e9d6-4020-953b-408b818ef367 If provided, the given Correlation Id will be logged with all log messages logged by the Merchant Analytics Service in executing this particular request. This will enable deeper issue analysis from source to Merchant Analytics service. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Merchant Analytics Service will generate its own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Structure { ""payload"":{ ""merchantId"":""Integer"", ""types"":[ { ""type"":""String"", ""Kpis"":[ { ""kpiMeta"":{ ""type"":""string"" }, ""kpiPayload"":{ ""values"":[ { ""period"":""string"", ""value"":""Float"" } ] } }, ....... ] } ] } } Payload < object array> Contains the entire response payload.This object array contains the kpis of a given merchant merchantId < Integer > Id of the merchant to whom the kpis in this response is related. type < object array> This array of customer types can contain one or more customer types related for the given merchant. Type < string> Type of user category included in the Payload Kpis < object array> This array of kpis can contain zero or more kpis related for the given merchant under given user type. kpiMeta < object array> Object containing metadata on how to interpret the kpiPayload. Type < string> Type of kpi category included in the kpiPayload (See KPI Category Types subsection below). Period <integer> KPI values calculated period included in the kpiPayload. kpiValue <float> Contains the actual kpi value KPI Category Types: The following table contains the current KPI category types Kpi category Type (kpiMeta => type) Description aov Average sales values for the given time period for the given customer type aoc Average order count for the given time period for the given customer type ucc Unique customer count for the given time period for the given customer type *Note: Integration developers should take required implementation steps to ignore any kpi category types that they do not want to process or do not know how to process. This allows for a future proof integration where the Merchant Customer Analytics KPI Service may add new kpi category types, but integrated parties will not break and will only start consuming such new types when the new code is required for processing such new types are added to the integrated party binaries. Customer Types The Below table contains the current customer types. Customer Types (Utype) Description rc Returned customers nc New customers Response Headers: Header Name Description EVOM-MERCHANTANALYTICS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Error Response Schema Error responses from Promo Code Usage API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Service Analytics API will include the following response header(s). Header Name Description EVOM-MERCHANTANALYTICS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors All client-initiated error responses from Merchant Analytics API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": (String), ""message"": (String) } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. The following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code EVOM-MERCHANTANALYTICS-1300 Missing authentication details. 401 EVOM-MERCHANTANALYTICS-1301 Throttling limit reached. Try again in a few seconds 401 EVOM-MERCHANTANALYTICS-4xx Client error. Please send an email to l2support@pickme.lk with the EVOM-MERCHANTANALYTICS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.4 Client Error section. Server Errors All server error responses from Merchant Analytics API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": ""(String)"", ""message"": ""(String)"" } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. As per best practices in not exposing the exact error that occurred on the server side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code EVOM-MERCHANTANALYTICS-5xx Server error. Please send an email to l2support@pickme.lk with the EVOM-MERCHANTANALYTICS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.5 Server Error section. An Example Request and Response Following is an example request with 'kpi' and ‘period’ optional parameters specified in the request: http://<endpoint_addr>/iserv/v1/evom/100/customeranalytics/?ctype=rc?kpi=aoc %2C aov?period=1 %7C 7 %2C 7 %7C 14 Please note that the following headers were included with the request: X-EVOM-MERCHANTANALYTICS-API-KEY: maprod X-EVOM-MERCHANTANALYTICS-API-SECRET: 3de88065-95f7-4159-9b48-464172b78043 The above request has requested for: Merchant Customer Analytics details for the merchant id 100. The requested customer type is returned customer ( ctype=rc) The requested KPIs are average order count and average order values. ( kpi=aoc %2C aov ) The requested KPI value periods are 1 to 7 days and 7 to 14 days. ( period=1 %7C 7 %2C 7 %7C 14 ) Following is an example response for the above request: { ""payload"": { ""companyId"": 100, ""types"": [ { ""type"": ""rc"", ""kpis"": [ { ""kpiMeta"": { ""type"": ""aov"" }, ""kpiPayload"":{ ""values"": [ { ""period"": ""1|7"", ""value"": 600 }, { ""period"": ""7|14"", ""value"": 500 } ] } }, { ""kpiMeta"": { ""type"": ""aoc"" }, ""kpiPayload"":{ ""values"": [ { ""period"": ""1|7"", ""value"": 6 }, { ""period"": ""7|14"", ""value"": 5 } ] } } ] } ] } } Note the following header value was included into the above response: EVOM-MERCHANTANALYTICS-CORRELATION-ID: 05d0edec-254d-40df-93d5-5a6aef71558e",False
2340487257,page,current,/x/WQCBiw,Sales Analytics API,"Revision History Date Version Change Request Description Author 2020-07-15 1.0.0 N/A Initial Version Shehan Ishanka 2020-10-22 1.1.0 N/A Added authentication for Admin controller to clear caches Shehan Ishanka 2020-11-04 1.1.1 N/A Fixed the bug of changing data type of daily sales Shehan Ishanka 2021-01-26 1.2.0 https://pickme.atlassian.net/browse/DATA-1079 Average order value of period and Previous day sales analytics values Shehan Ishanka References Reference Version Date Remarks SRS Document N/A N/A N/A Introduction Sales Analytics APIs hosted in the PickMe data backend under Enterprise View of Merchant (EVoM) API domain. This API will be integrated into the Merchant Analytics Web portal where merchants can check the history of their sales and orders. Currently, the API is facilitated as a RESTful API. Request Schema The following table includes the request schema of the Promo Code Usage REST API. URL http://<endpoint_addr>/iserv/v1/evom/{ merchantId }/salesanalytics Description Service to request sales history based on merchant id and other filtering parameter Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2 : Please see section response schema for response body schema Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Values Description Remarks {merchantId} Mandatory Path Any valid merchant id Id of the merchant whose promo code sales analytics history is requested None kpi Optional String all (default) ds to aov aovp List of kpi(s) being requested. Multiple types can be specified as a comma separated list. E.g. kpi=ds,to,aov URL encoded: kpi=ds %2C to %2C aov all - indicates to include all types (default) ds - Daily sales of the merchant to - Total orders of the merchant aov - Average order value for the specified period of the merchant aovp - Average order value of the period Note 1: Listed are only the current supported types, and new types will be added based on the new kpis business comes up with. Note 2: Multiple types can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 3: Any unknown types will be silently ignored by the API (i.e. no error will be thrown). If all specified types are unknown, API will return an empty kpi array in the response payload. Note 4: If ‘all’ is specified among other types in the comma separated list of types, ‘all’ will be given priority and any other specific types will be ignored. I.e. response will be generated as though only ‘all’ or no type is specified in the request. period Optional String 1|7 (default) 7|14 1|30 30|60 1|1 2|2 Period for the kpi(s) being requested. <startDay>|<endDay> Multiple types can be specified as a comma separated list. E.g. period=1|7,7|14,1|30 URL encoded: period=1 %7C 7 %2C 7 %7C 14 %2C 1 %7C 30 1|7 - kpis from day 1(yesterday) to day 7. 7|14 - kpis from day 7 to day 14. 1|30 - kpis from day 1(yesterday) to day 30. 30|60 - kpis from day 30 to day 60. 1|1 - kpis from yesterday 2|2 - kpis from day before yesterday Note 1: Listed are only the current supported periods, and new periods will be added based on the new periods business comes up with. Note 2: Multiple periods can be provided as a comma separated list as shown in the example. Please ensure to URL encode all comma (,) characters in the URL (i.e. %2C). Note 3: Any unknown periods will be silently ignored by the API (i.e. no error will be thrown). If all specified types are unknown, API will return 422 error code. Request Header The following table illustrates the Request Headers that must be included in the Merchant Analytics API. Header Name State Example Value Description Content Type Mandatory application/json Value must be application/json X-EVOM-MERCHANTANALYTICS-API-KEY Mandatory maprod API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-EVOM-MERCHANT ANALYTICS-API-SECRET Mandatory 3de88065-95f7-4159-9b48-464172b78043 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 9de32d9c-e9d6-4020-953b-408b818ef367 If provided, the given Correlation Id will be logged with all log messages logged by the Merchant Analytics Service in executing this particular request. This will enable deeper issue analysis from source to Merchant Analytics service. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Merchant Analytics Service will generate its own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Structure { ""payload"": { ""merchantId"": (integer), ""kpis"":[ { ""kpiMeta"":{ ""type"": (string), ""period"": (string) }, ""kpiPayload"":{ ... } } ] } } payload object array Contains the entire response payload.This object array contains the kpis of a given merchant. merchantId integer Id of the driver to whom the kpis in this response is related. kpis array This array of kpis can contain zero or more kpis related for the given merchant. kpiMeta object Object containing metadata on how to interpret the kpiPayload. type string Type of kpi category included in the kpiPayload (See KPI Category Types subsection below). period integer KPI values calculated period included in the kpiPayload. kpiPayload object Contains the actual kpi payload and schema will vary based on the type of kpi category (See KPI Payload Types subsection below). KPI Category Types: The following table contains the current KPI category types. This table also indicates the relevant KPI Payloads sub-sections which provide the exact kpiPayload schema for each supported KPI category type. New KPI category types can be added in the future based on the use cases. Kpi category Type (kpiMeta => type) Description Related kpiPayload subsection ds Sales values for each day daily sales based KPI Payload to Total Orders for each day total orders based KPI payload aov Average Order Values for each day average order values based KPI payload aovp Average Order Value for a period average order value period based KPI payload *Note: Integration developers should take required implementation steps to ignore any kpi category types that they do not want to process or do not know how to process. This allows for a future proof integration where the Sales Analytics KPI Service may add new kpi category types, but integrated parties will not break and will only start consuming such new types when the new code required for processing such new types are added to the integrated party binaries. KPI Payload Types Please ensure that you have gone through the Response Schema section for the base JSON response structure before reading this section. daily sales based kpi Payload (kpiMeta => type => do) { “values” : [ { “rank”: (string), “value”: (float) } ] } Object containing kpi details of daily sales category type . dailySales array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value float Sales value for a particular day total orders based kpi Payload (kpiMeta => type => to) { “values” : [ { “rank”: (string), “value”: (integer) } ] } Object containing kpi details of total orders category type . totalOrders array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value integer Total orders value for a particular day average order values based kpi Payload (kpiMeta => type => aov) { “values” : [ { “rank”: (string), “value”: (float) } ] } Object containing kpi details of daily sales category type . averageOrderValues array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value float Average order value for a particular day average order value period based KPI payload (kpiMeta => type => aovp) { “values” : [ { “rank”: (string), “value”: (float) } ] } Object containing kpi details of daily sales category type . averageOrderValuePeriod array rank string Empty string value float Average order value for a period Response Headers: Header Name Description EVOD-MERCHANTANALYTICS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Error Response Schema Error responses from Promo Code Usage API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Service Analytics API will include the following response header(s). Header Name Description EVOM-MERCHANTANALYTICS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors All client-initiated error responses from Merchant Analytics API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": (String), ""message"": (String) } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. The following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code EVOM-MERCHANTANALYTICS-1300 Missing authentication details. 401 EVOM-MERCHANTANALYTICS-1301 Throttling limit reached. Try again in a few seconds 401 EVOM-MERCHANTANALYTICS-4xx Client error. Please send an email to l2support@pickme.lk with the EVOM-MERCHANTANALYTICS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.4 Client Error section. Server Errors All server error responses from Merchant Analytics API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": ""(String)"", ""message"": ""(String)"" } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. As per best practices in not exposing the exact error that occurred on the server side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code EVOM-MERCHANTANALYTICS-5xx Server error. Please send an email to l2support@pickme.lk with the EVOM-MERCHANTANALYTICS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.5 Server Error section. An Example Request and Response Following is an example request with 'kpi' and ‘period’ optional parameters specified in the request: http://<endpoint_addr>/iserv/v1/evom/100/salesanalytics/?kpi=ds %2C aov?period=1 %7C 7 %2C 7 %7C 14 Please note that the following headers were included with the request: X-EVOM-MERCHANTANALYTICS-API-KEY: maprod X-EVOM-MERCHANTANALYTICS-API-SECRET: 3de88065-95f7-4159-9b48-464172b78043 The above request has requested for: Sales Analytics details for the merchant id 100. The requested KPIs are daily sales and average order values. ( kpi=ds %2C aov ) The requested KPI value periods are 1 to 7 days and 7 to 14 days. ( period=1 %7C 7 %2C 7 %7C 14 ) Following is an example response for the above request: { ""payload"": { ""merchantId"": 108, ""kpis"": [ { ""kpiMeta"": { ""type"": ""ds"", ""period"": ""1|7"" }, ""kpiPayload"": { ""values"": [ { ""rank"": 22/06 Monday, ""value"": 100 }, { ""rank"": 21/06 Tuesday, ""value"": 80 }, { ""rank"": 20/06 Wednesday, ""value"": 110 }, { ""rank"": 19/06 Thursday, ""value"": 120 }, { ""rank"": 18/06 Friday, ""value"": 90 }, { ""rank"": 17/06 Saturday, ""value"": 60 }, { ""rank"": 16/06 Sunday, ""value"": 80 } ] } }, { ""kpiMeta"": { ""type"": ""ds"", ""period"": ""7|14"" }, ""kpiPayload"": { ""values"": [ { ""rank"": 15/06 Monday, ""value"": 0 }, { ""rank"": 14/06 Tuesday, ""value"": 180 }, { ""rank"": 13/06 Wednesday, ""value"": 120 }, { ""rank"": 12/06 Thursday, ""value"": 220 }, { ""rank"": 11/06 Friday, ""value"": 190 }, { ""rank"": 10/06 Saturday, ""value"": 160 }, { ""rank"": 09/06 Sunday, ""value"": 180 } ] } }, { ""kpiMeta"": { ""type"": ""aov"", ""period"": ""1|7"" }, ""kpiPayload"": { ""values"": [ { ""rank"": 22/06 Monday, ""value"": 10.0 }, { ""rank"": 21/06 Tuesday, ""value"": 8.0 }, { ""rank"": 20/06 Wednesday, ""value"": 1.10 }, { ""rank"": 19/06 Thursday, ""value"": 12.0 }, { ""rank"": 18/06 Friday, ""value"": 9.0 }, { ""rank"": 17/06 Saturday, ""value"": 6.0 }, { ""rank"": 16/06 Sunday, ""value"": 8.0 } ] } }, { ""kpiMeta"": { ""type"": ""aov"", ""period"": ""7|14"" }, ""kpiPayload"": { ""values"": [ { ""rank"": 15/06 Monday, ""value"": 0 }, { ""rank"": 14/06 Tuesday, ""value"": 18.0 }, { ""rank"": 13/06 Wednesday, ""value"": 12.0 }, { ""rank"": 12/06 Thursday, ""value"": 2.20 }, { ""rank"": 11/06 Friday, ""value"": 1.90 }, { ""rank"": 10/06 Saturday, ""value"": 1.60 }, { ""rank"": 09/06 Sunday, ""value"": 1.80 } ] } } ] } } Note the following header value was included into the above response: EVOM-MERCHANTANALYTICS-CORRELATION-ID: 05d0edec-254d-40df-93d5-5a6aef71558e",False
2344124437,page,current,/x/FYC4iw,Merchant Analytics Promotions API,"Table Content 1 7 Revision History Date Version Change Request Description Author 2021-07-06 1.0.0 N/A Initial Version Aathif Sanaya 2022-04-18 1.0.1 N/A Release for Feedback Discount Sulhi Cader References Reference Version Date Remarks SRS Document N/A N/A N/A Introduction Merchant Analytics Promotions APIs hosted in the PickMe data backend under Enterprise View of Merchant (EVoM) API domain. This API will be integrated to the Merchant Analytics Web portal where merchants can check their ongoing and past promotions. Currently the API is facilitated as a RESTful API. Fetch Promotions Request Schema Following table includes the request schema of the Merchant Promotions REST API. URL http://<endpoint_addr>/merchantanalytics/iserv/v1/evom/{ merchantId }/promotions Description Service to request promotion history based on merchant id and other filtering parameter Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2 : Please see section response schema for response body schema Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Values Description Remarks {merchantId} Mandatory Path Any valid merchant id Id of the merchant whose promotion history is requested None type Optional String all ( default ) sm fd Requested promotion type Promotions would be filtered according to the type. sm - Spend More Save More fd - Feedback Discount If the type is not specified return all the promotion Note 1: Any other value for type would result in an error code 1302 be thrown. startDate Optional String Any valid date in the following format yyyy-MM-dd Starting date for the promotions(s) being requested. Note 1: Invalid date would be returned with an error code 1303. Note 2: If both start date and end date are provided, Start date should always be earlier than the end date, else an error code 1304 would be thrown. endDate Optional String Any valid date in the following format yyyy-MM-dd Ending date for the promotions(s) being requested. Note 1: Invalid date would be returned with an error code 1303. Note 2: If both start date and end date are provided, Start date should always be earlier than the end date, else an error code 1304 would be thrown. status Optional String ongoing expired cancelled scheduled all Current status of the promotion Promotions would be filtered according to the status. Note 1: Invalid status (anything which is different than given) will be returned with an error code 1305. offset Optional Integer 0 (default) Any >0 integer value Used by the API to support pagination. Any <0 values will be considered as 0. limit Optional Integer not specified (default) Any >0 integer value Used by the API to support pagination. Note 1: If not specified (i.e. default), all promotions from the specified offset will be returned. Note 2: Any <0 values will be considered as the default (i.e. will be ignored and considered as limit is not specified). Note 3: If the specified limit is larger than the remaining promotions from the specified offset, then all promotions from the specified offset will be returned. Note 4: If the specified offset is the index of the last promotions object, then the specified limit will be ignored. Response Schema http://<endpoint_addr>/merchantanalytics/iserv/v1/evom/100/ promotions /?type=sm?startDate=2021-06-01?endDate=2021-06-28?status=ongoing?offset=0?limit=10 { ""payload"": { ""merchantId"": 100, “count::1, “offset:”1, “limit:1, ""promotions"": [ { ""promoMeta"": { ""id"": ""2451"", ""name"": ""Spend 500, Save 100"", ""type"": ""sm"", ""startDate"": ""2021-06-25"", ""endDate"": ""2021-07-02"", ""currencyCode"": ""LKR"", ""totalBudget"": 15000.00, “promoCode”: “ABCD” }, ""promoPayload"": { ""status"": ""ongoing"", ""budgetSpent"": 12500.00, “newCustomers”: 25, “redeemedOrders”: 38, “remainingRedemptions”: 12, “promotionalSales”: 17000.00, “redeemedAmount”: 16000.00 } } ] } } payload object array Contains the entire response payload.This object array contains the promotions of a given merchant. merchantId integer Id of the merchant to whom the promotions in this response is related. count integer Count of the Payload Array items offset integer Offset for pagination limit integer Limit for pagination promotions array This array of promotions can contain zero or more related to the given merchant. promoMeta object Object containing metadata on how to interpret the promoPayload. id integer Promotion ID name string Name of the promotion type string Type of promotion (See Promo Category Types subsection below).. promoCode string promo code of the promotion startDate string Initiation date of the promotion. endDate string Ending date of the promotion. currencCode string Currency code of the country to which the merchant belongs. (ex: LKR) totalBudget float Total budget allocated for the promotion. promoPayload object Contains the actual promo payload and schema will vary based on the type of promo category (See Promo Payload Types subsection below). Promotion Category Types: Following table contains the current promotion types available for a merchant plus user. This table also indicates the relevant Promotion Payloads sub-sections which provide the exact promoPayload schema for each supported Promotion type. Promo category Type (promoMeta => type) Description Related kpiPayload subsection sm Spend More Save More Spend more based promotion Payload fd Feedback Discount Feedback Discount Based promotion Payload Promotion Payload Types Please ensure that you have gone through the Response Schema section for the base JSON response structure before reading this section. Spend more based promotion Payload (promoMeta => type => sm) { “status” : (string) ""budgetSpent"": (float), ""newCustomers"": (int), ""redeemedOrders"": (int), ""remainingRedemptions"": (int), ""promotionalSales"": (float), ""redeemedAmount"": (float) } Object containing promotion details of spend more save more promotion type . sm string Object containing performance statistics. status string Current status of the promotion. Ongoing Expired Cancelled Scheduled budgetSpent float Budget which is already been spent. newCustomers int Customers who have not ordered from the specific outlet within the last 90 days (configurable) and has placed the order with the specific promotion applied redeemedOrders int The number of completed orders that redeemed the promotion remainingRedemptions int The remaining number of orders that can redeem the promotion. (Only for Spend More, Save More type promotions) promotionalSales float The sum of the order values of all the orders that redeemed the promotion. redeemedAmount float The total sum of the rewards that were redeemed Fetch Daily KPIs Request Schema Following table includes the request schema of the Promo Code Usage REST API. URL http://<endpoint_addr>/merchantanalytics/iserv/v1/evom/{ merchantId }/promotions/{ promoId }/promoanalytics Description Service to request daily performance metrics based on merchant id, promo id and other filtering parameter Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2 : Please see section response schema for response body schema Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters Parameter Name State Type Values Description Remarks {merchantId} Mandatory Path Any valid merchant id Id of the merchant whose promotion history is requested None {promoId} Mandatory Path Any valid promotion id Id of the merchant promotion whose daily metrics is requested None kpi Mandatory String nc ro rr ps ra Requested kpi values nc - New Customers ro - Redeemed Orders rr - Remaining Redemptions ps - Promotional Sales ra - Redeemed Amount Note 1: Invalid kpi values would result in error code 1306. period Optional String 1|7 (default) 1|30 overall Period for the kpi(s) being requested. <startDay>|<endDay> Eg: period=1|7 1|7 - kpis from day 1(yesterday) to day 7. 1|30 - kpis from day 1(yesterday) to day 30. overall - kpis from the promotion start date till the current date. Note 1: Listed are only the current supported periods, and new ranges will be added based on the new periods business comes up with. Note 2: Invalid period values would result in error code 1307. Response Schema http://<endpoint_addr>/merchantanalytics/iserv/v1/evom/100/promotions/205/promoanalytics/?kpi=nc?period=1|7 { ""payload"": { ""merchantId"": 100, ""promoId"": 205, ""kpi"":{ “kpiMeta”:{ “type”:nc, “period”:1|7 }, “kpiPayload”:{ “values”:[ { ""rank"": 22/06 Monday, “value”:5 }, { ""rank"": 23/06 Tuesday, “value”:6 }, { ""rank"": 24/06 Wednesday, “value”:8 }, { ""rank"": 25/06 Thursday, “value”:11 }, { ""rank"": 26/06 Friday, “value”:12 }, { ""rank"": 27/06 Saturday, “value”:15 }, { ""rank"": 28/06 Sunday, “value”:20 } ] } } } payload object array Contains the entire response payload.This object array contains the kpis of a given merchant promotion. merchantId integer Id of the merchant. promoId integer Id of the promotion. kpi object This object can contain the kpi related for the given merchant promotion kpiMeta object type string Type of kpi category included in the kpiPayload (See KPI Category Types subsection below). period string Period for the kpi(s) being requested. kpiPayload object Contains the actual kpi payload and schema will vary based on the type of kpi category (See KPI Payload Types subsection below). KPI Category Types: Following table contains the current KPI types available for a merchant plus user. This table also indicates the relevant KPI Payloads sub-sections which provide the exact kpiPayload schema for each supported KPI type. KPI category Type (kpiMeta => type) Description Related kpiPayload subsection nc Customers who have not ordered from the specific outlet within the last 90 days (configurable) and has placed the order with the specific promotion applied. New Customers based KPI Payload. ro The number of completed orders that redeemed the promotion. Redeemed Orders based KPI Payload. rr The remaining number of orders that can redeem the promotion. (Only for Spend More, Save More type promotions) Remaining Redemptions based KPI Payload. ps The sum of the order values of all the orders that redeemed the promotion. Promotional Sales based KPI Payload. ra The total sum of the rewards that were redeemed. Redeemed Amount based KPI Payload. KPI Payload Types Please ensure that you have gone through the Response Schema section for the base JSON response structure before reading this section. new customers KPI Payload (kpiMeta => type => nc) { “values” : [ { “rank”: (string), “value”: (int) } ] } Object containing KPI details of new customers kpi type . nc array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value integer New customers for the particular day. redeemed orders KPI Payload (kpiMeta => type => ro) { “values” : [ { “rank”: (string), “value”: (int) } ] } Object containing KPI details of redeemed orders kpi type . ro array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value integer Redeemed Orders for the particular day. remaining redemptions KPI Payload (kpiMeta => type => rr) { “values” : [ { “rank”: (string), “value”: (int) } ] } Object containing KPI details of remaining redemptions kpi type . rr array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value integer Remaining Redemptions for the particular day. promotional sales KPI Payload (kpiMeta => type => ps) { “values” : [ { “rank”: (string), “value”: (float) } ] } Object containing KPI details of promotional sales kpi type . ps array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value integer Promotional Sales for the particular day. redeemed amount KPI Payload (kpiMeta => type => ra) { “values” : [ { “rank”: (string), “value”: (float) } ] } Object containing KPI details of redeemed amount kpi type . ra array rank string Date rank of the sales value object. E.g :- 22/06 Monday, 23/06 Tuesday, 24/06 Wednesday, 25/06 Thursday, ... value integer Redeemed Amount for the particular day. Request Header Following table illustrates the Request Headers that must be included in the Merchant Analytics API. Header Name State Example Value Description Content Type Mandatory application/json Value must be application/json X-EVOM-MERCHANTANALYTICS-API-KEY Mandatory maprod API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-EVOM-MERCHANT ANALYTICS-API-SECRET Mandatory 3de88065-95f7-4159-9b48-464172b78043 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 9de32d9c-e9d6-4020-953b-408b818ef367 If provided, the given Correlation Id will be logged with all log messages logged by the Merchant Analytics Service in executing this particular request. This will enable deeper issue analysis from source to Merchant Analytics service. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Merchant Analytics Service will generate its own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Headers: Header Name Description EVOD-MERCHANTANALYTICS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Error Response Schema Error responses from Promo Code Usage API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Service Analytics API will include the following response header(s). Header Name Description EVOM-MERCHANTANALYTICS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors All client initiated error responses from Merchant Analytics API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""correlationId"": (String), ""code"":(String), ""message"": (String), ""developerMessage"": (String) } ] } errors array Array containing one or more errors. correlationId string A UUID unique to each request code string Error code unique to the occurred error. message string A friendly error message. developersMessage string A more detailed technical error message if needed. Following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code DATA-1300 Missing authentication details. 401 DATA-1301 Throttling limit reached. Try again in a few seconds 401 DATA-1302 Invalid promotion type 400 DATA-1303 Invalid date. 400 DATA-1304 Start date should be earlier than the end date. 400 DATA-1305 Unknown status id 400 DATA-1306 Invalid kpi value. 400 DATA-1307 Invalid period. 400 DATA-1xxx Client error. Please send an email to l2support@pickme.lk with the EVOM-MERCHANTANALYTICS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.4 Client Error section. Server Errors All server error responses from Merchant Analytics API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""code"": ""(String)"", ""message"": ""(String)"" } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. As per best practices in not exposing the exact error that occurred on the server side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code DATA-5xxx Server error. Please send an email to l2support@pickme.lk with the EVOM-MERCHANTANALYTICS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.5 Server Error section.",False
2348908566,page,current,/x/FoABj,Merchant Analytics,"Overview Merchant Analytics provides different analytics for Merchant Plus Portal. For now, the analytics are Sales Analytics Customer Analytics Item Analytics GitHub Repository for ETL : dp-merchant-hadoop GitHub Repository for API : mserv-merchant-portal Azkaban Project : dp-merchant-hadoop Sales Analytics SRS Doc : SRS - FOOD - Merchant Plus: Sales Analytics API Doc : Sales Analytics API ETL Azkaban flow name : merchant_analytics_sales_analytics_flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 06:20 AM IST Sales Analytics Job : Under sales analytics, the following KPIs are computed. Sales No. of Orders Average Order Value Average Order Value Period KPIs are expected to be compared under the following periods. Today and Yesterday Last week and week before last week Last month and month before last month For computations, the following two tables are populated. Daily Sales Analytics Table Description : Daily Sales analytics of merchants Table Name : hive_db_insights.daily_sales_analytics Table Type : External, Date Time partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/daily_sales_analytics column type comment MerchantId BIGINT ID of the merchant OrderCount BIGINT Total orders of the merchant SalesTotal DOUBLE Total sales from the merchant AverageOrderValue DOUBLE Average Order Value of a merchant UpdatedHour INT Sales Analytics updated hour CreatedAt STRING Records ingested timestamp date_time STRING Partition Column Sample Data merchantid ordercount salestotal averageordervalue createdat updatedhour date_time 12476 42 33550 798.8095238 2020-10-04 20:10:21 12 2020-08-05 44490 8 3090 386.25 2020-10-04 20:10:21 11 2020-08-05 2. Sales Analytics Table Description : Aggregated Sales analytics of merchants Table Name : hive_db_insights.sales_analytics Table Type : External, Date Time partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/sales_analytics column type comment MerchantId BIGINT ID of the merchant DatePeriod STRING Date period : 1|1, 1|7, 1|30 DateRank STRING Date rank : 2020-06-22, 2020-06-23 OrderCount BIGINT Total orders of the merchant for a certain period SalesTotal DOUBLE Total sales from the merchant for a certain period AverageOrderValue DOUBLE Average Order Value of a merchant for a certain period CreatedAt STRING Records ingested timestamp date_time STRING Partition Column Sample Data merchantid dateperiod daterank ordercount salestotal averageordervalue createdat date_time 42357 1|1 2021-07-12 12:00:00 1 650 650 2021-07-13 6:21:01 2021-07-12 42357 1|1 2021-07-12 13:00:00 3 2050 683.3333333 2021-07-13 6:21:01 2021-07-12 After these tables are populated, the pre bulk loader job generates data in text format to be uploaded to HBase table. Pre bulk loader : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/dp-merchant-hadoop/merchant-analytics/sales-analytics/txtfiles HBase Table : merchant_analytics HBase Column Family : analytics Sample pre bulk loader text Data Sales Analytics Job Recovery : Run the jobs from the beginning whenever the job fails from any step as all the steps are INSERT OVERWRITE queries. Customer Analytics SRS Doc : SRS - FOOD - Merchant Plus: Customer Analytics API Doc : Customer Analytics API ETL Azkaban flow name : merchant_analytics_customer_analytics_flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 06:00 AM IST Customer Analytics Job : Under customer analytics, the following KPIs are computed. Returning customers (rc)- Order count is more than 1 within the period (90 days) Unique Customer Count Average Order Count Average Order Value New customers (nc)- Order count is 1 within the period (90 days) Unique Customer Count Average Order Count Average Order Value KPIs are expected to be compared under the following periods. Last week and week before last week Last month and month before last month For computations, the following table is populated. Customer Analytics Table Description : Customer analytics of merchants Table Name : hive_db_insights.customer_analytics Table Type : External, Date Time partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/customer_analytics column type comment MerchantId BIGINT ID of the merchant CustomerType STRING Type of the Customer; rc - return customer, nc - new customer DatePeriod STRING Date period : 1|7, 1|30 UniqueCustomerCount INT Unique Customer Count for a certain period and certain customer type of a merchant AverageOrderCount DOUBLE Average Order Count for a certain period and certain customer type of a merchant AverageOrderValue DOUBLE Average Order Value for a certain period and certain customer type of a merchant CreatedAt STRING Records ingested timestamp date_time STRING Partition Column Sample Data merchantid customertype dateperiod uniquecustomercount averageordercount averageordervalue createdat date_time 4276 rc 1|7 63 1.174603175 1640.31746 2021-07-13 6:00:36 2021-07-12 49390 nc 1|7 3 1.333333333 783.3333333 2021-07-13 6:00:36 2021-07-12 After the above table is populated, the pre bulk loader job generates data in text format to be uploaded to HBase table. Pre bulk loader : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/dp-merchant-hadoop/merchant-analytics/customer-analytics/txtfiles HBase Table : merchant_analytics HBase Column Family : analytics Sample pre bulk loader text Data Customer Analytics Job Recovery : Run the jobs from the beginning whenever the job fails from any step as all the steps are INSERT OVERWRITE queries. Item Analytics SRS Doc : SRS - FOOD - Merchant Plus: Item Analytics API Doc : ETL Azkaban flow name : merchant_analytics_item_analytics_flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 06:10 AM IST Item Analytics Job : Under item analytics, the following KPIs are computed. Item performance by order count Item performance by sales value KPIs are expected to be compared under the following periods. Last week and week before last week Last month and month before last month For computations, the following two tables are populated. Daily Item Analytics Table Description : Merchant item wise sales and order number analytics table Table Name : hive_db_insights.daily_item_analytics Table Type : External, Date Time partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/daily_item_analytics column type comment MerchantId BIGINT ID of the merchant ItemId BIGINT ID of the item ItemName STRING Name of the item OrderCount BIGINT Total orders of the item SalesTotal DOUBLE Total sales from the item CreatedAt STRING Records ingested timestamp date_time STRING Partition Column Sample Data merchantid itemid itemname ordercount salestotal createdat date_time 402667 1233 Dummy 5 4290 2021-07-13 6:10:32 2021-07-12 442229 6777 Flip 1 0 2021-07-13 6:10:32 2021-07-12 2. Item Analytics Table Description : Aggregated Item analytics of merchants Table Name : hive_db_insights.item_analytics Table Type : External, Date Time partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/item_analytics column type comment MerchantId BIGINT ID of the merchant DatePeriod STRING Date period : 1|7, 1|30 ItemId BIGINT ID of the item ItemName STRING Name of the item OrderCount BIGINT Total orders of the item SalesTotal DOUBLE Total sales from the item CreatedAt STRING Records ingested timestamp date_time STRING Partition Column Sample Data merchantid dateperiod itemid itemname ordercount salestotal createdat date_time 4736 1|7 209395 Tom Yam Seafood Soup (Cup) 1 580 2021-07-13 6:11:25 2021-07-12 4736 1|7 209902 Chicken Fried Rice Set Menu 3 1980 2021-07-13 6:11:25 2021-07-12 After these tables are populated, the pre bulk loader job generates data in text format to be uploaded to HBase table. Pre bulk loader : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/dp-merchant-hadoop/merchant-analytics/item-analytics/txtfiles HBase Table : merchant_analytics HBase Column Family : analytics Sample pre bulk loader text Data Item Analytics Job Recovery : Run the jobs from the beginning whenever the job fails from any step as all the steps are INSERT OVERWRITE queries. Promo Analytics SRS Doc : SRS - Delivery - ""Spend More, Save More"" Merchant Promotions API Doc : Merchant Analytics Promotions API ETL Azkaban flow name : merchant_analytics_promo_analytics_flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: - Promo Analytics Job : Provides KPIs of Merchant specific Delivery Promos where started and ended within the last 90 days, over the periods last 7 days, last 30 days, and Overall (performance from the promotion start date till the current date) Compute KPIs based on conditions KPIs Budget spent over the total specified budget - calculated based on the redemptions New Customers- Customers who have not ordered from the specific outlet within the last 90 days (configurable) and has placed the order with the specific promotion applied Redeemed Orders Remaining Redemptions Promotional Sales - Sum of the order values of all the orders that redeemed the promotion. Redeemed Amount- Total burn from all the redemptions Date Ranges Last 7 days Last 30 days Overall - performance from the promotion start date till the current date Filters Ongoing promotion and any past promotions that have run (started and ended) within the last 90 days. Promo types Currently KPIs calculated for two types of merchant promotions. The merchant promotion type is identified from Promotion_tag colum in hive_db_raw.hailing_pm_promotion ( pm_promotion ) . spend_more feedback_discount Table used to do the calculations Promotion_tag colum from hive_db_raw.hailing_pm_promotion → eztaxi pm_promotion table hive_db_raw.hailing_pm_promotion_condition→ eztaxi pm_promotion_condition table hive_db_raw.hailing_pm_promotion_usage_history→ eztaxi pm_promotion_usage_history table hive_db_stage.fact_unique_trip → extaxi passenger_log_archive table hive_db_stage.fact_food_orders → food_db jobs table 2. For computations, the following three tables are populated. Promo Metadata Table Description : Promo metadata Table Name : hive_db_stage.promo_meta_data Table Type : External, Non-partitioned Table Location : wasbs://hive-db-stage@hdinprdshrprimpkme.blob.core.windows.net/promo_meta_data column type comment PromoId BIGINT ID of the promo MerchantId BIGINT ID of the merchant. If the promo is not tagged to a merchant, then use -1. PromoName STRING Promo Name PromoCode STRING Promo code PromoType STRING Type of the promo.merchantPromo-Promo created through merchant portal.pickmePromo-Promos given by pickme PromoStatus STRING Status of the promo-Ongoing ,Expired ,Cancelled, Scheduled CurrencyCode STRING Currency code StartDate Timestamp Promotion Campaign Start Date EndDate Timestamp Promotion Campaign End Date Budget DOUBLE Defined budget RedemptionCount INT Defined redemptions CreatedAt STRING Records ingested timestamp Sample Data Daily Promo Analytics Table Description : Promo analytics data for a particular day Table Name : hive_db_insights.daily_promo_analytics Table Type : External, Date Time partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/daily_promo_analytics column type comment PromoId BIGINT ID of the promo RedeemedOrders INT Redeemed orders RemainingRedemptions INT Remaining redemptions RedeemedAmount INT Total burn from all the redemptions Sales STRING Sum of the order values of all the orders that redeemed the promotion NewCustomers INT New customers date_time STRING Partition Column Sample Data Promo Analytics Table Description : Aggregated Promo analytics Table Name : hive_db_insights.promo_analytics Table Type : External, Date Time partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/promo_analytics column type comment PromoId BIGINT ID of the promo DatePeriod STRING “1|7” - Last 7 days(ongoing promos) “1|30” - Last 30 days(ongoing promos) “1|-1” - Start date till the current date (ongoing promos) & start date till the end/cancelled date(Past promos) TotalRedeemedOrders INT Redeemed orders TotalRemainingRedemptions INT Remaining redemptions TotalRedeemedAmount INT Total burn from all the redemptions TotalSales STRING Sum of the order values of all the orders that redeemed the promotion TotalNewCustomers INT New customers date_time STRING Partition Column Sample Data After these tables are populated, the pre bulk loader job generates data in text format to be uploaded to HBase table. Pre bulk loader : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/dp-merchant-hadoop/merchant-analytics/promo-analytics/txtfiles HBase Table : merchant_analytics HBase Column Family : analytics Sample pre bulk loader text Data Promo Analytics Job Recovery : Run the jobs from the beginning whenever the job fails from any step as all the steps are INSERT OVERWRITE queries. Hbase loader flow GitHub Repository : hbase-loader Azkaban project : hbase-loader Azkaban flow name : merchant_analytics_tb_analytics_cf_flow HBase Table : merchant_analytics HBase Column Family: analytics Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 06:45 AM IST Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Utility Projects and Flows 1.Previous data uploader flow Azkaban project : dp-merchant-hadoop Azkaban flow name : merchant_analytics_previous_loader_flow This flow is to populate past data for the sales analytics, customer analytics and item analytics before scheduling the analytics flows since all analytics need previous two months(60 days) data analytics. Data is populated for 60 days and to daily analytics tables of each analytics. 2.Hadoop UDF Project GitHub Repository : dp-hadoop-udf This utility project is to build Hive UDFs. For merchant analytics, “HBasePreBuilderUDAF” is used to generate hash keys for HBase and “SERIALIZE_JSON” is used for generating the JSON string for HBase. Following flow can be used to install the jar in HDFS. Azkaban project : dp-merchant-hadoop Azkaban flow name : install_hive_udf_jar_workflow Jar location : wasbs://configs@hdinprdshrprimpkme.blob.core.windows.net/dp-merchant/hive-packages/udf-jar",False
2357493815,page,current,/x/N4CEj,Search Enhancements,"1 7 Introduction This document describes the solution for enhancing the search capability by, Incorporating popularity into search results ranking Matching personalized tags with item and outlet tags to influence text matching score Ranking based on Popularity Item Popularity The popularity is determined by the following elements. Items sales over a period of time (period should be configurable) Item rating Outlet rating Outlet’s SKU variety Sections below outline the calculation approach for each of the above parameters Item Sales Item sales would be determined per menu item and also per a “time of the day bin”. This is required as different items could become popular at different times of the day. The sales figures would be determined based on the fact_food_order_items table This needs to be joined with the fact_food_oders table to determine the time bins. Item sales would be calculated in the following manner Normalization would be done based on the below formula. Item Rating and Outlet Rating These values would be calculated by accumulating the ratings across a given period (configurable) available in the following tables. hive_db_raw.food_menu_item_ratings hive_db_raw.food_place_ratings Outlet’s SKU variety SKU variety would be calculated by joining fact_food_order_items and fact_food_orders and counting the unique sku_ids per restaurant. The values would also be normalized. Final Item Popularity Calculation Final value for popularity for a time of the day bin would be calculated in the following manner. The final value would be a weighted average of the 4 elements where the weights would be configurable. Data Pipelines Following ETLs would be developed. These ETLs would run on a weekly basis and create a weekly partitioned table. Item Sales Calculator Item Rating Calculator Outlet Rating Calculator Item Variety Calculator Data Exporter Data Export The final data export would take the following format. The export would be made to a table specified by the delivery team. Elasticsearch API Calls The menu items ES index should be rebuilt including the above mentioned popularity fields. This would need to happen weekly in sync with the popularity scores update. The popularity could be incorporated to search by using the script_score option in ES. Consider the following example. In this example the text matching score returned by the ES is modified using a script passed at the search time. Following points to be noted, Search is made during break time hence break_popularity is considered. This should be dynamically changed depending on the time of the day. Text matching score is given 0.8 weight and popularity is given 0.2 weight There is no score range for the text matching score returned by _score. Therefore weighting to apply, this score should be normalized (i.e. between 0 and 1). For normalization the maximum score should be known (in this case 0.8275676). This can be obtained by sending a regular request but limiting result count to 1. Request Response (see max score) Note : This approach means that two ES searches should be performed for every request. Any performance impacts should be evaluated. Outlet Popularity The popularity is determined by the following elements. Outlet sales over a period of time (period should be configurable) Outlet rating Outlet’s SKU variety Sections below outline the calculation approach for each of the above parameters Outlet Sales Outlet sales would be determined by joining fact_food_order_items and fact_food_orders tables for a period of time (configurable) and aggregating sales counts per restaurant. Outlet Rating Values determined in the above mentioned workflows would be used. Outlet’s SKU variety Values determined in the above mentioned workflows would be used. Final Outlet Popularity Calculation Final outlet popularity would be determined based on a weighted average of Outlet Sales, Outlet Rating and Outlet’s SKU variety Data Pipelines In addition to the pipelines discussed above to calculate Outlet Rating and Item Variety, following pipelines would be added to calculate Outlet sales and Data Export. These pipelines would also be run on a weekly basis. Outlet Sales Calculator Data Exporter Data Export The final data export would take the following format. The export would be made to a table specified by the delivery team. Elasticsearch API Calls The outlets ES index should be rebuilt including the above mentioned popularity fields. This would need to happen weekly in sync with the popularity scores update. The outlet popularity could be incorporated to search by using the script_score option in ES. Consider the following example. Here a weighted score is applied between the text match score (0.8) and the outlet popularity (0.2) Personalization based on Tags Objective here is to maintain a list of tags per user and apply these tags during the search process in a way that influences scoring. Tags for the user would be derived based on the purchase history of the user in a given period of time (configurable). Also item tags and outlet tags would be maintained separately. Item tags per user Determining Tags The item tags are already available in the data lake as a daily partitioned table. See below structure for hive_db_raw.food_tag_item The tag information such as the tag names and the tag type mapping is available on the taxonomy_tags table , the tag type information will be available in tag_types table, the order sku to tag mapping from sku_taxonomy_tags table in Foodie database will be used in mapping the passenger to their item tags. The schemas of the above tables are available in Taxonomy Tags - Technical Document . Currently, the above mentioned three tables are not ingested into hive_db_raw . A dim table should be developed to maintain the current snapshot of the item to tag mapping along with the tag name. The table would contain, item_id tag_id tag_name Note: if a table exists foodie db where this information is available, it would make sense to ingest the complete table on a weekly basis. Above table would be used to determine the items tags related to users. Following approach would be used Data Pipelines Following data pipelines would be developed ETL ingestion of taxononmy_tags, sku_taxonomy_tags, tag_types to hive_db_raw . Flow to create a dim table food_tag_items Flow that would determine the top 5 tags per user Data export that pushes out per user tag information (both item and outlet tags) to a table specified by the delivery team Data Export The data exported to the delivery team database would take the following format. Elasticsearch API Calls The item and outlet tags should be built into the elastic search in the form of arrays. At the query time the tags can be matched with a boolean should match in the form below. Tags passed in the terms field are based on the users tags. Outlet Tags per user Determining Tags The outlet tags are already available in the data lake as a daily partitioned table. See below structure for hive_db_raw.food_tag_place The tag information such as the tag names and the tag type mapping is available on the taxonomy_tags table , the tag type information will be available in tag_types table, the place to tag mapping from place_taxonomy_tags table in Foodie database will be used in mapping the passenger to their item tags. The schemas of the above tables are available in Taxonomy Tags - Technical Document . Currently, the above mentioned three tables are not ingested into hive_db_raw . A dim table should be developed to maintain the current snapshot of the outlet to tag mapping along with the tag name. The table would contain, place_id tag_id tag_name Note: if a table exists foodie db where this information is available, it would make sense to ingest the complete table on a weekly basis. Above table would be used to determine the outlet tags related to users. Following approach would be used Data Pipelines Following data pipelines would be developed ETL ingestion of place_taxonomy_tags to hive_db_raw is only needed since taxononmy_tags, sku_taxonomy_tags, tag_types are already ingested for determining item tags per user . Flow to create a dim table food_tag_place Flow that would determine the top 5 tags per user Data export - can be done from the same job described in the item tags section Data Export The above export described in item tags section covers this. It would be a single export covering item and outlet tags. Elasticsearch API Calls Same approach as described in item tags section. Questions We need to understand exactly the tables from which we can pull the menu_item and outlet tags please provide this information. Right now we are considering food_tag_items, food_tag_place, food_tags from foodie db We need to understand exactly the tables to which we should be exporting data to. please provide this information. Given that all tags would carry the same weight, we have decided to limit the customer tag list to the top 5. The top 5 would be determined based on the number of occurrences in the items and outlets related to the orders. Please confirm this approach. In the tags matching in elasticsearch both product text matching and the keyword matching would carry the same weight. Do we need to give more weight to one category? In the popularity script_score method we need to do two API calls, first one a light weight call to understand the max score. Please evaluate the performance impact on this. For the outlet sales calculation, We currently planed to take the count of orders the item included. So In an order, if purchase more than 1 quantity of an item, our method consider it as 1. Main intuition behind the above decision is, Some items are normally ordered as bulks ( Parata, Ice-cream cones ) and avoid those items to gain higher popularity score from lower orders. What is your opinion about this ?",False
2358476844,page,current,/x/LICTj,June Release 2021,,False
2359164935,page,current,/x/BwCej,October Release 2021,,False
2359361542,page,current,/x/BgChj,August Release 2021,,False
2360901647,page,current,/x/D4C4j,Location based filter for recommended drop locations,"Background Knowledge Currently there is a API to get the recommended pickup and drop locations when a passenger hailing a vehicle in PickMe passenger App. There are two types of recommendations for drop location. Associated Drop Locations To find the associated drop locations use the following three factors. Cell id of the location request generated - consider all the trip picked up from the parent cell of given location Passenger Id Time bin of the day day of the week 2. Generic Drop Location In this category, API recommend all the drop locations associated to that driver regardless of the pickup location, date or time the request coming. Overview The purpose of this drop location recommendation is to improve the user experience. When recommending the drop locations some times ( specially in Generic type of recommendation ) more near places can be recommended as drop location. This task is carried out to filter the drop location in recommended within given range. Plan of implementation Calculating the distance between two locations We can use haversine method to find the distance between two location in KMs given its latitude and longitudes. ( https://www.researchgate.net/post/What-is-the-best-formula-in-finding-the-distances-between-one-coordinate-in-earth-to-another-given-the-longitude-and-latitude ) SuccessPayload ( This is a java object which include all info included in response JSON )object generated from current version of API will be processed to find the distance between location request coming from and all the drop location recommended. While finding the distance remove the locations which produce the distance less than the limit given. Structure of the Response JSON included in the API document. ( Recommended Locations API )",False
2397110376,page,current,/x/aADhjg,Food Recommendations,"1 7 Overview Food recommendations are provided for customers based on items and restaurants. Food recommendations provided from the data platform can be segmented as below. Main type Recommendation type Recommendation Restaurants Personalized *Previously ordered restaurants *Recommended for you *Top Picks Trending Newly joined restaurants Popular restaurants Items Frequent Pattern Frequently bought together Trending *Best seller items Popular picks (per each merchant) Personalized Picked up for you Note : The ETL code base for marked recommendations is in the following repos. The ETL codebase of the rest of the recommendations are expected to be transferred to the following repos also. GitHub Repository for ETL : dp-food-recommendations dp-food-recommendations-spark-jobs GitHub Repository for HBase : hbase-loader GitHub Repository for API : mserv-food-recs API Doc : Food Recommendations API Personalized recommendations The data for personalized recommendations are stored in Hive and HBase as follows. Hive Table Description : Personalized recommendations Table Name : hive_db_insights.personalized_recommendations Table Type : External, partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/hive_db_insights/stage/hive/recommendations/personalized column type comment key STRING Key of the recommendations value STRING Value of the recommendations timestamp TIMESTAMP Recommendation data ingested timestamp rec_type STRING Partition Column Sample Data key value timestamp rec_type 1613 RICE_&_CURRY1:1 2021-07-23 12:42:01 1003 1661 BURGERS_&_WRAPS1:1 2021-07-23 12:42:01 1003 5959 4,048,738,783,418,080 2021-07-30 12:59:03 1008 HBase Table Name : personalized_recommendations Column family : rec_type Azkaban project name : hbase-loader Bulk loader Azkaban flow name : personalized_recommendations_tb_rec_type_cf_flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 01:30 PM IST Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Previously ordered restaurants SRS : Past Restaurants as a Slider - Phase 1 GitHub Repository for ETL : food-recommendations-algos Azkaban project name : food-recommendation-algos ETL Azkaban flow name : previously_ordered_restaurants Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 00:45 AM IST ETL Job : ETL job processes orders of each customer last 90 days and attempts to identify recently ordered restaurants. Table Name : hive_db_insights.food_customer_recs_restaurants Table Type : External, partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/hive_db_insights/stage/hive/recommendations/trending column type comment passenger STRING Customer id res_ids STRING Restaurants in a orting order rec_type STRING Partition Column Sample Data passenger res_ids rec_type 1598 41566 1000 3052 4,171,740,483 1000 6087 39394 1000 Job Recovery : Run the jobs from the beginning whenever the job fails from any step. GitHub Repository for HBase : hbase-bulk-loader Azkaban project name : hbase-bulk-loader ETL Azkaban flow name : workflow_foodCustomerRecsRestaurants Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 01:30 AM IST Table Name : food_customer_recs_restaurants Column family : rec_type Qualifier name : 1000 Bulk loader Job : This job reads recommendations from the hive table and generates the hfiles to be uploaded in HBase table. Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Note : The refactored code for the previously ordered restaurants is available in dp-food-recommendations repo but it is not yet scheduled. Recommended for you Model details : ""Recommended for you"" personalized recommendations LightFM vs ALS testing SRS : SRS - Food/Market - Recommended Outlets Slider Azkaban project name : dp-food-recommendations ETL Azkaban flow name : recommended-for-you-flow Flow Execution Schedule : Frequency of execution: Daily Execution time slot: 12:50 PM IST ETL Job : Interaction matrix is generated from customer and restaurant propensities after normalizing propensity values and removing data with zero for sum of propensity value. 2. Generate predictions(top 20) using ALS algorithm Average order value bin wise. Sample Data key value timestamp rec_type 5959 4,048,738,783,418,080 2021-07-30 12:59:03 1008 11878 4,179,437,652,416,640 2021-07-30 12:59:03 1008 16967 4,073,041,808,416,640 2021-07-30 12:59:03 1008 After the table is populated, the pre bulk loader job generates data in text format to be uploaded to HBase table. Pre bulk loader : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net//hive_db_insights/stage/hive/recommendations/personalized/recommendedForYou HBase Table : personalized_recommendations HBase Column Family : rec_type HBase qualifier : 1008 Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Top Picks SRS : SRS - Food/Market - Top Picks Sliders Azkaban project name : dp-food-recommendations ETL Azkaban flow name : top-picks-flow Flow Execution Schedule : Frequency of execution: Daily Execution time slot: 12:40 PM IST ETL Job : Compute top pick categories (5) for each customer using customer propensities. The tags in the intersection are the ones that will be used for the top category sliders For the initial phase Burgers & Wraps category will contain = Burgers + Wraps + Submarines + Sandwiches Sample Data key value timestamp rec_type 59519 RICE_&_CURRY1:1 2021-08-01 12:41:51 1003 59519 BURGERS_&_WRAPS1:1 2021-08-01 12:41:51 1004 59519 SEAFOOD1:1 2021-08-01 12:41:51 1005 59519 KOTTU1:1 2021-08-01 12:41:51 1006 59519 INDIAN1:1 2021-08-01 12:41:51 1007 After the table is populated, the pre bulk loader job generates data in text format to be uploaded to HBase table. Also the restaurants related to each category are found based on hex id location and that data is also generated in text format to be uploaded to HBase table. Pre bulk loader Top Picks : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net//hive_db_insights/stage/hive/recommendations/personalized/topPicks HBase Table : personalized_recommendations HBase Column Family : rec_type HBase qualifier : 1003, 1004, 1005, 1006, 1007 Pre bulk loader restaurants : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net//hive_db_insights/stage/hive/recommendations/categories HBase Table : recommendations_utilities HBase Column Family : utilities HBase qualifier : 3000 Bulk loader Azkaban flow name : recommendations_utilities_tb_utilities_cf_flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 12:55 PM IST Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Picked up for you SRS : SRS -[Delivery]- Menu Recommendations Azkaban project name : dp-food-recommendations-test ETL Azkaban flow name : recommended-for-you-items-flow Flow Execution Schedule : Frequency of execution: Daily Execution time slot: 4:30 AM IST ETL Job : Interaction matrix is generated from customer and item propensities after normalizing propensity values and removing data with zero for sum of propensity value for each restaurant Sample Data id recommendations 964 38655:[236400,236576,235695,235623,235665]n41237:[177848,174748,175623,174542,178068]n41707:[195621,196244,196031,196370,196195]n37911:[3242663,3242676,3242660,3242685,3242674]n41540:[191610,190732,190840,190318,191210] In the above sample data, id is the passenger id and in the recommendations column, following structure is used “restaurant_1 : [skuid_1, skuid_2,…] n restaurant_2 : [skuid_1, skuid_2,…]n…” Output is simply put, a list of sku ids for each restaurant for each passenger. After the table (hive_db_tmp.pickup_for_you_recommendations) is populated, we directly export the data into foodie_db by use of sqoop exporter. Trending recommendations The data for trending recommendations are stored in Hive and HBase as follows. Hive Table Description : Trending recommendations Table Name : hive_db_insights.trending_recommendations Table Type : External, partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/hive_db_insights/stage/hive/recommendations/trending column type comment key STRING Key of the recommendations value STRING Value of the recommendations timestamp STRING Recommendation data ingested timestamp rec_type STRING Partition Column Sample Data key value timestamp rec_type 88611cb331fffff 40426:1,39587:44 2021-08-02 13:19:27 1001 88611cb357fffff 39204:2,38202:91,40252:107 2021-08-02 13:19:27 1001 88611cb11bfffff 40410:3,41424:11,40675:145,41096:224 2021-08-02 13:19:27 1001 HBase Table Name : trending_recommendations Column family : rec_type Newly Joined restaurants SRS : https://pickme.atlassian.net/wiki/spaces/BC/pages/1139146868/SRS+-+Food+Market+-+Slider+Display+and+Show+All+Option?preview=%2F1139146868%2F1141997607%2FNewly%20Joined%20Slider.png GitHub Repository for ETL : dp-food-recommendations-flow Azkaban project name : food-recommendation-algos-v2 ETL Azkaban flow name : newly_joined_merchants-workflow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 01:00 PM IST ETL Job : ETL Job tries to find the new merchants joined in the last 60 days in an sorting order. Then the result will be stored as a text file which will be used by the API. Output file : wasbs://food-recs@hdinprdshrprimpkme.blob.core.windows.net//stage/hive/trending/newly_joined_merchants/output/output.txt Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Popular restaurants Logic Details : Popular Restaurant - phase 1 GitHub Repository for ETL Workflow : dp-food-recommendations-flow GitHub Repository for ETL : dp-food-recommendations-spark-jobs Azkaban project name : food-recommendation-algos-v2 ETL Azkaban flow name : popular_merchants_bySales-workflow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 01:10 PM IST ETL Job : Retrieve past 30 days orders for each merchant Filter out the merchants which have working days more than 20 days Filter out merchants which have sales more than the average sales - popular merchants Rank each merchant according to sales in descending order Group popular merchants using cell id. eg;- cell_id | merchant1:rank,merchant2:rank,... | rec_type Sample Data key value timestamp rec_type 88611cb331fffff 40426:1,39587:44 2021-08-02 13:19:27 1001 88611cb357fffff 39204:2,38202:91,40252:107 2021-08-02 13:19:27 1001 88611cb11bfffff 40410:3,41424:11,40675:145,41096:224 2021-08-02 13:19:27 1001 Job Recovery : Run the jobs from the beginning whenever the job fails from any step. GitHub Repository for HBase : hbase-food-recommendations-bulk-loader Azkaban project name : hbase-bulk-loader-food-recommendations ETL Azkaban flow name : popular_merchants_bySales--bulkloader-workflow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 01:30 PM IST Table Name : trending_recommendations Column family : rec_type Qualifier name : 1001 Bulk loader Job : This job reads recommendations from the hive table and generates the hfiles to be uploaded in HBase table. Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Best seller items SRS : SRS - Food/Market - Best Selling Items Indication Azkaban project name : dp-food-recommendations ETL Azkaban flow name : best-seller-items-flow Flow Execution Schedule : Frequency of execution: Daily Execution time slot: 08:00 AM IST ETL Job : Read past 2 weeks order items data The best selling items for each merchant should be determined as follows: The total quantity of an item sold during the last 7 days. E.g. Last 7 days→ 550 The total quantity of an item sold during the previous 7-14 days. E.g. Last 7-14 days→ 400 The quantity difference between the two duration. E.g. + 150 The percentage difference between the quantities sold in both duration. E.g. 37.5% ↑ Calculated as follows Item sold % = [(Qty sold for last 7D - Qty sold for last 7to14D)/ Qty Sold for last 7to14D] *100% Items with the highest ""Item sold %"" to be considered as the ""Bestsellers"" Only positive percentages to be considered when defining Bestsellers If more than one item has the same positive percentage, the ordering should be refined based on which item has contributed the highest to the total revenue. Total revenue = Sum of (Item Price*Qty) Top 5 bestsellers are chosen. The processed data is written to hive_db_insights.trending_recommendations and hive_db_tmp.best_seller_items tables. Sample Data key value timestamp rec_type 38,219 895,349,896,317,895 2021-08-05 13:34:16 2001 38,225 229,922,298,723,002 2021-08-05 13:34:16 2001 38,370 86,439,635,838,358,500 2021-08-05 13:34:16 2001 38,708 635,483,100,716,635 2021-08-05 13:34:16 2001 38737 204,052 2021-08-05 13:34:16 2001 Table Name : hive_db_tmp.best_seller_items Table Type : External, Non-partitioned Table Location : wasbs://hive-db-tmp@ hdinprdshrprimpkme.blob.core.windows.net/ hive_db_tmp/stage/hive/best_seller_items place_id menu_ids created_at 38,153 874,634,874,635 2021-08-05 8:05:51 38,422 5,098,950,983 2021-08-05 8:05:51 38,868 137,845,137,838,137 2021-08-05 8:05:51 40,383 612,490,612,501,615 2021-08-05 8:05:51 After the tables are populated, sqoop loader flow is triggered to export data to GCP SQL table. Database Name : Foodie Table Name : best_seller_items Trending Slider SRS : https://pickme.atlassian.net/wiki/spaces/BC/pages/1139146868/SRS+-+Food+Market+-+Slider+Display+and+Show+All+Option?preview=%2F1139146868%2F1141997607%2FNewly%20Joined%20Slider.png GitHub Repository for ETL : dp-food-recommendations Azkaban project name : dp-food-recommendations ETL Azkaban flow name : trending-flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 06:00 AM IST ETL Job : ETL Job tries to find the trending merchant by percentage difference between the sales count of previous 1-15 days and 16-30 days. Output file : gs://hive-db-insights/stage/hive/recommendations/trending/trending Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Popular Picks SRS : Azkaban project name : dp-food-recommendations-test-place-popular-picks ETL Azkaban flow name : place-popular-picks-flow Flow Execution Schedule : Frequency of execution: Daily Execution time slot: 2:00 AM IST ETL Job : Calculate the sum of sales for each sku ID for each restaurant and order them in descending order. Then take the required number of sku IDs from the dataset. Sample Data place_id popular_picks 476 210741,209710,209902,210022,211164 3151 3874869,3874858,3874945,3874873,3874929 11717 2747880,2747883,207893,4145476,2747884 16383 792667,789981,789910,790050,792809 23038 3874639,3874646,3874650,3874710,3874654 After the table (hive_db_tmp.place_popular_picks ) is populated, we directly export the data into foodie_db by use of sqoop exporter. Frequent Pattern recommendations The data for frequent pattern recommendations are stored in Hive and HBase as follows. Hive Table Description : Frequent Pattern recommendations Table Name : hive_db_insights.frequent_pattern_recommendations Table Type : External, partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/hive_db_insights/stage/hive/recommendations/frequent_pattern column type comment key STRING Key of the recommendations value STRING Value of the recommendations timestamp STRING Recommendation data ingested timestamp merchant_id STRING Merchant id related to recommendation rec_type STRING Partition Column Sample Data key value timestamp rec_type merchant_id 239455 9232:1,9539:2,9538:3 2020-08-20 4:11:41 2000 10025 92274 9261:1,9458:2,9256:3,9308:4,9307:5 2020-08-20 4:11:41 2000 10025 9300 9261:1,9256:2 2020-08-20 4:11:41 2000 10025 HBase Table Name : frequent_pattern_recommendations Column family : rec_type Frequently bought together SRS : SRS - Food - Item Recommendation with Cart Revamp Model Details : Frequently Bought Together GitHub Repository for ETL Workflow : dp-food-recommendations-flow GitHub Repository for ETL : dp-food-recommendations-spark-jobs Azkaban project name : food-recommendation-algos-v2 ETL Azkaban flow name : frequently_brought_together-workflow Flow Execution Schedule : Frequency of execution: Weekly Execution timeslot: Thursday 02:30 PM IST ETL Job : Read past 30 days order items for each merchant filtering out duplicate items Loop through each merchant to generate frequently bought together recommendations If order count for a certain merchant is below (30), that merchant is skipped. Run FP-Growth algo for each merchant If merchant is MARKET_PLACE => minimum support - 0.05 Else minimum support - 0.001 minConfidence - 0.01 minLift - 1 3. Combine the associations as ""ItemId1:rank,ItemId2:rank,.."" by ranking over lift Sample Data key value timestamp rec_type merchant_id 239455 9232:1,9539:2,9538:3 2020-08-20 4:11:41 2000 10025 92274,1133 9261:1,9458:2,9256:3,9308:4,9307:5 2020-08-20 4:11:41 2000 10025 9300,1334,6433 9261:1,9256:2 2020-08-20 4:11:41 2000 10025 Job Recovery : Run the jobs from the beginning whenever the job fails from any step. GitHub Repository for HBase : hbase-food-recommendations-bulk-loader Azkaban project name : hbase-bulk-loader-food-recommendations ETL Azkaban flow name : frequently_brought_together-bulkloader-workflow Flow Execution Schedule : Frequency of execution: Weekly Execution timeslot: Thursday 11:00 PM IST Table Name : frequent_pattern_recommendations Column family : rec_type Qualifier name : 2000 Bulk loader Job : This job reads recommendations from the hive table and generates the hfiles to be uploaded in HBase table. Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Utility Flows Restaurant Master data GitHub Repository for HBase : hbase-bulk-loader Azkaban project name : hbase-bulk-loader ETL Azkaban flow name : workflow_restaurantMDM Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 01:15 AM IST ETL Job : Provide latitude, longitude, passenger listing radius for each restaurant Table Name : restaurant_master_data Column family : demographics Qualifier names : latitude, longitude, radius Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Recommendation utilities GitHub Repository for HBase : hbase-loader Azkaban project name : hbase-loader ETL Azkaban flow name : recommendations_utilities_tb_utilities_cf_flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 12:55 PM IST ETL Job : Prepare merchants tagged for each category cell wise(resolution = 5) Table Name : recommendations_utilities Column family : utilities Qualifier name : 3000 Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Tagger GitHub Repository for ETL : dp-food-recommendations Azkaban project name : dp-food-recommendations ETL Azkaban flow name : daily-tagger-flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 12:10 PM IST ETL Job : Read food order items data and tagged data (FOOD_DELIVERY orders only) Apply initial filters Remove orders with missing order values Remove missing item names or basic type or region Remove items with item id = 0 Remove test merchants Apply region filters Take Only Main Regions and Apply NULL for Other Regions Replace South and North with Indian, Mainland and SriLankan Chinese with Chinese Drop Duplicates in ItemName Build a descriptive Title Fill NULL with Empty String and Concat ItemName, Region and BasicType to Generate Title Remove Duplicate words in the Title Clean the title Remove Punctuations ToLowerCase Remove Numbers Remove both leading and trailing white spaces Remove two letter words Remove Stop Words Remove Custom Stop Words - ""Mr.Burger"" Remove Duplicate Titles, Titles contain Promo Words, + Marks Add Basic and Sub Category labels Table Name : hive_db_insights.dim_menu_items Table Type : External, Non-partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/hive_db_insights/stage/hive/dim_menu_items column type comment itemid INT Menu item ID cleaned_title STRING cleaned title for tagging itemname STRING Menu item label category1 STRING item belonged region category2 STRING item belonged category rice_curry DOUBLE category label pizza_pasta DOUBLE category label burger DOUBLE category label wrap DOUBLE category label submarine DOUBLE category label biriyani DOUBLE category label kottu DOUBLE category label desserts DOUBLE category label beverages DOUBLE category label noodles DOUBLE category label chinese DOUBLE category label indian DOUBLE category label japanese DOUBLE category label thai DOUBLE category label italian DOUBLE category label vegetarian DOUBLE category label salad DOUBLE category label seafood DOUBLE category label chicken DOUBLE category label pork DOUBLE category label beef DOUBLE category label mutton DOUBLE category label egg DOUBLE category label cheese DOUBLE category label spicy DOUBLE category label fried DOUBLE category label bbq DOUBLE category label grilled DOUBLE category label breakfast DOUBLE category label bread DOUBLE category label sandwich DOUBLE category label bakery DOUBLE category label fast_food DOUBLE category label bite DOUBLE category label soup DOUBLE category label curry DOUBLE category label condiments DOUBLE category label timestamp STRING data ingested timestamp Sample Data itemid cleaned_title itemname category1 category2 rice_curry pizza_pasta desserts indian 4140 fruit indian musket sweets Fruit Musket Indian Indian Sweets 1 1 4,149 indian musket nut sweets Nut Musket Indian Indian Sweets 1 1 4,152 boondhi indian special sweets Special Boondhi Indian Indian Sweets 1 Job Recovery : Run the jobs from the beginning whenever the job fails from any step. Propensities GitHub Repository for ETL : dp-food-recommendations Azkaban project name : dp-food-recommendations ETL Azkaban flow name : propensity-flow Flow Execution Schedule : Frequency of execution: Daily Execution timeslot: 12:20 PM IST ETL Job : Read food order items data and tagged data (FOOD_DELIVERY and completed orders only) Set order hour bins (Night_snack_bin, breakfast_bin, lunch_bin, afternoon_snack_bin, dinner_bin) Compute customer propensities Compute average order value Find AOV bin based on the percentiles of AOV (0.3, 0.5, 0.8) Compute total orders Compute total order value Compute total days ordered Compute total unique restaurants ordered Compute propensities by (orders for a category / total orders) Compute restaurant propensities Compute propensities by (orders for a category / total orders) Table Name : hive_db_insights.customer_propensity Table Type : External, Non-partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/hive_db_insights/stage/hive/propensities/customer_propensity column type comment passengerid BIG INT Passenger ID total_orders INT total number of orders total_order_value DOUBLE total value of all orders total_days_ordered INT number of orders placed days total_uniquerest_ordered INT number of unique rest orders aov DOUBLE average order value aov_bin STRING AOV Bin night_snack_bin INT order hour bin - night_snack breakfast_bin INT order hour bin - breakfast lunch_bin INT order hour bin - lunch afternoon_snack_bin INT order hour bin - afternoon_snack dinner_bin INT order hour bin - dinner rice_curry DOUBLE propensity value pizza_pasta DOUBLE propensity value burger DOUBLE propensity value wrap DOUBLE propensity value submarine DOUBLE propensity value biriyani DOUBLE propensity value kottu DOUBLE propensity value desserts DOUBLE propensity value beverages DOUBLE propensity value noodles DOUBLE propensity value chinese DOUBLE propensity value indian DOUBLE propensity value japanese DOUBLE propensity value thai DOUBLE propensity value italian DOUBLE propensity value vegetarian DOUBLE propensity value salad DOUBLE propensity value seafood DOUBLE propensity value chicken DOUBLE propensity value pork DOUBLE propensity value beef DOUBLE propensity value mutton DOUBLE propensity value egg DOUBLE propensity value cheese DOUBLE propensity value spicy DOUBLE propensity value fried DOUBLE propensity value bbq DOUBLE propensity value grilled DOUBLE propensity value breakfast DOUBLE propensity value bread DOUBLE propensity value sandwich DOUBLE propensity value bakery DOUBLE propensity value fast_food DOUBLE propensity value bite DOUBLE propensity value soup DOUBLE propensity value curry DOUBLE propensity value condiments DOUBLE propensity value timestamp STRING data ingested timestamp Sample Data passengerid total_orders total_order_value total_days_ordered total_uniquerest_ordered aov aov_bin night_snack_bin breakfast_bin lunch_bin afternoon_snack_bin dinner_bin rice_curry pizza_pasta burger wrap 857 1 320 1 1 320 low 1 0 0 0 0 1,533 15 15971.85 13 11 1064.79 high_mid 0 0 0 0 0 0.133 2,971 2 1778 2 2 889 high_mid 0 0 0 0 1 0.5 3087 4 9285 4 3 2321.25 high 0 0 0 0 0 0.5 0.25 Table Name : hive_db_insights.restaurant_propensity Table Type : External, Non-partitioned Table Location : wasbs://hive-db-insights@hdinprdshrprimpkme.blob.core.windows.net/hive_db_insights/stage/hive/propensities/restaurant_propensity column type comment resturantid BIG INT Restaurant ID rice_curry DOUBLE propensity value pizza_pasta DOUBLE propensity value burger DOUBLE propensity value wrap DOUBLE propensity value submarine DOUBLE propensity value biriyani DOUBLE propensity value kottu DOUBLE propensity value desserts DOUBLE propensity value beverages DOUBLE propensity value noodles DOUBLE propensity value chinese DOUBLE propensity value indian DOUBLE propensity value japanese DOUBLE propensity value thai DOUBLE propensity value italian DOUBLE propensity value vegetarian DOUBLE propensity value salad DOUBLE propensity value seafood DOUBLE propensity value chicken DOUBLE propensity value pork DOUBLE propensity value beef DOUBLE propensity value mutton DOUBLE propensity value egg DOUBLE propensity value cheese DOUBLE propensity value spicy DOUBLE propensity value fried DOUBLE propensity value bbq DOUBLE propensity value grilled DOUBLE propensity value breakfast DOUBLE propensity value bread DOUBLE propensity value sandwich DOUBLE propensity value bakery DOUBLE propensity value fast_food DOUBLE propensity value bite DOUBLE propensity value soup DOUBLE propensity value curry DOUBLE propensity value condiments DOUBLE propensity value timestamp STRING data ingested timestamp Sample Data resturantid rice_curry pizza_pasta burger wrap submarine biriyani kottu desserts beverages noodles 37,850 0 0.0330920 0.012409 0.009307 0.27507 0.146845 0.00413 0.01240 0.033 38,313 1 0.029289 0.170414 0.00621 0.05266 0.0582 38,403 1 0.175384 0.4523 0.058 Job Recovery : Run the jobs from the beginning whenever the job fails from any step.",False
2398519322,page,current,/x/GoD2jg,Generic KPI Calculation Engine,,False
2398978131,page,current,/x/U4D9jg,Generic Cohort Engine,,False
2399535138,page,current,/x/IgAGjw,"Generalized framework for KPIs, Cohorts and Incentives",,False
2399895562,page,current,/x/CoALjw,Generic Incentive Engine,,False
2408054906,page,current,/x/egCIjw,Prep-time prediction model,"The prep-time model is a part of the ETA progress screen model which is been used to predict the food preparation time. Food preparation time is an important phase in predicting the ETA as it is one of the critical parts of the food delivery process. This information is necessary to inform the customer upfront so they know the time they need to wait for the order and also to dispatch the driver-partner to pick up the food. Inaccurate predictions can lead to customer dissatisfaction due to late deliveries and either can make the driver-partner arrive early. Else, the wrong predictions will make the driver-partner late so the food will get cold. The food preparation time varies due to various factors which are both controllable and uncontrollable. However, the delivery platform can depend only on the historical data about the food preparation behaviour due to the lack of data. Hence, this prep-time prediction model will solely depend on historical data and some near real-time features. The model building procedure is as follows. Data cleaning and preparation Train and test dataset - 11th June 2021: 11th July 2021 Validation dataset - 12th July 2021: 25 July 2021 Prep-time is the food/order preparation time taken by food or marketplace merchants and this time is measured between the merchant accepted (status 12) and order ready (status 17/18). The initial data preparation steps taken are as follows. Raw data was obtained from the fact_unique_food_orders, fact_rides, dim_resturants, fact_food_orders_log, driver_hex_level_supply, fact_food_order_items tables Selected completed food orders. Some orders have a completed status on the food_order_log but subsequently, they have been cancelled. Therefore, we have only considered the food orders which are in completed status on fact_unique_food_orders There are orders with jobs completed, but haven't been delivered - those have to be removed (no status 8) All the time features consider the merchant confirmed time Remove test merchants Since the model considers either the prep time expired status or order ready status, we have imputed any orders without the order ready status with the prep time expired timestamp. If there are multiple statuses for order ready time, consider the latest entry and discard other records. Dropped orders with missing item count Note: Outliers were removed considering the prep time feature before training the models. The upper fence value of the prep-time in the above-considered data set is 33 minutes. Thus, the model is trained considering the prep times below that value. The distribution of the response variable is as follows. Feature engineering The features used for the model are as follows. feature Realtime Location Date time Service group order item quantity Near realtime Accepted orders in the last 2 hours Average preptime before 1 hour number of orders placed before 15 minutes Historical Merchant density Average order volume Average order cancellations by merchant 90 day average prep time by merchant 7 day average prep time by merchant Model building An XGBoost model was trained considering the above feature set. The performance of the models was measured considering the RMSE values and the obtained test RMSE was 5.77 minutes . The feature importance is as follows.",False
2412478522,page,current,/x/OoDLjw,"One-Day Incentives Tri-lingual Naming and Start-Time, End-Time Config Options for the Driver-Portal","Overview The purpose of this document is to propose the technical solution for the above-titled requirement. With this requirement, the following projects need to be updated to capture the Tri-lingual names/Incentives start-time and end-time from the Driver-Portal for One-Day Incentives. Incentives Information API (mserve-incentive-portal-backend) Incentives Information (dp-incentives-information) Sqoop Loader (sqoop-loader) The following changes need to be done in each project. Incentives Information API (mserve-incentive-portal-backend) Three request body parameters will be added to JSON schema to capture the Incentives start-time,end-time and Tri-lingual Names. Tri-Lingual Names will include as a MetaData for the incentive and stored in a new table called “incentives_metadata“. Incentives start-time,end-time: Format (yyyy-MM-dd HH:mm:ss) ""startTime"": ""1970-01-01 08:30:00"", ""endTime"": ""1970-01-01 14:30:00"" *When processing the incentives date part will be ignored and consider as the current date. Incentives meta-data ""incentiveMetadata"": [ { metaKey:""sinhalaTitle"", metaValue:""අවුරුදු විශේෂ සහන"" }, { metaKey:""tamilTitle"", metaValue:""புத்தாண்டு சிறப்பு ஊக்கத்தொகை"" } ] Metadata Table Structure incentive_id key value 12558 sinhalaTitle “අවුරුදු විශේෂ සහන"" 12558 tamilTitle “புத்தாண்டு சிறப்பு ஊக்கத்தொகை"" Incentives Information (dp-incentives-information) *Pending Sqoop Loader (sqoop-loader) *Pending For Reference The following table describes the current incentives sqoop flows. Incentives Rules (API) Incentives Informations (Intermediate Table) Sqoop Import (SQL > HIVE) incentive incentive_strategy incentive_strategy_target ETL (Driver calculation for each incentive) Sqoop Export (Hive > SQL) incentive Create Incentives Intemediate Table Sqoop Export (Hive > SQL) hive_db_stage.incentives_information > driver_db.driver_incentives",False
2421653548,page,current,/x/LIBXk,Outlier model for prep time prediction,"The prep time model was built in order to predict the food preparation time for both food and marketplace orders. The initial model had an RMSE of 5.77 minutes when there are no outliers. Why do we need an outlier model? The prep time model was initially trained under certain limitations. An outlier removal step was carried out to improve the model’s prediction accuracy in the data preprocessing and cleaning phase. Thus, the orders which exceed the limitations have a lower prediction accuracy. Since the model has been trained on orders below a certain threshold of prep time, the outlier orders always have a lower predicted prep time than the actual prep time . However, in the real-time model evaluation phase, almost all the orders get predictions through this one model. The batch and real-time features that depend on historical values are also calculated without any limitations (without removing the outliers). Hence, predictions get highly affected by the outliers. Evaluating the default model performance - with outliers Initially, the model was trained after removing the outlier orders. However, all the orders including outliers have to be predicted considering the same model. Thus, the model performance for both regular orders and outlier orders were measured as follows. Validation RMSE - without outliers Validation RMSE - all data Prep time model 5.77 11.05 According to the above results, the model has a high RMSE when there are outlier orders in the validation set. Thus, the model is underperforming for outlier orders. The below tables represents the loss of data after the outlier removal process. Nearly 25% of the marketplace orders have been removed from the dataset as outliers. The data loss for the food place related orders is only 1.46%. The RMSEs for the prep time model for marketplace orders is 25.50 minutes whereas the RMSE for the food place orders is 5.96 minutes. Thus, the model has less accuracy when predicting the prep time for marketplace orders as well. Model Upper fence RMSE food orders RMSE marketplace orders Prep time 33.0 5.96 25.50 Outlier model for prep time To handle the outlier orders, an outlier model was trained considering the following procedure. The outlier model was initially trained considering the similar set of features used for the default prep time model. The model parameters were also kept unchanged. Data for the outlier model was selected considering the percentage of outliers orders in each merchant Merchants with more than a particular percentage of outlier records during the considered period were selected as Outlier merchants The remaining merchants will be modelled separately. The optimal percentage of outliers to be considered when selecting the outlier merchant was decided by considering different splits. The following table denotes the RMSEs for different splits done considering the percentage of outliers in each merchant. 10% 15% 25% 30% 40% Number of outlier merchants 122 104 79 72 55 Number of outlier model records 56078 45567 39392 33237 28070 Outlier model test RMSE 22.31 18.96 20.05 21.54 23.53 Without outlier model test RMSE 5.61 7.04 7.05 7.07 7.19 ** Thus the merchants with more than 15% of outliers will be considered as outlier merchants Evaluation of the outlier model The outlier model was evaluated considering the dataset from 12th July - 19th July 2021. For outlier merchants only Model Default model Outlier model Val RMSE 41.61 19.71 Error distribution Mean 28.94 18.62 SD 29.90 15.37 Min 0.01 0.00 25% 7.68 5.38 50% 15.76 10.51 75% 41.23 19.22 80% 49.93 22.45 90% 77.85 32.99 95% 98.29 41.75 98% 105.99 52.94 Max 152.59 138.58 2. For without outlier merchants Model Default model Without outlier model Val RMSE 6.66 6.06 Error distribution Mean 4.39 4.30 SD 5.01 4.75 Min 0.00 0.00 25% 1.18 1.02 50% 3.08 2.87 75% 6.35 6.03 80% 7.24 6.98 90% 10.06 9.59 95% 12.67 12.02 99% 18.35 16.95 Max 655.07 530.83 3. Aggregated prediction Model Default model Aggregated model Val RMSE 11.05 8.52 Error distribution Mean 5.46 4.91 SD 9.27 6.31 Min 0.00 0.00 25% 1.23 1.11 50% 3.27 3.19 75% 6.75 6.72 90% 11.11 10.90 95% 14.79 14.38 98% 25.33 20.80 99% 45.62 30.18 Max 655.07 530.83 Conclusions The outlier model predicts the records from outlier merchants with an error of fewer than 30 minutes 80% of the time. The error is less than ~15 minutes 50% of the time. The default model only predicts 65% of the orders with an error that is less than 30 minutes. For 25% of orders, the error is more than 40 minutes. “Without outlier model” ?(regular model) is similar to the default model. Code: https://git.mytaxi.lk/pickme/data-services/dp-eta-delivery-screen-model-training",False
2421882960,page,current,/x/UABbk,En-route time model,"Problem definition The en-route time is the time from order pickup status to order delivery status during a food/marketplace order. Currently, this part of the delivery time is modelled by the ‘Core ETA hailing model’ which has been trained on hailing data. This analysis aims to build a predictive model to predict the en-route time by considering the delivery vertical data. Introduction The analysis was conducted considering one month of data from 07th July 2021 to 07th August 2021 . The OSRM API has been used to calculate the distance between the merchant location and the delivery location. The raw data obtained from data tables are as follows. Feature Data table orderid fact_unique_food_orders tripid fact_unique_food_orders resturantid fact_unique_food_orders pickuplat fact_unique_food_orders pickuplon fact_unique_food_orders droplat fact_unique_food_orders droplon fact_unique_food_orders distance fact_rides taximodelid fact_rides Merchant confirmed time fact_food_orders_log Order picked up time fact_food_orders_log Order delivered time fact_food_orders_log Pooling data hailing_meter_calculation Extracted features Extracted feature Description enroute_time Dependent variable. The time between the order pickup and the order delivered date, time, hour, min Obtained from the merchant confirmed time stamp weekday The day of the week with Monday=0, Sunday=6 OSRM distance Calculated considering pickup lat, lon and drop lat,lon StartHexCenterLatitude, StartHexCenterLongitude Center location of the merchant hexbin EndHexCenterLatitude, EndHexCenterLongitude Center location of the delivery location hexbin AverageSpeed distance/enroute_time DistanceRatio OSRM_distance/distance ispooled_7daysAverage Average number of poolings happened in last 7 days (merchant level) Descriptive analysis Initially, a descriptive analysis was done to investigate the behaviour of the variables. En-route time (Dependent variable) The en-route time ranges from 0 to 300 minutes where 99% of orders have an en-route time which is lesser than 35 minutes. Thus, higher en-route times can be considered as outliers. DSD Taxi model Pooling Filtrations A filtration step was done in order to select regular trips considering the following criteria. Removed trips that have an average speed > 100km/h Removed the trips which have an actual distance > 35km Considered only the following taxi models 43 - Food - Food Delivery Base model 52 - Food Plus 56 - Food Exclusive 67 - Gas Delivery 68 - Market Pool 69 - Grocery Regular 70 - Market Pool 71 - Market Delivery filter Loss of data Loss of data (%) average speed > 100km/h 1 actual distance > 20km 47 0.01% Outlier removal After filtering the data, an outlier removal step was carried out as follows. The maximum en-route time in the training dataset after removing the outliers is 27.35 minutes. Variable Mean st.dev min 25% 50% 75% max Upper fence En-route time 11.77 7.08 0.12 6.82 10.37 15.03 305.93 27.35 **Data loss - 3.24% Model training An XgBoost model was trained considering the below-extracted features. Model errors and feature importance are as follows. Treat as numerical features. StartHexCenterLatitude StartHexCenterLongitude EndHexCenterLatitude EndHexCenterLongitude OSRM_distance Pooled_7day_average Treat as cyclical features Weekday Hour Train RMSE Test RMSE Validation RMSE (With outliers) 3.57 3.69 5.21 RMSE for outliers and non-outliers non-outliers outliers All orders Validation RMSE 3.75 20.39 5.21 The RMSE for outlier orders (3.25% of the dataset) is 20.39 minutes and the individual error is less than 20 minutes for 75% of outlier orders. The individual errors are less than 5.55 minutes for 90% of the non-outlier orders. RMSE for pooled and non-pooled orders non-pooled pooled All orders Validation RMSE 4.38 7.84 5.21 The model RMSE for pooled orders is higher than the RMSE of the non-pooled orders. The individual errors are less than 5 minutes for 66% of pooled orders and 90% of non-pooled orders.",False
2430369820,page,current,/x/HIDck,Incentive Types and Tables,"This document provides an overview on the types of incentives and related tables . Hailing Incentives (Weekday and Weekend) Qualifier: reads from hive_db_insights.driver_anomalous_stats fraud data hive_db_stage.driver_incentive_strategy strategies based on primary taxi model hive_db_stage.driver_incentive_strategy_target targets for the strategies and incentive amount hive_db_stage.driver_active_primary_taxi_model active primary taxi models of the drivers hive_db_stage.fact_rides past rides data hive_db_stage.fact_daily_driver_kpi rpp, itc and hfc trips data hive_db_stage.driver_incentive_qualifiers previous qualifier data hive_db_stage.dim_driver driver data writes to hive_db_stage.driver_incentive_qualifiers qualification data Daily Calculator: reads from hive_db_insights.driver_anomalous_stats fraud data hive_db_stage.driver_active_primary_taxi_model active primary taxi models of the drivers hive_db_stage.fact_rides past rides data hive_db_stage.fact_daily_driver_kpi rpp, itc and hfc trips data hive_db_stage.fact_corporate_trip corporate trips data hive_db_stage.driver_incentive_qualifiers qualifier data hive_db_stage.trip_flow rejected trips data writes to hive_db_stage.driver_incentive_achievements achievement data Food Weekly Incentives Qualifier: reads from hive_db_stage.food_incentive_strategy strategies based on primary taxi model and district hive_db_stage.food_incentive_strategy_target targets for the strategies and incentive amount hive_db_stage.driver_active_primary_taxi_model active primary taxi models of the drivers hive_db_stage.fact_rides past rides data hive_db_stage.food_incentive_qualifiers previous qualifier data hive_db_stage.dim_driver driver data writes to hive_db_stage.food_incentive_qualifiers qualification data Daily Calculator: reads from hive_db_stage.driver_active_primary_taxi_model active primary taxi models of the drivers hive_db_stage.fact_rides past rides data hive_db_stage.food_incentive_qualifiers qualifier data hive_db_stage.trip_flow rejected trips data writes to hive_db_stage.food_incentive_achievements achievement data Oneday Incentives Qualifier: reads from hive_db_stage.driver_active_primary_taxi_model active primary taxi models of the drivers hive_db_stage.incentive incentive rules hive_db_stage.incentive_strategy_targets targets and incentive amount hive_db_stage.dim_driver driver data hive_db_raw.hailing_dr_cohort_driver cohorts and drivers hive_db_stage.dim_driver_cohort cohort modified date details hive_db_stage.dim_taxi_model taxi model data writes to hive_db_stage.incentive_qualifiers qualification data Daily Calculator: reads from hive_db_stage.fact_rides past trips data hive_db_stage.driver_active_primary_taxi_model active primary taxi models of the drivers hive_db_insights.driver_anomalous_stats fraud data hive_db_stage.incentive_qualifiers qualifier data hive_db_stage.fact_corporate_trip corporate trips data hive_db_stage.trip_flow rejected trips data writes to hive_db_stage.incentive_achievements achievement data Welcome Bonus Flash Weekly Incentives This type of incentives is implemented on Generic Incentives Engine Framework (An overview of the framework can be found here ) According to the framework, tables from 3 different engines are required for this incentives. Generic Kpi Engine reads from hive_db_stage.generic_kpi_rule_config query configurations for required kpis flash filter: identifier IN ('INCENTIVES', 'DRIVER_COMPLETITION_RATIO', 'PRIMARY_TAXI_MODEL') writes to hive_db_stage.generic_daily_driver_kpi pre calculated kpi data flash filter: context_1 IN ('49', ‘18') AND kpi IN (’totallegtrips','totaltrips','fraudtrips','itctrips','hfctrips','rpptrips') context_1 IN ('MON_TO_SUN') AND context_2 IN ('all') AND kpi IN ('driver_completion_ratio') Generic Cohort Engine reads from hive_db_stage.generic_cohort_config cohort rule configurations flash filter: cohort_name IN ('FLASH_49', 'FLASH_18') hive_db_stage.generic_cohort_rule_kpi cohort rule kpi requirements flash filter: cohort_name IN ('FLASH_49', 'FLASH_18') hive_db_stage.generic_daily_driver_kpi pre calculated kpi data flash filter: identifier IN ('INCENTIVES', 'DRIVER_COMPLETITION_RATIO', 'PRIMARY_TAXI_MODEL') writes to hive_db_stage.generic_driver_cohort evaluated cohort data flash filter: cohort_name IN ('FLASH_49', 'FLASH_18') Generic Incentive Engine reads from hive_db_stage.generic_incentive_config incentive rule configurations flash filter: rule_name IN ('FLASH_WEEKLY_49', 'FLASH_WEEKLY_18') hive_db_stage.generic_cohort_config cohort rule configurations flash filter: cohort_name IN ('FLASH_49', 'FLASH_18') hive_db_stage.generic_incentive_rule incentive rule requirements flash filter: rule_name IN ('FLASH_WEEKLY_49', 'FLASH_WEEKLY_18') hive_db_stage.generic_incentive_reward reward configurations for the rules flash filter: rule_name IN ('FLASH_WEEKLY_49', 'FLASH_WEEKLY_18') hive_db_stage.generic_driver_cohort evaluated cohort data flash filter: cohort_name IN ('FLASH_49', 'FLASH_18') hive_db_stage.generic_daily_driver_kpi pre calculated kpi data flash filter: identifier IN ('INCENTIVES', 'DRIVER_COMPLETITION_RATIO', 'PRIMARY_TAXI_MODEL') writes to hive_db_stage.generic_driver_rewards_eligibility calculated reward eligibility details according to the provided rule, cohort, kpi and reward data. flash filter: rule_name IN ('FLASH_WEEKLY_49', 'FLASH_WEEKLY_18')",False
2456420371,page,current,/x/EwBqkg,CleverTap Click Stream Ingestion,"This is the Documentation on How CleverTap clickstream data is ingested to DataLake. This process consists of three main aspects. CleverTap to GCP export GCS Connector Configuration in Hadoop GCP to Hive Data export. CleverTap to GCP export This is a very straightforward process that enables us to bulk export the CleverTap event or profile data to a GCP bucket. Step 1: Create a service account for the project with Storage Admin permission, Step 2: Create a Storage Bucket Step 3: Add the GCP service account credentials and Bucket details to CleverTap. Step 4: Create a Data Export - (In our case we have created a recurring data export to happen every 24 hours for selected events). Comprehensive Documentation: https://docs.clevertap.com/docs/data-export-to-gcp GCS Connector Configuration in Hadoop The Cloud Storage connector lets us run Hadoop or Spark jobs directly on data in Cloud Storage. The Cloud Storage connector is installed by default on all Dataproc cluster nodes. For Non-Dataproc clusters, we can install and use the Cloud Storage connector on a Hadoop/Spark cluster. Step 1: Download Cloud Storage Connector from here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage as per the Hadoop version. Step 2: Place the downloaded Cloud Storage Connector jar in the $HADOOP_COMMON_LIB_JARS_DIR directory. Or alternatively, we can add HADOOP_CLASSPATH=$HADOOP_CLASSPATH:</path/to/gcs-connector.jar> to hadoop-env.sh in the Hadoop configuration (in our case through Ambari). Step 3: Configuring Hadoop To begin, we need a JSON key file so the connector can authenticate to Google Cloud Storage. We can use the JSON key file of the service account created for CleverTap to GCP Export. Once the JSON key file is obtained, we can configure the following properties in core-site.xml on the Hadoop configuration. xml <property> <name>fs.AbstractFileSystem.gs.impl</name> <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value> </property> <property> <name>fs.gs.project.id</name> <value>{project_id}</value> </property> <property> <name>google.cloud.auth.service.account.enable</name> <value>true</value> </property> <property> <name>google.cloud.auth.service.account.json.keyfile</name> <value>{/path/to/keyfile}</value> </property> The detailed documentation and the advanced properties can be found here: https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md GCP to Hive Data Export Since the CleverTap events are exported as JSON and the individual entries are also in the JSON structure, we had to create a table with complex DataTypes such as Arrays, Maps, and Structs. sql CREATE EXTERNAL TABLE IF NOT EXISTS ${stage_db}.clevertap_click_stream ( ts bigint, eventname string, profile struct<all_identities:array<string>, email:string, identity:string, name:string, phone:bigint, platform:string, push_token:string>, deviceinfo struct<appversion:string, browser:string, dimensions:struct<height:bigint, unit:string, width:bigint>, dpi:bigint, make:string, model:string, osversion:string, sdkversion:string>, controlGroupName string, eventprops Map<string, string> ) PARTITIONED BY(date_time STRING) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' STORED AS TEXTFILE LOCATION ${table_location}; Here we user JSON Serde. This library enables Hive to read and write in JSON format. It includes support for serialization and deserialization. Here we have used the JSON Serde from http://www.congiu.net/hive-json-serde/ which is Highly recommended by many in internet forums. So, before we create the table we have to add the Serde to the session as, ADD JAR ${Serde Location} And then we can run the create script. Finally, the deployed flow will extract data from GCP and move into the corrosponding locations, where we now have access to the CleverTap data in the HIve Table. Thank You :)",False
2465234964,page,current,/x/FIDwkg,Generic Rewards Engine,Untag passengers from promo codes upon usage Offer Promo Rewards Offer Cash Rewards TBA,False
2509111337,page,current,/x/KQCOlQ,Outlet Search Technical Documentation,Overview The purpose of this document is to give high level insight into the logic of extracting the popular categories for each restaurant. Used tables hive_db_stage.fact_food_orders hive_db_stage.fact_food_order_items hive_db_stage.dim_food_skus Popular tag calculation Filter tuples from hive_db_stage.fact_food_orders corresponding to the previous 30 days Filter tuples from hive_db_stage.fact_food_order_items corresponding to the previous 30 days Join the above two relations on order id to get a single relation of ordered items of each restaurant for the last 30 days Join the resulting relation with hive_db_stage.dim_food_skus on sku id to get a single relation of* ordered categories of restaurants for the last 30 days. Group the resulting relation by restaurant and category and order by the descending order of the order count Group the resulting relation by restaurant and concat the category IDs into a single column ETL Pipelines dp-customer-hadoop pipeline will be used for the pre-bulk loading purpose using the above logic and then sqoop-loader pipeline for handling the export to Foodie database.,False
2513698831,page,current,/x/DwDUlQ,Trip Counts of Driver Referral Invitee Drivers,"Overview Objective of this implementation is to build a pipeline to calculate the trip counts and date of the last completed trip of invitees in the driver referral program. Plan of the Implementation ETLs There are two steps in ETL pipeline Step 1 : Adding a new workflow to KPI engine ( dp-kpi-engine ) to calculate completed trip counts each day for each invitee driver with active referral. Get the list of driver ids should need the daily trip calculation Take the records of last 100 days which has the registered state or claimed status ( can claim before the expiry ) and expiry date is yesterday or before from driver referral log Partition by the id and pick the latest record to avoid duplicates 2. Calculate Trip count Use the fact_rides table yesterday data to calculate the completed trips for list of drivers generated in above part 1 Following types of trips are not considered for calculate the trip count ( part 2) and last trip date ( part 3) When trip distance is less than 50m Road Pickups Not completed trips Assumption - The trip count and last trip date only need to be track until the expiry date and return the progress until the expiry for the request after the expiry Workflow :- Frequency :- Daily Project :- dp-kpi-engine Table :- Use case Table KPI Daily Trip Count generic_daily_driver_kpi driver_referrals_daily_trip_counts Step 2: Track the incremental trip count To calculate the total trip count of invitee drivers, use generic_daily_driver_kpi data calculated above and aggregated_driver_referral_trips tables last partition. Aggregated trip counts inserted into the yesterday partition of aggregated_driver_referral_trips table where that partition have trip counts of all invitee drivers until expiry date or yesterday This data is used to provide Driver referral Invitee trip counts via API. API Doc : Driver Referral - Get Completed Trip Count API",False
2514092050,page,current,/x/EgDalQ,December Release,,False
2520023041,page,current,/x/AYA0lg,Include delivery drop locations - Recommended Locations,"System JIRA 31d5135d-f379-368a-8d06-7e4d81dc9da2 DATA-1662 Objective Suggest delivery drop location as start location through the API. Currently, only pickup locations are considered for start location regardless of the service group. Plan of the Implementation Flow Abstract Flow Start Location Logic Flow Steps highlighted with the red color border will change Proposed Query For reading Start Location SELECT PassengersLogId, PassengerId, CASE WHEN servicecode IN ('FOOD_DELIVERY', 'MARKET_PLACE') THEN droplatitude ELSE PickUpLatitude END AS PickUpLatitude, CASE WHEN servicecode IN ('FOOD_DELIVERY', 'MARKET_PLACE') THEN droplongitude ELSE PickUpLongitude END AS PickUpLongitude FROM hive_db_stage.fact_unique_trip WHERE travelstatus = 1 AND createdDate >= <date passed from the program>",False
2527526913,page,current,/x/AQCnlg,Driver Rating Logic Streamlining - Driver Performance,"Change of the implementation of Driver rating logic in the Driver performance project in order to bring the aforementioned logic to a common implementation across all the projects where driver rating is used. The output table in this project has the following columns: driver_id one_starts two_stars three_stars four_stars five_stars total_ratings average_rating good_comments bad_comments The average_rating column contains a regular average of ratings the driver has obtained after the first day of the current quarter. This rating logic should be replaced with the logic described here . This is pre-calculated in the latest_driver_rating column of the daily_driver_ratings . As the new rating logic considers last 100 trips within the last one year, the data in other columns: one_stars … five_stars , total_ratings , good_comments and bad_comments should also be modified to be calculated with the same conditions.",False
2549252120,page,current,/x/GIDylw,Driver Referral - Get Completed Trip Count API,"Revision History Date Version Change Request Date Description Author 2021-11-08 1.0.0 N/A Initial Version Aathif Sanaya 2022-06-21 1.0.1 N/A Added 2 new fields (Fraud Checks) to the payload Aathif Sanaya References Reference Version Date Remarks Tech Doc - Driver Referral N/A N/A Link Introduction Driver Referral Completed Trip Count API is hosted in the PickMe data backend under Enterprise View of Driver (EVoD) API domain. This API will provide the completed trip count and the last trip date of the given drivers. Currently the API is facilitated as a RESTful API. Request Schema Following table includes the request schema of the Driver Referral Completed Trip Count API. URL http://<endpoint_addr>/iserv/v1/evod/ {driverIds} /getcompletedtrips Description Service to request driver completed trips based on driver id and other filtering parameters. Request Method GET Request Body Format N/A Response Schema (Success) application/json Note 1: HTTP 200 (OK) response code. Note 2: Please see section Response Schema for response body schema. Response Schema (Error) application/json Note 1: Respective HTTP 4xx (Client Error) or 5xx (Server Error) response code. Note 2: Please see section Error Response Schema for error response body schema. Query Parameters: Parameter Name State Type Values Description Remarks {driverIds} Mandatory Path One or more valid driver ids Id(s) of the driver(s) whose KPIs are being requested. E.g. http://<endpoint_addr>/iserv/v1/evod/ 821546%3B821547 /kpis/agg At least one driver Id must be provided. In case multiple driver ids are provided they must be separated with semicolon characters as shown below. String of format driverId1;driverId2 Note - Please ensure to URL encode all semicolon(;) characters in the URL (i.e. %3B). Note 1: Invalid driver id/s would result in error code 1302. period Optional int 90 (default) Period for the values being requested. Note 1: Past 90 days is the only supported period currently. Note 2: Invalid period values would result in error code 1303. Request Headers: Following table illustrates the Request Headers that must be included in the Driver Referral Completed Trip Count API. Header Name State Example Value Description X-EVOD-AGG-KPIS-API-KEY Mandatory androidprod API key issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-EVOD-AGG-KPIS-API-SECRET Mandatory feaee65a-2f54-4b18-8a7e-ea409a2e0d72 API secret issued to the consumer by the Big Data team. Must be kept in the consumer's config as value will be changed in a breach. X-CORRELATION-ID Optional 223e1203-0237-4770-89f6-31d65843f4c1 If provided, the given Correlation Id will be logged with all log messages logged by the Driver Aggregated KPI Service in executing this particular request. This will enable deeper issue analysis from source to driver kpi service backend. If the same Correlation Id is tracked in the source side also, then the issue analysis will be extremely efficient. In case this header is not included in the request, Driver Aggregated KPI Service will generate its own Correlation Id for each incoming request and use it on the log messages logged for each processed request. Response Schema HTTP 200(OK) response schema of Driver Referral Completed Trip Count API will follow the base JSON response structure indicated below. { ""payloads"":[ { ""inviteeDriverId"": (integer), ""completedTripCount"": (integer), ""lastTripOn"": (string), ""isActiveInviterFraud"": (boolean), ""isBankDetailsFraud"": (boolean) } ] } payload array Contains the entire response payload.This array can contain the driver profiles of given all drivers. inviteeDriverId integer Id of the driver to whom this response obj is related. completedTripCount integer Count of the trips which the driver has completed for the given period. lastTripOn String Date of the last trip done by driver. isActiveInviterFraud Boolean Flag to Check if the Inviter reached his target after the invitee isBankDetailsFraud Boolean Flag to check if the bank details between inviter and invitee are the same at any point during the referral Response Headers: Header Name Description EVOD-AGG-KPIS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Error Response Schema Error responses from Driver Referral Completed Trip Count API can be two folds, Client errors (4xx codes). Server errors (5xx codes). Error Response Headers: With all error responses, Driver Referral Completed Trip Count API will include the following response header(s). Header Name Description EVOD-AGG-KPIS-CORRELATION-ID Correlation Id for this request. This will be the same Correlation Id value that was passed in the X-CORRELATION-ID header if that header was included by the caller in the request. If not, server generated Correlation Id will be returned. Note: It will be helpful to submit this Correlation Id value to L2 and Engineering whenever the exact cause for the API error response needs to be further analyzed in the backend server logs. Client Errors: All client initiated error responses from Driver Referral Completed Trip Count API will be respective HTTP 4xx responses, and the response body will be in the following JSON schema. { ""errors"": [ { ""correlationId"": (String), ""code"":(String), ""message"": (String), ""developerMessage"": (String) } ] } errors array Array containing one or more errors. correlationId string A UUID unique to each request code string Error code unique to the occurred error. message string A friendly error message. developersMessage string A more detailed technical error message if needed. Following table illustrates the possible error codes and corresponding error messages. Error Code Error Message Corresponding HTTP Code EVOD-AGG-KPIS-1300 Missing authentication details. 401 DATA-1301 Throttling limit reached. Try again in a few seconds. 401 DATA-1302 Invalid Driver Id/s. 401 DATA-1303 Invalid Period 401 EVOD-AGG-KPIS-4xx Client error. Please send an email to l2support@pickme.lk with the EVOD-AGG-KPIS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.4 Client Error section. Server Errors: All server error responses from Driver Referral Completed Trip Count API will be respective HTTP 5xx responses, and the response body will be in the following JSON schema. { ""errors"":[ { ""code"": “(string)”, ""message"": “(string)” } ] } errors array Array containing one or more errors. code string Error code unique to the occurred error. message string Error message with a textual description of the error which may be useful for logging purposes in the consumer side. As per best practices in not exposing the exact error that occurred in the server side to the caller, the error code and message will always be the following. Error Code Error Message Corresponding HTTP Code EVOD-AGG-KPIS-5xx Server error. Please send an email to l2support@pickme.lk with the EVOD-AGG-KPIS-CORRELATION-ID response header value. Any valid client error code as defined in link 10.5 Server Error section. An Example Request and Response Following is an example request with 'type' optional parameter specified in the request: http://<endpoint_addr>/iserv/v1/evod/45789%3B12345%3B24578%3B30158/getcompletedtrips?period=90 Please note that the following headers were included with the request: X-EVOD-AGG-KPIS-API-KEY: androidprod X-EVOD-AGG-KPIS-API-SECRET: feaee65a-2f54-4b18-8a7e-ea409a2e0d72 The above request has requested for: Driver Trip Count details for the driver ids 45789,12345,24578,30158 ( 45789%3B12345%3B24578%3B30158 ). The requested period is 90 which is the default value. Following is an example response for the above request: c { ""payloads"": [ { ""inviteeDriverId"": 12345, ""completedTripCount"": 0, ""lastTripOn"": null, ""isActiveInviterFraud"": False, ""isBankDetailsFraud"": False }, { ""inviteeDriverId"": 24578, ""completedTripCount"": 17, ""lastTripOn"": ""2021-08-25"", ""isActiveInviterFraud"": False, ""isBankDetailsFraud"": False }, { ""inviteeDriverId"": 45789, ""completedTripCount"": 25, ""lastTripOn"": ""2022-11-01"", ""isActiveInviterFraud"": False, ""isBankDetailsFraud"": False }, { ""inviteeDriverId"": 30158, ""completedTripCount"": 101, ""lastTripOn"": ""2020-03-14"", ""isActiveInviterFraud"": False, ""isBankDetailsFraud"": False } ] } Note the following header value was included into the above response: EVOD-AGG-KPIS-CORRELATION-ID: 05d0edec-254d-40df-93d5-5a6aef71558e",False
2614755329,page,current,/x/AQDamw,ETA Log Extractor,"ETA Log Extractor is a Cron Job, which extracts ETA Log files from Kubernetes Pod and loads them into a Hive table. The eta_log_extractor.sh need to be run every day which extracts and pushes the log files into Hadoop Clusters. Eta_Log_Extractor pod1=$(kubectl --namespace data-services get pods -l app=deliveryeta -o jsonpath=""{.items[0].metadata.name}"") log_date=${1:-date +%Y-%m-%d -d ""yesterday""} echo ""copying logs from ${pod1} ......."" for location in kubectl exec -n data-services ${pod1} -- ls -la /opt/service/mserv-delivery-eta/logs/app | awk '{print $NF}' | grep ${log_date} | tr '\n' ' '; do echo ""copying file from ${pod1}/opt/service/mserv-delivery-eta/logs/app/${location} to files/${location}"" kubectl --namespace data-services cp ${pod1}:/opt/service/mserv-delivery-eta/logs/app/${location} files/pod1-${location} done #scp files/-application.log-${log_date}. prd-hadoop-ssh@hdp-hpprd-prim-pkme-ssh.azurehdinsight.net:/home/prd-hadoop-ssh/deliver-eta-logs sshpass -p ""U?4x207jbaZ1J0U"" scp files/-application.log-${log_date}. dev-spark-ssh@sprk-hpdev-prim-pkme-ssh.azurehdinsight.net:/home/dev-spark-ssh/delivery-eta-logs/ # Remove File Folder rm -r files Running the command ./eta_log_extractor.sh will extract the log files which are generated a day before and if log files have to be extracted on a specific day the command ./eta_log_extractor.sh ""2022-01-16"" by specifying the date at the end. Since the log extractor has to be run first, before the extracted files are loaded into hive tables, this job has to be scheduled before the hive load job. In the crontab file, add the following line to schedule the eta_log_extractor job 0 3 * * * /location/to/eta_log_extractor.sh > /dev/null 2>&1 . So, this job will run at 3am daily. The next step is, once these data are exported to Hadoop, it has to be loaded to hive tables. Upload_Logs log_date=${1:-date +%Y-%m-%d -d ""yesterday""} hdfs dfs -mkdir -p wasbs://hive-db-stage@hdinhpdevshrprimpkme.blob.core.windows.net/eta_accuracy_analysis_log_files/$log_date/ hdfs dfs -put $log_date.gz wasbs://hive-db-stage@hdinhpdevshrprimpkme.blob.core.windows.net/eta_accuracy_analysis_log_files/$log_date/ rm -r $log_date.gz Running the command ./upload_logs.sh will load the data into hive tables. and as above here also a custom date can be specified. And in the crontab file, add the following line to schedule the upload job 0 4 * * * /location/to/upload_logs.sh > /dev/null 2>&1 Cheers!",False
2616885446,page,current,/x/xoD6mw,Google Cloud Dataproc Provisioning,This section includes the PickMe Big Data and Data Science stack provisioning instructions in Google Cloud Dataproc. Setting up Hive metastore in a MySQL database on Cloud SQL Setting up Dataproc Cluster,False
2617704452,page,current,/x/BAAHn,Setting up Hive metastore in a MySQL database on Cloud SQL,"Apache Hive is a popular open source data warehouse system built on Apache Hadoop . Hive offers a SQL-like query language called HiveQL , which is used to analyze large, structured datasets. The Hive metastore holds metadata about Hive tables, such as their schema and location. Where MySQL is commonly used as a backend for the Hive metastore, Cloud SQL makes it easy to set up, maintain, manage, and administer your relational databases on Google Cloud. While Hive gets provisioned when deploying an Dataproc cluster, it is important to keep the Hive metastore external to the cluster as a cluster disaster will not affect the Hive metastore to go down. Additionally, external metastore is the only mechanism where multiple Dataproc clusters can share the same Hive layer.",False
2626781360,page,current,/x/sICRn,April Release 2022,,False
2627469440,page,current,/x/gACcn,Adding Effective Date & Times and Achievements calculated by Date & Times in the Incentive Feeds,"Objective Objective of this development is to provide a visibility of the effective from and to date & time and the achievement calculated by date & time of the incentives consistantly through the API and the data export. Such information would be used by the driver app, Achievement calculated date & time would indicate until which date & time the achievement is valid upto. So when displaying the current achievement, it is important to get the trip counts from this date & time from the driver db and add this to the value sent in data API/data export Effective from & to date & time would be used to further narrow down the achievement calculation scope in case of the timebanded incentives. Solution The requirement is partially covered with some of the fields already existing in the API and database table inwhich case they will be reused to reduce the integration complexity. Following table indicates how various information is available in the API and driver_incentives table Information Field API Field column in driver_incentives Remarks Effective from date & time elgInfo → startDateTime (existing field) effective_from (new column) Data Type : DateTime Data Type Format : YYYY-MM-DD hh:mm:ss This field would consistantly indicate across all incentive types the achievement calculation start date and time Effective to date & time elgInfo → endDateTime (existing field) expire_date (existing column) Data Type : DateTime Data Type Format : YYYY-MM-DD hh:mm:ss This field would consistantly indicate across all incentive types the achievement calculation end date and time Achivement Calculated by date & time achInfo → achByDateTime (existing field) achievement_by (new column) Data Type : DateTime Data Type Format : YYYY-MM-DD hh:mm:ss This field would consistantly indicate across all incentive types the date and time up to which the returned incentive achievement counts are valid for Notes: For oneday incentives starting today which will always have 0 achievement, this date would be set to previous date 23:59. E.g. oneday incentive happening 30-01-2022 would have this time set to 29-01-2022 23:59 Calculating current achievement count from driver db go Calculation start date & time = latest_of( achievement calculated by date & time, effective from date & time) Calculation end date & time = earliest_of (current time, effective to date & time) Achievement calculated from driver db = trip_count_between (Calculation start date & time,Calculation end date & time) Total achievement = Achievement returned from data API/table + Achievement calculated from driver db Assumptions The time banded oneday incentives should be displayed in the driver app entire day and not just in the timeband it is valid for Mandatory Tests Type of Incentive Date & Time Driver Opening the App Effective from date & time Effective to date & time Achievement Calculated By Date & Time Achievement Calculation Logic for driver db Hailing Weekly Incentive Efcctive Jan 17-21 (Mon-Fri) Jan 17 - 07:00 Jan 17 - 00:00 Jan 21 - 23:59 Jan 16 - 23:59 trip_count_between (Jan 17 - 00:00 , Jan 17 - 07:00) Jan 17 - 13:00 Jan 17 - 00:00 Jan 21 - 23:59 Jan 16 - 23:59 trip_count_between (Jan 17 - 00:00 , Jan 17 - 13:00) Jan 18 - 07:00 Jan 17 - 00:00 Jan 21 - 23:59 Jan 16 - 23:59 trip_count_between (Jan 17 - 00:00 , Jan 18 - 07:00) Jan 18 - 13:00 Jan 17 - 00:00 Jan 21 - 23:59 Jan 17 - 23:59 trip_count_between (Jan 17 - 23:59 , Jan 18 - 13:00) Timebanded Oneday Incentive Effective Jan 17 (13:00-15:00) Jan 17 - 08:00 Jan 17 - 13:00 Jan 17 - 15:00 Jan 16 - 23:59 trip_count_between (Jan 17 - 13:00 , Jan 17 - 08:00) = Always 0 Jan 17 - 14:00 Jan 17 - 13:00 Jan 17 - 15:00 Jan 16 - 23:59 trip_count_between (Jan 17 - 13:00 , Jan 17 - 14:00) Jan 17 - 16:00 Jan 17 - 13:00 Jan 17 - 15:00 Jan 16 - 23:59 trip_count_between (Jan 17 - 13:00 , Jan 17 - 15:00) Whole day oneday Incentive Effective Jan 17 (00:00-23:59) Jan 17 - 09:00 Jan 17 - 00:00 Jan 17 - 23:59 Jan 16 - 23:59 trip_count_between (Jan 17 - 00:00 , Jan 17 - 09:00) Jan 17 - 14:00 Jan 17 - 00:00 Jan 17 - 23:59 Jan 16 - 23:59 trip_count_between (Jan 17 - 00:00 , Jan 17 - 14:00)",False
2627993603,page,current,/x/AwCkn,Demand Prediction,"Introduction to the problem Defining Supply and Demand (Grab) A single unit of Supply is considered as a driver who is Online and Idle (not currently on a job) at the beginning of an x seconds slot, where x is a minuscule unit of time. The driver’s GPS ping at the beginning of this x seconds slot is considered to be his or her location. (Location can be considered as a certain hexbin) A single unit of Demand is considered as a passenger who is looking for a ride via the app within the same x seconds slot. We consider the passenger’s location to be the pick up address entered. The underlying aim of predicting demand and supply in a particular area at a certain time is to decrease the difference between it. Grab has calculated this difference considering a supply and demand ratio and a difference using below metrics. The below metrics shows the calculated ratio and difference between demand and effective supply for every geohash i and a time slot j. According to Grab, a visualization of the gap between demand and supply in Singapore is as follows. Each bubble indicates a minuscule area on the map. The size of each bubble indicates the Supply Demand difference in that area - the bigger the bubble, the bigger the gap. They’ve also coloured the bubbles to indicate the Supply Demand Ratio where Red signifies Undersupplied and Green signifies Oversupplied. Literature review Both machine learning and time series methodologies can be used for demand prediction. A study done by Zhao, Khryashchev & Vo (2021) to predict taxi and Uber demand defines the demand prediction problem as follows: Given historical taxi demand data in a region, we want to predict the number of pickups that will emerge within the next time interval. This study predicts the met taxi demand which is the demand of picking up a customer. Thus the number of pickups within a defined region will represent the taxi demand of that region. This study treats demand as a time series problem and has concluded that simpler methodologies such as Markov prediction algorithms outperforms deep learning algorithms like LSTM . The exogenous variables used in this study for the LSTM model are ”temperature”, ”precipitation”, ”wind speed”, ”day of the week”, and ”hour of the day”. In addition to that, the authors have used the ARIMA model to predict the demand. Another study done by Faghih, Safikhani, Moghimi & Kamga (2019) presents a Spatial-Temporal Modeling method to predict the short term Uber demand. This study has found that the demand for an Uber changes in every 15 minutes time interval and also the volatility of the demand over time differs from one area of a city to another. Thus the demand is changing both spatially and temporally. The study has used a time series approach to capture the time and spatial variation in the Uber demand. The number of pickups have been considered as the demand in this study too. A study done by Wang, C., Hou, Y., & Barth, M. (2019, April) shows how Convolutional Neural Networks can be used to predict the demand in ride hailing services. It uses trip request data offered by DiDi Chuxing to represent the demand. A model done previously by the same authors predicts the next 1-hour total number of demands for a certain region in the city. The main focus of this paper is to provide accurate demand predictions for predefined zones every 10 minutes. Suggested Scope and the limitations for the project Problem statement - Predicting the demand (number of requests/met demand) for rides (hailing vertical) at a certain hexbin in a 15min time interval. Outcome - Show the driver high demanding areas based on his location (hexbin) so he can relocate him if he is already in a low demand area with a higher supply Rules that can be considered High demanding areas should be notified only to the drivers in oversupply and low demanding areas All (or a filtered set) drivers are notified in the beginning of the day about high demanding/low demanding areas (Graphs for nearest areas with time bins. Driver can scroll/swap and find the demand in nearest locations) A driver can’t see the demand of all areas Drivers can identify demand and surge and relocate themselves beforehand Procedure Define the demand (based on a bin, area) Predict the demand based on the other impacting factors (with machine learning)/ predict the demand using time series methods Problems Does the prediction have to be based on the taxi model the customers are looking for? Ex: Show only the tuk demand for tuk drivers. Limitations There can be an imbalance in the driver count when the drivers start to move into high demand areas. This has to be controlled by only showing the demand moving information to a selected number of drivers. Drivers can start questioning when the similar information is not provided to all the drivers References Faghih, S., Safikhani, A., Moghimi, B., & Kamga, C. (2019). Predicting Short-Term Uber Demand in New York City Using Spatiotemporal Modeling. Journal Of Computing In Civil Engineering , 33 (3), 05019002. doi: 10.1061/(asce)cp.1943-5487.0000825 Wang, C., Hou, Y., & Barth, M. (2019, April). Data-driven multi-step demand prediction for ride-hailing services using convolutional neural networks. In Science and Information Conference (pp. 11-22). Springer, Cham. Zhao, K., Khryashchev, D., & Vo, H. (2021). Predicting Taxi and Uber Demand in Cities: Approaching the Limit of Predictability. IEEE Transactions On Knowledge And Data Engineering , 33 (6), 2723-2736. doi: 10.1109/tkde.2019.2955686 https://docs.google.com/presentation/d/1gRtgn7gn_rHeAgynttAOkjrXnTzHfC8Jbo5Bj6abBnk/edit#slide=id.g10f3a14e72a_0_108",False
2628222977,page,current,/x/AYCnn,Personalization,"This article focus on how personalization can be added to the pickMe app based on the implementations currently available on the market. Grab Each Grab user could initiate their own unique personalized impact report at a touch of a button within the app itself. Each report was hyper-personalized based on different preset variables: geographical location, their unique in-app transactions and behaviors. The company also built a direct share functionality into each personal impact report to allow users to share their impact on social media. 2. Recommending content to users (ex: daily deal, rewards, ads, news) (Reference: https://engineering.grab.com/grab-everyday-super-app ) Grab has personalized what each user sees mostly by considering the nature of their profile. Showing many collaborations, ads or deals on the app can make it difficult for the consumer to find what they actually want. Thus, personalizing and recommending makes it easier to find what each user wants to see. Also, grab is offering bill payments, hotel bookings, promos and deals according to the user’s preferences. The aim is not to pack the home screen with various deals and widgets. But to give a unique and personalized experience to the user. Notes: (reference : https://www.grab.com/sg/press/consumers-drivers/grab-introduces-four-new-services-in-singapore-in-its-super-app/ ) Singapore, 23 April 2019 — Grab, the leading super app in Southeast Asia, today announced the launch of four new services – ‘Hotels’ booking, on-demand ‘Videos’ streaming, ‘Tickets’ purchasing and ‘Trip Planner’ that enables Grab users to plan their multimodal trips in real-time, in Singapore. Grab app can be used to plan commutes to work and to order the next meal and get ready for the next movie date or holiday trip. All these can be done through one app, enjoying exclusive offers and even greater value when the user is paying with GrabPay and also he can earn GrabRewards points. These points can, in turn, be used for more discounts or vouchers on other transactions ‘Hotels’ service: Special hotel booking offers through the Grab app Grab offers discounts for platinum and gold users. Also, insurance for travel accidents and flight delays. Also, can pay through Grabpay to earn rewards so they can spend later. Directly streaming video content through the app Grab users can get discounts for streaming through the collaborators’ app too according to their loyalty tier. After including the above service, the feed looks as follows. How the recommendations work The recommendations depend on the below 3 factors. Users - Understand the user behavior to match the relevant and suitable content Items - Characteristics of the content Context - Time of the day, location, etc. Mainly consider the action and the recency to decide the behavior. Recommendation engine Popularity - Using clickstream data of all users User favourite - Personal preference Collaborative Filtering - Behaviours of similar users Habits - Things a user used to do and frequently do 3. Widgets Grab has widgets for games, polls, videos, news and even food recommendations. Recommend useful and relevant widgets to passengers. Letting the passenger know about the transport demand (peak shifting) using a widget in the app. (reference: https://engineering.grab.com/peak-shift-demand-travel-trends ) This widget lets the consumer have the ability to plan their trips beforehand. This widget is able to provide the pricing trends for the next 2 hours. The technique used is to forecast the demand/supply using a widget that can help our users and give them the power to plan their trips beforehand Benefit - Passengers can plan to leave at times when there is more supply, and with potentially more savings for fares Example: This store has showcased their busiest hours so the consumers can understand and shift the peak hours. The widget looks as follows. It pulls together historically observed imbalances between supply and demand, for the consumer’s current location and nearby time periods. Aggregated data is displayed to consumers in easily interpreted visualisations so that they can plan to leave at times when there are more supply, and with potentially more savings for fares Problems addressed by the widget - Offer transparency on the fares: By exposing our historic surge levels for a 4 hour period, we wanted to ensure that the passenger is aware of the surge levels and does not treat the fare as a nasty shock. Give information that helps them plan: By showing them surge levels for the future 2 hours, we wanted to help consumers who have the flexibility, plan for a better time, hence, giving them the power to decide based on transparent information. Provide helpful tips: Every bar gives users tips on the conditions at that time and the immediate future. For instance, a low surge bar, followed by a high surge bar gives the tip “Psst… Leave now, It might get busy later!”, helping people understand the graph better and nudging them to take action. If you are interested in saving fares, may we suggest tapping around all the bars to reveal the secret pro-tips? How the widget is build Data - traffic trends, surge patterns, and behavioral insights of both passengers and drivers Hypothesis testing have been done to confirm the trends and seasonality in the data to conclude that the data can be used to predict behavior or a pattern. Behavioral science is a key part of the widget. (Behavioral science - https://suebehaviouraldesign.com/behavioural-science-and-data/ ) Use data to investigate the most efficient method to forecast surge trends. 4. Demand moving Providing information to drivers to move away from oversupply areas so they can cater to other areas with higher demands 5. Providing good times to pre-book rides Havn London (use only jaguar cars and known as a personalized chauffeur service) Let the driver know your preferences when riding (ex: quiet mode if you are tired and want to have a nap along the way) Uber Trip branding - (Reference: https://eng.uber.com/trip-branding-personalization/ , https://www.uber.com/en-AU/newsroom/trip-branding/ ) Trip branding and personalization allows you to specify branded, customized elements that show up on trips in the Uber app and link back to a URL of your choosing. Include a customized message that is relevant to a specific user at a specific time. For example, Hilton allows users to view stay details and check into their hotel on the way there. Give details about the destination to the user (example: a link for the restaurant menu by zomato) More empathetic and personalized customer support experience (reference - https://eng.uber.com/personalized-customer-support-experiences /) Suggesting more relevant next content as predefined messages after typing in a complaint https://docs.google.com/presentation/d/1SWZSO5_H7N79SXG6HcFsOKdER3jTPVicFvgKUOKjwZE/edit#slide=id.g10a174a78a6_0_85",False
2632941575,page,current,/x/B4Dvn,Retrieving Past Trip Information Technical Document,Overview The purpose of this technical document is to give a high level overview of the preliminary activities required for the Driver Past Trip Information API Requirements The requirements of the API endpoint are mentioned in the ticket DATA-1736 31d5135d-f379-368a-8d06-7e4d81dc9da2 System JIRA . Preliminary Activities Ingest tables missing in Hive from Eztaxi using dp-hailing-raw pm_fare_calculations pm_fare_details Ingest the following tables from DriverDB using dp-driver-db-raw complaint Use prebulkloader job to export the desired information into HBase,False
2633728174,page,current,/x/roD7n,Fraud Tagger,This flow goes through three jobs. First to filter out the anomalous drivers and passengers and then to get the anomalous driver-passenger pairs.,False
2644738373,page,current,/x/RYGjnQ,Data Ingestion from Hive to Druid Data Source,"Objective Objective of this document is to outline the approach of data Ingestion from Hive tables to Druid data sources. This is a generic framework and to add a new ingestion task follow the step described in the below section. Methodology A generic pipeline is implemented in dp-druid-ingestion to ingest data from hive tables to druid data sources. When adding new ingestion task, come up with two below things are required. A hive query to transform the data in hive table to required format in druid data source. Supervisor spec to perform the ingestion task. A hive query to transform the data in hive table to required format in druid data source Step 1 -: This query is depend on source table and the transformation need to perform and it can be any hive query. ex -: Query for Sales Analytics SELECT sales_1to30.merchantid merchantid, sales_1to30.daterank daterank, sales_1to30.sales sales_130, sales_1to30.orders orders_130, sales_1to30.aov aov_130, sales_30to60.sales sales_3060, sales_30to60.orders orders_3060, sales_30to60.aov aov_3060 FROM ( SELECT *, date_sub(date(current_timestamp()), 1)-date(daterank) days_back FROM hive_db_insights.sales_analytics WHERE dateperiod='1|30' AND date(date_time)=date_sub(date(current_timestamp()), 1) ) sales_1to30 INNER JOIN ( SELECT *, date_sub(date(current_timestamp()), 31)-date(daterank) days_back FROM hive_db_insights.sales_analytics WHERE dateperiod='1|30' AND date(date_time)=date_sub(date(current_timestamp()), 31) ) sales_30to60 ON sales_1to30.merchantid=sales_30to60.merchantid AND sales_1to30.days_back=sales_30to60.days_back Step 2 -: After come up with the query make it as a single string and insert it to the generic_druid_config table as below INSERT INTO TABLE hive_db_stage.generic_druid_config VALUES ( “sales_analytics”, “DAY”, ""SELECT sales_1to30.merchantid merchantid, sales_1to30.daterank daterank, sales_1to30.sales sales_130, sales_1to30.orders orders_130, sales_1to30.aov aov_130, sales_30to60.sales sales_3060, sales_30to60.orders orders_3060, sales_30to60.aov aov_3060 FROM (SELECT *, date_sub(date(current_timestamp()), 1)-date(daterank) days_back FROM hive_db_insights.sales_analytics WHERE dateperiod='1|30' AND date(date_time)=date_sub(date(current_timestamp()), 1)) sales_1to30 INNER JOIN (SELECT *, date_sub(date(current_timestamp()), 31)-date(daterank) days_back FROM hive_db_insights.sales_analytics WHERE dateperiod='1|30' AND date(date_time)=date_sub(date(current_timestamp()), 31)) sales_30to60 ON sales_1to30.merchantid=sales_30to60.merchantid AND sales_1to30.days_back=sales_30to60.days_back"" , FROM_UTC_TIMESTAMP(UNIX_TIMESTAMP() * 1000, 'IST'), FROM_UTC_TIMESTAMP(UNIX_TIMESTAMP() * 1000, 'IST') ); Supervisor spec to perform the ingestion task A supervisor spec is a script in JSON format which include the details about how the data should be ingested to the druid data source. We can create a supervisor spec using the druid web UI. export the result of the above hive ETL query to a CSV file and upload it to the GCP bucket. Finally the path of the uploaded data file should be as follow. gs://hive-db-stage/druid_files/${data_source_name}/data_exported.csv ex:- gs://hive-db-stage/druid_files/sales_analytics/data_exported.csv Step 1 : Click on the “Load Data” button in druid UI homepage and then on “Start a new Spec” Step 2 : Select “Google Cloud Storage” option and click on “Connect data” button. Inside the space ( marked with yellow square ) give the path to the CSV file. ( ex-: gs://hive-db-stage/druid_files/sales_analytics/data_exported.csv) Then click on apply and after successful data load “Parse data” button is visible in the right bottom. Step 3 : Then parsed data will be displayed as above figure. Because our dataset is in CSV format and it contains the headers don’t need to change any configurations and press “Next: Parse Time” button right bottom. Step 4 : Here We should select the timestamp column. If a timestamp column in the dataset should be used to find the segment granularity set that column as timestamp column as highlighted in right bottom. If a timestamp column not exist or can use a existing timestamp column for segment granularity set a placeholder value. In this case all the data will be written into the same segment. Step 5: According to our use case, don't need transformations. Step 6: According to our use case, don't need filters eather. Step 7 : Check the data types of columns detected by druid is correct. If any need changes select the column and change the data type as highlighted in the right bottom. Step 8 : Set the segment granularity as per the use case. Step 9 : Step 10 : give the data-source name in the highlighted box. the name should be same, which mentioned in the insert query. Step 11: Finally the ingestion script will be generated as in the below figure. Copy the entire spec in the highlighted area, export it to a JSON file. then move it to gs://hive-db-stage/druid_files/${datasource}/ingestion_spec.json path. Create a job for ingestion task Add the identifier which used in above steps in dev.gradle and prod.gradle “MERCHANT_ANALYTICS” : [ ”sales_analytics” , ] After successful build process there will be a new workflow created.",False
2645295283,page,current,/x/swCsnQ,Superset Integration with Web Portals,"Objective Objective of this document is to outline the approach of integrating Apache Superset with other portals for Data Visualisation requirements. Approach The key would be to design the reports/dashboards from Apache Superset and embed these in the portant pages. There are 3 key concerns in integrating superset with the portals, Context Filtering - it is not scalable to prepare individual datasets for each and every customer (such as a Merchant). In this case the approach would be to create a single dataset and context filter during visualization. Superset supports Jinja templates which enables passing parameters to the reports during runtime https://superset.apache.org/docs/installation/sql-templating Data preparation with Druid - Apache Druid would be used as the backend for superset for providing faster data visualisations. This would mean that the data prepared at the data lake would then be required to be pushed to Druid. Authentication - as the reports would be rendered in superset, it is required that the user has logged into superset. However the navigation from the portal to the superset should happen in a SSO manner. Workflow The typical workflow in building embedded data visualisations is outlined below. Context Filtering The report query should be defined with the jinja template format. The required context value could be passed when accessing the report. http://10.88.1.117:8088/superset/dashboard/10/?standalone=true& context=10025 It is advisable to use a hashed key for the context identification so that the context parameter would not be randomly tried. A randomly generated hash key would be assigned to each merchant and returned to the portal client for query purposes. The same key would be associated with the dataset. Data Preparation with Druid Data preparation would happen in two different ways, Through standard ETL processes developed with Hive and Spark. Hive + Druid integration would push the data off to Druid Druid + Kafka ingestion where Druid itself would build summarized data stores - https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html Authentication Authentication aims to make navigation seamless by providing SSO access to Superset dashboards. Following are the key design decisions. The Superset authentication should utilize jwt token produced by the core application. This would be achieved by introducing a custom authenticator in Superset. See https://sairamkrish.medium.com/apache-superset-custom-authentication-and-integrate-with-other-micro-services-8217956273c1 The user creation in the portals should trigger user creation in the Superset as well The custom authenticator would obtain the username from the jwt token and login to superset using the username bypassing password authentication. Integration of Authentication Service in Web Portal Integration in Portal Front end ( This part not integrating in the Superset site but in the portal, where going to embeds the superset dashboards ) Get a JWT token from auth server by sending request with login details ex -: curl -X POST http://end-point/auth/login -H 'cache-control: no-cache' -H 'content-type: application/json' -H 'postman-token: 0e96bc7f-2890-0658-ba0a-e43ab098be46' -d '{ ""userId"": userID, ""userType"": userType, ""password"": password, ""appId"": appID, ""deviceType"": device, ""deviceId"": ""1"" }' A successful request should get a response with 'jwt' as key. Embed that token in src url for superset login as below http://localhost:8088/login? token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ2ZXJzaW9uIjoidjEiLCJ1c2VyX2lkIjoxMDE2LCJyZWZlcmVuY2VfaWQiOiJhdXRoLXNkay11cGRhdGUtdXNlci10ZXN0IiwidG9rZW5faWQiOiI2MmZkYzY4ZS04ZDZiLTRlYWMtOTgyYi04NmM4MTI2MDViYTIiLCJ1c2VyX3JvbGVzIjpbIiRfY3JlYXRlX3VzZXIiLCIkX3VwZGF0ZV91c2VyIiwiJF9nZXRfdXNlcnMiLCIkX2RlbGV0ZV91c2VyIiwiJF9yZXZva2VfdXNlciJdLCJpYXQiOjE2NDQ4MTk1NTgsImV4cCI6MTY0NDgyMzE1OCwiZGV2aWNlX3R5cGUiOiJwb3J0YWxzIiwiYXBwX2lkIjoiQXV0aCBBZG1pbiBQb3J0YWwiLCJkZXZpY2VfaWQiOiIxIiwiY291bnRyeV9jb2RlIjoiTEsifQ.7i05de2IQvZmVsfoa1B8V6PWAAG_rEjIi6aoLutpcKE Integration in Superset Write a custom security class to incorporate new functionalities Take the token value passed from frontend Call the auth server validation endpoint to validate the JWT passed from front end. ex -: curl -X POST http://end-point/auth/validate -H 'cache-control: no-cache' -H 'content-type: application/json' -H 'postman-token: 0e96bc7f-2890-0658-ba0a-e43ab098be46' -d '{ ""jwt"": ""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ2ZXJzaW9uIjoidjEiLCJ1c2VyX2lkIjoxMDE2LCJyZWZlcmVuY2VfaWQiOiJhdXRoLXNkay11cGRhdGUtdXNlci10ZXN0IiwidG9rZW5faWQiOiI2MmZkYzY4ZS04ZDZiLTRlYWMtOTgyYi04NmM4MTI2MDViYTIiLCJ1c2VyX3JvbGVzIjpbIiRfY3JlYXRlX3VzZXIiLCIkX3VwZGF0ZV91c2VyIiwiJF9nZXRfdXNlcnMiLCIkX2RlbGV0ZV91c2VyIiwiJF9yZXZva2VfdXNlciJdLCJpYXQiOjE2NDQ4MTk1NTgsImV4cCI6MTY0NDgyMzE1OCwiZGV2aWNlX3R5cGUiOiJwb3J0YWxzIiwiYXBwX2lkIjoiQXV0aCBBZG1pbiBQb3J0YWwiLCJkZXZpY2VfaWQiOiIxIiwiY291bnRyeV9jb2RlIjoiTEsifQ.7i05de2IQvZmVsfoa1B8V6PWAAG_rEjIi6aoLutpcKE"" }' If the response is success the token is a valid one. Can extract the user name from the token by decoding it ex -: payload = jwt.decode(token, options={""verify_signature"": False}) user_id = payload[""user_id""] Use the extracted user_id to login the user to superset account Create Users in superset through API When We have the access to the terminal of the instance where superset is installed, We can use following superset command to create user as below. superset fab create-user --role $role_name --username $user_name --firstname $first_name --lastname $last_name --email $email --password $password Users can be created in superset from a remote host using following end point. The placeholders should be replaced by respective values. the USERNAME should the user id of the user. Currently merchant team is using this endpoint to create users. curl --location --request POST ' https://analytics-service.pickme.lk/users/add ' \ --header 'authority: analytics-service.pickme.lk ' \ --header 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9' \ --header 'accept-language: en-US,en;q=0.9' \ --header 'cache-control: max-age=0' \ --header 'content-type: multipart/form-data; boundary=----WebKitFormBoundaryAGpDAmb2HuVeolXy' \ --header 'cookie: __adroll_fpc=999b92168716f402e2a7f010658e4b38-1640017068751; __insp_wid=2137654025; __insp_slim=1640073406768; __insp_nv=true; __insp_targlpu=aHR0cHM6Ly9waWNrbWUubGsvY2FyZWVycw%3D%3D; __insp_targlpt=UGlja21lIENhcmVlcnMgfCBMaWZlIGF0IFBpY2tNZSB8IExldHMgd29yayB0b2dldGhlciB0byBtYWtlIHRyYXZlbGluZyBmdW4gLSBQaWNrTWUubGs%3D; __insp_norec_howoften=true; __insp_norec_sess=true; __ar_v4=XMEHDYMLJZFX7C6F7ACV2M%3A20220019%3A2%7CZ746SHZATNBSNPIHJGWO2C%3A20220019%3A2%7CEU2T6FJFG5C55NGKLCMROZ%3A20220019%3A2; _ga_65300MG0K7=GS1.1.1640076651.3.0.1640076651.0; _ga=GA1.1.1523978963.1640017067; _ga_SJV841SYQT=GS1.1.1655894242.8.1.1655895434.0; session=.eJytkk9LxDAQR79LznWbJpN_BRHEq0e9iJRMZmLL1u3SdJVF_O5G_QQLe5wH7_E7zJcY8splFP22nrgRw0SiF8YDW-i8iqgBOvAQSIMn7LTVRjtnLRgHFNBqn4x3AXXwCIRRuQjKUJBeORsSITuJPtugk0QZTXJZQUQl2ZJREhFMyi471VECBqM6nbxoRCprHrZlz4e6h1hqQ86n2ucM3kZgth0qyhVhIMUODOnqzUuKM1enio04xjcexqlsy3oW_YsYt-1Y-raNhziftymVm8Lrx5R4d5zS_p138749VVTauUrt3Z__VMHD_eNCPD9P_HmrTajp66S8vV7KXZ66yIhE4rURv8f_m3Ti-wdvX74W.Yrw67Q.UkcmmNoaGCJRz-xdVUu8BCdSynU; session=.eJytkd1KAzEQhV-l5Lp2d5PJ34KI4q03gt5IKUlm4i6N3dJsLaX03R31FXo1zOF8h2HORWxyCXWgKvqPi1jMPEQ9pkS1iqV4RCRcvE4nsb6ul-w9UB1En0OpxOuIohfaARnonAxRAXTgwKMCh7FTRmllrTGgLaCPRrmknfVReRcBY5A2gNToWyet8Qkj2Ta6bLxKbWyDTjZLCFG2ZFDLNkbQKdtsZYcJCLTsVHJ8ZqqHvJmnLe34HqRWabQucT5lcCYAkemixMxS9CjJgkbFXJlSKMQMg0uxD5-0GcY6T4fz7xuGed7XvmnCLpTzPKZ6V-nwPSZa7ce0_aJV2TZHlmpTGGoe_vg3Fp6fXiak8j7S6V5pz9G3iXLmdlFWcJ-_vv8SO3H9AZj5qNM.Yr7Feg.PZuvIcQQ65ekAwLOycKu9JhMgMo' \ --header 'origin: https://analytics-service.pickme.lk ' \ --header 'referer: https://analytics-service.pickme.lk/users/add ' \ --form 'csrf_token=""ImRlMDM1ZDc4Y2JhMmVmNDg2YTRlZTYxYjJkZmNiYWI5ZDJlNzQ1ZDMi.Yrw67A.2KhDA4bUlAtwogv3VT4eM8RhFUA""' \ --form 'first_name=""{FIRST_NAME}""' \ --form 'last_name=""{LAST_NAME}""' \ --form 'username=""{USERNAME}""' \ --form 'active=""y""' \ --form 'email=""{EMAIL}""' \ --form 'roles=""8""' \ --form 'password=""{PASSWORD}""' \ --form 'conf_password=""{PASSWORD}""' POC Backlog Item Description Status Superset + Druid Integration Check the integration between Druid & Superset for data visualisation purpose Done Context filtering Using Jinja templates for context filtering Done Embedding Superset dashboards Embedding using the IFrames Done Embedding using an Angular component Not Started Address the CORS issues Not Started How to add report filters Not Started Data Preparation Druid + Kafka Integration In-progress Druid + Hive Integration In-progress Authentication Using a custom authenticator with Superset using jwt token in query string Done Using bearer token in Auth Header. Is this possible? Not Started Agreement on the jwt token formats Not Started User Creation User creation through Superset API Not Started",False
2659123211,page,current,/x/CwB-ng,Probability based availability on vehicle selection,Job to output the probability based availability of vehicle models. This is the probability of at least one vehicle existing in the given time bin for the vehicle model. The output is sent as a CSV file to a GCS location. Calculate the mean number of vehicles per day per minute bin per dsd belonging to a given vehicle model (λ). Calculate the probability of no vehicle existing in the given time period using poisson distribution (k=0). Substract the above calculated probability from 1 to get the probability of at least one vehicle existing. The implementation is generalized to get the probability of at least x number of vehicles existing where the value for x can be configured.,False
2659450918,page,current,/x/JgCEng,Optimization of the preptime model,"Problem statement The ‘outlier preptime models’ were initially deployed on 29th January 2022. The RMSE trends were fluctuating afterwards and this analysis aims to evaluate the performance of the outlier models while looking for other factors to optimize the preptime RMSE. Introduction to the problem The below chart depicts the weekly RMSE trends of the progress screen models. The brown coloured line represents the preptime RMSE. The weekly preptime RMSE has increased more than 10 minutes from 25th of Friday in February. According to the second graph below, the monthly RMSE has also increased after February 25th. However, there is a clear indication of stable RMSEs after the deployment of the outlier model, only with few fluctuations in between. 1.Evaluating the models - Outlier merchants Initially the outlier models were evaluated considering the data from 29th of January. Only the predictions obtained at the merchant confirmed status were used for this analysis. Only the outlier merchants selected as of 2nd of February were used. Furthermore, the predictions were evaluated considering the SLAS set in the ETA dashboard. According to the above graphs, the predictions exceeding the 20 minutes are lower in the outlier model. Also, there is a significant increase in the WOW count when the outlier model was used. Thus, the outlier model has an acceptable accuracy in predicting the prep time. 2.Impact of the preptime model to the total ETA The end goal of optimizing each of the single models is to improve the accuracy of the final ETA. The percentage of orders within the wow and 5 mins zone has to be improved in order to increase the customer satisfaction. The impact of the preptime accuracy towards the final ETA accuracy was analyzed under this section. The analysis was done considering data from 29th of January to 24th of February. There were 292,222 completed orders in the platform within the time period considered for the analysis. 64% of orders fall into WOW category while there are 15% of other orders delivered after 5 minutes of the predicted time which is acceptable. If we consider the orders where the total ETA is in the WOW category, the majority of orders has a predicted preptime less than 5 minutes to the actual preptime. In the 5 minutes category of the total ETA, the majority of preptime orders have a predicted prep time less than 5 minutes to the actual preptime. In the 10 minutes category also the majority orders have a predicted preptime which is less than 10 minutes to the actual preptime. However, the orders with predicted preptime which is more than 10 minutes greater than the actual preptime are not significantly leading to higher errors in ETA. 3.Association of the preptime expiry status to the preptime predictions and to the total ETA There are 51% of orders where the order ready event was used while the prep time expired event was used only in 49% of orders. The RMSEs of the prep time model in preptime expiry and order ready status are as follows. The preptime expiry event is a default value given for each merchant by the system. If a merchant has let the preptime expiry event fire in a considerable amount of orders, the model learns those patterns with time. That can be the reason for the reduced RMSEs in the events where the preptime expiry has been fired. The actual preptime at order ready events are not stable as preptime expiry events and those are mostly random. That can lead to higher RMSEs as the model always does not learn all the order ready times. However, when the order ready event is used, usually the order is actually prepared. But, the merchant might still need additional time in an event where the preptime expiry event automatically fires. This makes the driver wait at the merchant for a longer time. Thus, the impact of the preptime expiry event for the waiting time accuracy was evaluated as the next step. The predicted waiting time is considered to be a static time of 3 minutes for each merchant. The RMSE is less than 3 minutes for the orders where the order ready event has been used. Thus, the fluctuation of the actual value from the predicted value is less in such cases. However, the RMSE is greater than 5 minutes when the preptime expiry was used. This is due to drivers having to wait until the merchant actually prepares the order. The below plot evaluates the RMSE of the total ETA at each preptime expiry event. The total RMSE seems to be higher when the order ready event was used. Thus, the type of preptime expiry event used has an impact on the final predicted ETA. Solutions by Doordash Adjust the preptime expired data to represent an actual preptime Solutions by Grab measure preparation time from the moment you accept an order to the moment the driver picks it up for delivery Observations and conclusions The outlier model predicts the prep time for selected outlier merchants more accurately than the previous default model The impact of the prep time model accuracy to the final ETA results is significantly high The RMSE of the prep time model at an event where the order ready status is used is higher due to the randomness of the actual preptime The waiting time highly affected when the merchant lets the prep time expiry event to fire automatically Even though the waiting times are higher in preptime expiry events, the accuracy of the total ETA is lower in order ready events. This is due to the higher impact of the preptime accuracy to the final ETA results",False
2663251986,page,current,/x/EgC_ng,GKE Setup,"Pushing Images to container registry Container Registry is available at https://console.cloud.google.com/artifacts/docker/pickme-dataprod/asia-southeast1/pickmeprodcontainerregistry?project=pickme-dataprod Login to Docker Registry docker login asia-southeast1-docker.pkg.dev/pickme-dataprod/pickmeprodcontainerregistry Build your project ./docker-start.sh Tag the image e.g. sudo docker tag docker.pickme.lk/deliveryeta:latest asia-southeast1-docker.pkg.dev/pickme-dataprod/pickmeprodcontainerregistry/deliveryeta:latest Push the image e.g. docker image push asia-southeast1-docker.pkg.dev/pickme-dataprod/pickmeprodcontainerregistry/deliveryeta:latest Deploying the Nginx Ingress Controller Ensure following permissions are available to the default service account Installation steps are available here https://cloud.google.com/community/tutorials/nginx-ingress-gke However in order to use a local ip for the ingress, the ingress controller should be installed in the following manner. Create internal-ingress.yaml file controller: service: namespace: data-services loadBalancerIP: 10.130.200.231 annotations: networking.gke.io/load-balancer-type: ""Internal"" Use following command to create the ingress controller helm install nginx-ingress ingress-nginx/ingress-nginx --namespace data-services -f internal-ingress.yaml --set controller.replicaCount=2 --set controller.nodeSelector.""beta\.kubernetes\.io/os""=linux --set defaultBackend.nodeSelector.""beta\.kubernetes\.io/os""=linux --set controller.admissionWebhooks.patch.nodeSelector.""beta\.kubernetes\.io/os""=linux export NGINX_INGRESS_IP=$(kubectl --namespace data-services get service nginx-ingress-ingress-nginx-controller -ojson | jq -r '.status.loadBalancer.ingress[].ip') Create Ingress resource. Shown below is the yaml file and the command. kubectl --namespace data-services apply -f ingress-controller.yaml",False
2693234691,page,current,/x/A4CHo,Passenger to Passenger Referrals - Daily Promo code Generation,"Overview The requirement is that a promocode should be created every day and when a referrer (existing user) meets the criteria on a specific day and they should be assigned to the promocode created on that day. This dynamic promocode creation and the analysis of which referrer/s are eligible for the promocode should happen once everyday. This implementation doesn't apply to referred users(new users) as they will be assigned to a single static promo code provided by the Operations team. API Document: https://pickme.atlassian.net/wiki/spaces/FS/pages/1001619574/Promo+Engine+API#POST-%2Fapi%2Fpromotion%2Fcreate SRS: SRS - Passenger - Passenger Referral Design Pax to Pax - Inviter Flow Solution Design Assumptions : The daily created promo codes need to have a fixed duration for expiry rather than a fixed date. Step 1 : Adding a promo group instead of promo code for the specific pax_pax_referral_incentive reward. rule_name reward_type value promo_usage_type kpi qualifier_value PAX_TO_PAX_INVITER_R1 PROMO REFERRALS_PAX_PAX_GROUP ONCE_DAILY REFERRED_PAX_BATCH_MAX_TRIPS (greater than) 10 Step 2 : Generate a new promo code daily. json { ""name"": ""Passenger Referrer Code for <creation date>"", ""displayName"": ""100 LKR off on for referring new passenger!"", ""description"": ""As you have successfully referred a new passenger to PickMe, you get 100 LKR on any two rides."", ""activeFrom"": ""2022-01-01T09:22:00.000+0530"", ""activeUntil"": ""2022-08-30T09:22:00.000+0530"", ""discountType"": ""A"", ""discountAmount"": 100.0, ""serviceType"": ""RIDES"", ""status"": ""A"", ""type"": ""C"", ""displayStatus"": true, ""currencyCode"": ""LKR"", ""creatorType"": ""C"", ""usageType"": ""I"", ""objectiveId"": 14, ""code"": ""REF<random 5 alphanumeric digit> (the letters should be capital)"", ""promotionConditions"": [ { ""conditionType"": ""max_promotion_passenger_usage"", ""conditionValue"": ""[2]"" }, { ""conditionType"": ""passenger_selected"", ""conditionValue"": ""[1]"" } ] } Step 3 : Assign the newly created promo code to the promo group and insert record to the table. promo_group valid_to valid_from promo_code REFERRALS_PAX_PAX_GROUP 2022-05-01 2022-06-01 REFA2BSD REFERRALS_PAX_PAX_GROUP 2022-05-02 2022-06-02 REF4HJJ6 REFERRALS_PAX_PAX_GROUP 2022-05-03 2022-06-03 REFPOHN9 REFERRALS_PAX_PAX_GROUP 2022-05-04 2022-06-04 REFL9J77 Step 3: Get the promo codes from the promo groups in passenger rewards eligibility. Add new eligible customers for the promo code representing the current date. Update the promo_unfulfilled_passenger_referrals.",False
